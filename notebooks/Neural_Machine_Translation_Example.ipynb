{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation Example",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vi8tH5xO_x",
        "colab_type": "text"
      },
      "source": [
        "# Neural Machine Translation Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiQIr4FSloBt",
        "colab_type": "code",
        "outputId": "855f31e1-0d28-4552-e048-41fd8d6804ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Install TensorFlow and also our package via PyPI\n",
        "!pip install tensorflow-gpu==2.0.0\n",
        "!pip install headliner"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 61kB/s \n",
            "\u001b[?25hCollecting tensorboard<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 30.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Collecting gast==0.2.2 (from tensorflow-gpu==2.0.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.16.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (41.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=15982130e821474e44f06e8d00750b762f5f468f7502c231c6fbbadf753c61da\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement tensorboard<1.15.0,>=1.14.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement tensorflow-estimator<1.15.0rc0,>=1.14.0rc0, but you'll have tensorflow-estimator 2.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, gast, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: gast 0.3.2\n",
            "    Uninstalling gast-0.3.2:\n",
            "      Successfully uninstalled gast-0.3.2\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "Successfully installed gast-0.2.2 tensorboard-2.0.0 tensorflow-estimator-2.0.0 tensorflow-gpu-2.0.0\n",
            "Collecting headliner\n",
            "  Downloading https://files.pythonhosted.org/packages/04/91/93ba4130eb6d793923bf45a695e090f703a7f4af0d433685563a1704d54d/headliner-0.0.6-py3-none-any.whl\n",
            "Collecting nltk==3.4.5 (from headliner)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 16.8MB/s \n",
            "\u001b[?25hCollecting pyyaml==5.1.2 (from headliner)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn==0.21.3 in /usr/local/lib/python3.6/dist-packages (from headliner) (0.21.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5->headliner) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->headliner) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->headliner) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->headliner) (1.16.5)\n",
            "Building wheels for collected packages: nltk, pyyaml\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449906 sha256=617c68746b0df4fdf8562620d20f0060e8b0df042eb0ca754a903638015edc1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44104 sha256=bb7f44d1c3abf8bd7e69634efe111d4fa77db1e70466b31bfe947f220511ce22\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "Successfully built nltk pyyaml\n",
            "Installing collected packages: nltk, pyyaml, headliner\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed headliner-0.0.6 nltk-3.4.5 pyyaml-5.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLWI5oUvJ1St",
        "colab_type": "code",
        "outputId": "33b8d3fe-c86c-4a98-b183-50c67af6067c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# Download the German-English sentence pairs\n",
        "!wget http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip\n",
        "!head deu.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-04 09:17:09--  http://www.manythings.org/anki/deu-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:30::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4541707 (4.3M) [application/zip]\n",
            "Saving to: ‘deu-eng.zip’\n",
            "\n",
            "deu-eng.zip         100%[===================>]   4.33M  5.36MB/s    in 0.8s    \n",
            "\n",
            "2019-10-04 09:17:10 (5.36 MB/s) - ‘deu-eng.zip’ saved [4541707/4541707]\n",
            "\n",
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n",
            "Hi.\tHallo!\n",
            "Hi.\tGrüß Gott!\n",
            "Run!\tLauf!\n",
            "Wow!\tPotzdonner!\n",
            "Wow!\tDonnerwetter!\n",
            "Fire!\tFeuer!\n",
            "Help!\tHilfe!\n",
            "Help!\tZu Hülf!\n",
            "Stop!\tStopp!\n",
            "Wait!\tWarte!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYyOWzeep2lU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the dataset but only take a subset for faster training\n",
        "import io\n",
        "\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    word_pairs = [[w for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    return zip(*word_pairs)\n",
        "\n",
        "eng, ger = create_dataset('deu.txt', 30000)\n",
        "data = list(zip(ger, eng))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPiBB8TCzCVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOz2fIXQz_Kz",
        "colab_type": "code",
        "outputId": "d8fec654-2251-4a2a-c61d-3c9bd9b03c1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define the model and train it\n",
        "from headliner.trainer import Trainer\n",
        "from headliner.model.summarizer_attention import SummarizerAttention\n",
        "\n",
        "summarizer = SummarizerAttention(lstm_size=1024, embedding_size=256)\n",
        "trainer = Trainer(batch_size=64, \n",
        "                  steps_per_epoch=100, \n",
        "                  steps_to_log=20, \n",
        "                  max_output_len=10, \n",
        "                  model_save_path='/tmp/summarizer')\n",
        "trainer.train(summarizer, train, num_epochs=10, val_data=test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training a bare model, initializing preprocessing...\n",
            "vocab encoder: 7551, vocab decoder: 4715, start training loop...\n",
            "epoch 0, batch 20, logs: {'loss': 3.1557514667510986}\n",
            "epoch 0, batch 40, logs: {'loss': 2.48339581489563}\n",
            "epoch 0, batch 60, logs: {'loss': 2.7589330673217773}\n",
            "epoch 0, batch 80, logs: {'loss': 2.0260400772094727}\n",
            "epoch 0, batch 100, logs: {'loss': 2.1709651947021484}\n",
            "\n",
            "(input) Warum bist du zu spät gekommen? \n",
            "(target) Why were you late? \n",
            "(prediction) you you ? <end>\n",
            "\n",
            "\n",
            "(input) Benehmt euch nicht daneben! \n",
            "(target) Don't misbehave. \n",
            "(prediction) you is . <end>\n",
            "\n",
            "\n",
            "(input) Ich kann auf dich warten. \n",
            "(target) I can wait for you. \n",
            "(prediction) i i have have . <end>\n",
            "\n",
            "\n",
            "(input) Tom starb an Krebs. \n",
            "(target) Tom died of cancer. \n",
            "(prediction) tom is . <end>\n",
            "\n",
            "\n",
            "(input) Mache bitte mein Bett zurecht! \n",
            "(target) Please make my bed. \n",
            "(prediction) i have . <end>\n",
            "\n",
            "loss_val improved from None to 2.345742702484131, saving summarizer to /tmp/summarizer\n",
            "epoch 1, batch 120, logs: {'loss': 1.932040810585022, 'loss_val': 2.345742702484131}\n",
            "epoch 1, batch 140, logs: {'loss': 2.341142177581787, 'loss_val': 2.345742702484131}\n",
            "epoch 1, batch 160, logs: {'loss': 2.21513295173645, 'loss_val': 2.345742702484131}\n",
            "epoch 1, batch 180, logs: {'loss': 2.0410573482513428, 'loss_val': 2.345742702484131}\n",
            "epoch 1, batch 200, logs: {'loss': 1.5671595335006714, 'loss_val': 2.345742702484131}\n",
            "\n",
            "(input) Warum bist du zu spät gekommen? \n",
            "(target) Why were you late? \n",
            "(prediction) you are you ? <end>\n",
            "\n",
            "\n",
            "(input) Benehmt euch nicht daneben! \n",
            "(target) Don't misbehave. \n",
            "(prediction) you have a car . <end>\n",
            "\n",
            "\n",
            "(input) Ich kann auf dich warten. \n",
            "(target) I can wait for you. \n",
            "(prediction) i want a little car . <end>\n",
            "\n",
            "\n",
            "(input) Tom starb an Krebs. \n",
            "(target) Tom died of cancer. \n",
            "(prediction) tom is a car . <end>\n",
            "\n",
            "\n",
            "(input) Mache bitte mein Bett zurecht! \n",
            "(target) Please make my bed. \n",
            "(prediction) you is a car . <end>\n",
            "\n",
            "loss_val improved from 2.345742702484131 to 1.992127776145935, saving summarizer to /tmp/summarizer\n",
            "epoch 2, batch 220, logs: {'loss': 1.7159204483032227, 'loss_val': 1.992127776145935}\n",
            "epoch 2, batch 240, logs: {'loss': 2.157316207885742, 'loss_val': 1.992127776145935}\n",
            "epoch 2, batch 260, logs: {'loss': 1.726230263710022, 'loss_val': 1.992127776145935}\n",
            "epoch 2, batch 280, logs: {'loss': 1.4444698095321655, 'loss_val': 1.992127776145935}\n",
            "epoch 2, batch 300, logs: {'loss': 1.5384840965270996, 'loss_val': 1.992127776145935}\n",
            "\n",
            "(input) Warum bist du zu spät gekommen? \n",
            "(target) Why were you late? \n",
            "(prediction) are you your car ? <end>\n",
            "\n",
            "\n",
            "(input) Benehmt euch nicht daneben! \n",
            "(target) Don't misbehave. \n",
            "(prediction) don't be to come . <end>\n",
            "\n",
            "\n",
            "(input) Ich kann auf dich warten. \n",
            "(target) I can wait for you. \n",
            "(prediction) i like to do you . <end>\n",
            "\n",
            "\n",
            "(input) Tom starb an Krebs. \n",
            "(target) Tom died of cancer. \n",
            "(prediction) tom is biased . <end>\n",
            "\n",
            "\n",
            "(input) Mache bitte mein Bett zurecht! \n",
            "(target) Please make my bed. \n",
            "(prediction) you're a car . <end>\n",
            "\n",
            "loss_val improved from 1.992127776145935 to 1.80632746219635, saving summarizer to /tmp/summarizer\n",
            "epoch 3, batch 320, logs: {'loss': 1.7118028402328491, 'loss_val': 1.80632746219635}\n",
            "epoch 3, batch 340, logs: {'loss': 1.782655119895935, 'loss_val': 1.80632746219635}\n",
            "epoch 3, batch 360, logs: {'loss': 1.5010501146316528, 'loss_val': 1.80632746219635}\n",
            "epoch 3, batch 380, logs: {'loss': 1.4390811920166016, 'loss_val': 1.80632746219635}\n",
            "epoch 3, batch 400, logs: {'loss': 1.5901590585708618, 'loss_val': 1.80632746219635}\n",
            "\n",
            "(input) Warum bist du zu spät gekommen? \n",
            "(target) Why were you late? \n",
            "(prediction) you you you do ? <end>\n",
            "\n",
            "\n",
            "(input) Benehmt euch nicht daneben! \n",
            "(target) Don't misbehave. \n",
            "(prediction) don't don't be . <end>\n",
            "\n",
            "\n",
            "(input) Ich kann auf dich warten. \n",
            "(target) I can wait for you. \n",
            "(prediction) i can help you . <end>\n",
            "\n",
            "\n",
            "(input) Tom starb an Krebs. \n",
            "(target) Tom died of cancer. \n",
            "(prediction) tom is at mary . <end>\n",
            "\n",
            "\n",
            "(input) Mache bitte mein Bett zurecht! \n",
            "(target) Please make my bed. \n",
            "(prediction) you're good . <end>\n",
            "\n",
            "loss_val improved from 1.80632746219635 to 1.6914535760879517, saving summarizer to /tmp/summarizer\n",
            "epoch 4, batch 420, logs: {'loss': 1.25198495388031, 'loss_val': 1.6914535760879517}\n",
            "epoch 4, batch 440, logs: {'loss': 1.5152461528778076, 'loss_val': 1.6914535760879517}\n",
            "epoch 4, batch 460, logs: {'loss': 1.2698713541030884, 'loss_val': 1.6914535760879517}\n",
            "finished iterating over dataset, total batches: 467\n",
            "epoch 4, batch 480, logs: {'loss': 1.2309539318084717, 'loss_val': 1.6914535760879517}\n",
            "epoch 4, batch 500, logs: {'loss': 1.2133030891418457, 'loss_val': 1.6914535760879517}\n",
            "\n",
            "(input) Warum bist du zu spät gekommen? \n",
            "(target) Why were you late? \n",
            "(prediction) why are you in . <end>\n",
            "\n",
            "\n",
            "(input) Benehmt euch nicht daneben! \n",
            "(target) Don't misbehave. \n",
            "(prediction) don't don't look ! <end>\n",
            "\n",
            "\n",
            "(input) Ich kann auf dich warten. \n",
            "(target) I can wait for you. \n",
            "(prediction) i can go to tom . <end>\n",
            "\n",
            "\n",
            "(input) Tom starb an Krebs. \n",
            "(target) Tom died of cancer. \n",
            "(prediction) tom is in home . <end>\n",
            "\n",
            "\n",
            "(input) Mache bitte mein Bett zurecht! \n",
            "(target) Please make my bed. \n",
            "(prediction) take your vegetables . <end>\n",
            "\n",
            "loss_val improved from 1.6914535760879517 to 1.5026410818099976, saving summarizer to /tmp/summarizer\n",
            "epoch 5, batch 520, logs: {'loss': 1.098203182220459, 'loss_val': 1.5026410818099976}\n",
            "epoch 5, batch 540, logs: {'loss': 1.1929012537002563, 'loss_val': 1.5026410818099976}\n",
            "epoch 5, batch 560, logs: {'loss': 1.0628494024276733, 'loss_val': 1.5026410818099976}\n",
            "epoch 5, batch 580, logs: {'loss': 1.2494388818740845, 'loss_val': 1.5026410818099976}\n",
            "epoch 5, batch 600, logs: {'loss': 1.6566234827041626, 'loss_val': 1.5026410818099976}\n",
            "\n",
            "(input) Warum bist du zu spät gekommen? \n",
            "(target) Why were you late? \n",
            "(prediction) did you miss you . <end>\n",
            "\n",
            "\n",
            "(input) Benehmt euch nicht daneben! \n",
            "(target) Don't misbehave. \n",
            "(prediction) don't get this ! <end>\n",
            "\n",
            "\n",
            "(input) Ich kann auf dich warten. \n",
            "(target) I can wait for you. \n",
            "(prediction) i can talk to us . <end>\n",
            "\n",
            "\n",
            "(input) Tom starb an Krebs. \n",
            "(target) Tom died of cancer. \n",
            "(prediction) tom died . <end>\n",
            "\n",
            "\n",
            "(input) Mache bitte mein Bett zurecht! \n",
            "(target) Please make my bed. \n",
            "(prediction) please be time . <end>\n",
            "\n",
            "loss_val improved from 1.5026410818099976 to 1.3813378810882568, saving summarizer to /tmp/summarizer\n",
            "epoch 6, batch 620, logs: {'loss': 0.9445387721061707, 'loss_val': 1.3813378810882568}\n",
            "epoch 6, batch 640, logs: {'loss': 1.0116173028945923, 'loss_val': 1.3813378810882568}\n",
            "epoch 6, batch 660, logs: {'loss': 1.0270342826843262, 'loss_val': 1.3813378810882568}\n",
            "epoch 6, batch 680, logs: {'loss': 0.9378625154495239, 'loss_val': 1.3813378810882568}\n",
            "epoch 6, batch 700, logs: {'loss': 1.1653074026107788, 'loss_val': 1.3813378810882568}\n",
            "\n",
            "(input) Warum bist du zu spät gekommen? \n",
            "(target) Why were you late? \n",
            "(prediction) why are you late ? <end>\n",
            "\n",
            "\n",
            "(input) Benehmt euch nicht daneben! \n",
            "(target) Don't misbehave. \n",
            "(prediction) don't get not . <end>\n",
            "\n",
            "\n",
            "(input) Ich kann auf dich warten. \n",
            "(target) I can wait for you. \n",
            "(prediction) i can talk with . <end>\n",
            "\n",
            "\n",
            "(input) Tom starb an Krebs. \n",
            "(target) Tom died of cancer. \n",
            "(prediction) tom died at cancer . <end>\n",
            "\n",
            "\n",
            "(input) Mache bitte mein Bett zurecht! \n",
            "(target) Please make my bed. \n",
            "(prediction) get your money . <end>\n",
            "\n",
            "loss_val improved from 1.3813378810882568 to 1.2401784658432007, saving summarizer to /tmp/summarizer\n",
            "epoch 7, batch 720, logs: {'loss': 1.0613627433776855, 'loss_val': 1.2401784658432007}\n",
            "epoch 7, batch 740, logs: {'loss': 1.3194881677627563, 'loss_val': 1.2401784658432007}\n",
            "epoch 7, batch 760, logs: {'loss': 0.82048100233078, 'loss_val': 1.2401784658432007}\n",
            "epoch 7, batch 780, logs: {'loss': 0.8620443940162659, 'loss_val': 1.2401784658432007}\n",
            "epoch 7, batch 800, logs: {'loss': 0.8453934192657471, 'loss_val': 1.2401784658432007}\n",
            "\n",
            "(input) Warum bist du zu spät gekommen? \n",
            "(target) Why were you late? \n",
            "(prediction) why are you late ? <end>\n",
            "\n",
            "\n",
            "(input) Benehmt euch nicht daneben! \n",
            "(target) Don't misbehave. \n",
            "(prediction) don't follow you . <end>\n",
            "\n",
            "\n",
            "(input) Ich kann auf dich warten. \n",
            "(target) I can wait for you. \n",
            "(prediction) i can wait for you . <end>\n",
            "\n",
            "\n",
            "(input) Tom starb an Krebs. \n",
            "(target) Tom died of cancer. \n",
            "(prediction) tom died at wrong . <end>\n",
            "\n",
            "\n",
            "(input) Mache bitte mein Bett zurecht! \n",
            "(target) Please make my bed. \n",
            "(prediction) please do my bed . <end>\n",
            "\n",
            "loss_val improved from 1.2401784658432007 to 1.0675169229507446, saving summarizer to /tmp/summarizer\n",
            "epoch 8, batch 820, logs: {'loss': 1.3830360174179077, 'loss_val': 1.0675169229507446}\n",
            "epoch 8, batch 840, logs: {'loss': 0.7073920369148254, 'loss_val': 1.0675169229507446}\n",
            "epoch 8, batch 860, logs: {'loss': 0.9910572171211243, 'loss_val': 1.0675169229507446}\n",
            "epoch 8, batch 880, logs: {'loss': 0.8322504162788391, 'loss_val': 1.0675169229507446}\n",
            "epoch 8, batch 900, logs: {'loss': 0.6723040342330933, 'loss_val': 1.0675169229507446}\n",
            "\n",
            "(input) Warum bist du zu spät gekommen? \n",
            "(target) Why were you late? \n",
            "(prediction) why are you late ? <end>\n",
            "\n",
            "\n",
            "(input) Benehmt euch nicht daneben! \n",
            "(target) Don't misbehave. \n",
            "(prediction) don't get it . <end>\n",
            "\n",
            "\n",
            "(input) Ich kann auf dich warten. \n",
            "(target) I can wait for you. \n",
            "(prediction) i can wait . <end>\n",
            "\n",
            "\n",
            "(input) Tom starb an Krebs. \n",
            "(target) Tom died of cancer. \n",
            "(prediction) tom died wrong . <end>\n",
            "\n",
            "\n",
            "(input) Mache bitte mein Bett zurecht! \n",
            "(target) Please make my bed. \n",
            "(prediction) get my bed . <end>\n",
            "\n",
            "loss_val improved from 1.0675169229507446 to 1.0127602815628052, saving summarizer to /tmp/summarizer\n",
            "epoch 9, batch 920, logs: {'loss': 0.8564573526382446, 'loss_val': 1.0127602815628052}\n",
            "finished iterating over dataset, total batches: 934\n",
            "epoch 9, batch 940, logs: {'loss': 0.4496544897556305, 'loss_val': 1.0127602815628052}\n",
            "epoch 9, batch 960, logs: {'loss': 0.5859712362289429, 'loss_val': 1.0127602815628052}\n",
            "epoch 9, batch 980, logs: {'loss': 0.6050708889961243, 'loss_val': 1.0127602815628052}\n",
            "epoch 9, batch 1000, logs: {'loss': 0.751362144947052, 'loss_val': 1.0127602815628052}\n",
            "\n",
            "(input) Warum bist du zu spät gekommen? \n",
            "(target) Why were you late? \n",
            "(prediction) why are you late ? <end>\n",
            "\n",
            "\n",
            "(input) Benehmt euch nicht daneben! \n",
            "(target) Don't misbehave. \n",
            "(prediction) don't touch you . <end>\n",
            "\n",
            "\n",
            "(input) Ich kann auf dich warten. \n",
            "(target) I can wait for you. \n",
            "(prediction) i can wait with you . <end>\n",
            "\n",
            "\n",
            "(input) Tom starb an Krebs. \n",
            "(target) Tom died of cancer. \n",
            "(prediction) tom died with cookies . <end>\n",
            "\n",
            "\n",
            "(input) Mache bitte mein Bett zurecht! \n",
            "(target) Please make my bed. \n",
            "(prediction) please get my bed . <end>\n",
            "\n",
            "loss_val improved from 1.0127602815628052 to 0.9476278424263, saving summarizer to /tmp/summarizer\n",
            "finished iterating over dataset, total batches: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKYgDLngooL1",
        "colab_type": "code",
        "outputId": "a30bc24e-1ae0-4446-8fd3-fe819768958e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Do some prediction\n",
        "summarizer.predict('Hallo mir geht es gut.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"hi , i'm fine . <end>\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u62rjlcu1AKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}