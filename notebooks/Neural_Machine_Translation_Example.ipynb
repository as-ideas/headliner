{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation Example",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vi8tH5xO_x",
        "colab_type": "text"
      },
      "source": [
        "# Neural Machine Translation Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiQIr4FSloBt",
        "colab_type": "code",
        "outputId": "7fdf6017-4afe-4f8f-e947-f9eb22586cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "# Install TensorFlow and also our package via PyPI\n",
        "!pip install tensorflow-gpu==2.0.0\n",
        "!pip install headliner"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 87kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 35.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.16.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.7.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed tensorboard-2.0.0 tensorflow-estimator-2.0.0 tensorflow-gpu-2.0.0\n",
            "Collecting headliner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/5d/ce41845d2a93ba2bdff560f895fcb231d76c1cd5c4e152ed0c308d0a0054/headliner-0.0.18-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from headliner) (3.13)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from headliner) (0.21.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from headliner) (3.2.5)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->headliner) (1.16.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->headliner) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->headliner) (0.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->headliner) (1.12.0)\n",
            "Installing collected packages: headliner\n",
            "Successfully installed headliner-0.0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLWI5oUvJ1St",
        "colab_type": "code",
        "outputId": "785a297c-a15c-473f-c816-fa7ef18f7797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Download the German-English sentence pairs\n",
        "!wget http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-15 16:10:20--  http://www.manythings.org/anki/deu-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:30::6818:6dc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7612057 (7.3M) [application/zip]\n",
            "Saving to: ‘deu-eng.zip’\n",
            "\n",
            "deu-eng.zip         100%[===================>]   7.26M  2.89MB/s    in 2.5s    \n",
            "\n",
            "2019-10-15 16:10:23 (2.89 MB/s) - ‘deu-eng.zip’ saved [7612057/7612057]\n",
            "\n",
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYyOWzeep2lU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4d4584c8-b6ed-4f40-923c-cbc11a6230ad"
      },
      "source": [
        "# Create the dataset but only take a subset for faster training\n",
        "import io\n",
        "\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    word_pairs = [[w for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
        "    return zip(*word_pairs)\n",
        "\n",
        "eng, ger = create_dataset('deu.txt', 30000)\n",
        "data = list(zip(eng, ger))\n",
        "data[:5]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hi.', 'Hallo!'),\n",
              " ('Hi.', 'Grüß Gott!'),\n",
              " ('Run!', 'Lauf!'),\n",
              " ('Wow!', 'Potzdonner!'),\n",
              " ('Wow!', 'Donnerwetter!')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPiBB8TCzCVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOz2fIXQz_Kz",
        "colab_type": "code",
        "outputId": "2a07f18f-7818-4ec5-e14f-c89a5de6ef5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define the model and train it\n",
        "from headliner.trainer import Trainer\n",
        "from headliner.model.summarizer_attention import SummarizerAttention\n",
        "\n",
        "summarizer = SummarizerAttention(lstm_size=1024, embedding_size=256)\n",
        "trainer = Trainer(batch_size=64, \n",
        "                  steps_per_epoch=100, \n",
        "                  steps_to_log=20, \n",
        "                  max_output_len=10, \n",
        "                  model_save_path='/tmp/summarizer')\n",
        "trainer.train(summarizer, train, num_epochs=10, val_data=test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training a bare model, preprocessing data to init model...\n",
            "fitting tokenizers...\n",
            "vocab encoder: 4710, vocab decoder: 7526\n",
            "epoch 0, batch 20, logs: {'loss': 4.108179092407227}\n",
            "epoch 0, batch 40, logs: {'loss': 3.623246765136719}\n",
            "epoch 0, batch 60, logs: {'loss': 3.370672607421875}\n",
            "epoch 0, batch 80, logs: {'loss': 3.2116676330566407}\n",
            "epoch 0, batch 100, logs: {'loss': 3.091304931640625}\n",
            "\n",
            "(input) Please sit down. \n",
            "(target) Bitte setz dich! \n",
            "(prediction) ich bin nicht . <end>\n",
            "\n",
            "\n",
            "(input) We study music. \n",
            "(target) Wir studieren Musik. \n",
            "(prediction) ich ist nicht . <end>\n",
            "\n",
            "\n",
            "(input) I want this one. \n",
            "(target) Ich will dieses. \n",
            "(prediction) ich bin nicht . <end>\n",
            "\n",
            "\n",
            "(input) I am not a monster. \n",
            "(target) Ich bin kein Ungeheuer! \n",
            "(prediction) ich bin nicht . <end>\n",
            "\n",
            "\n",
            "(input) Tom succeeded. \n",
            "(target) Tom hatte Erfolg. \n",
            "(prediction) ich ist nicht . <end>\n",
            "\n",
            "loss_val improved from None to 2.6096150875091553, saving summarizer to /tmp/summarizer\n",
            "epoch 1, batch 120, logs: {'loss': 2.5131534576416015, 'loss_val': 2.6096150875091553}\n",
            "epoch 1, batch 140, logs: {'loss': 2.453067970275879, 'loss_val': 2.6096150875091553}\n",
            "epoch 1, batch 160, logs: {'loss': 2.4117579142252605, 'loss_val': 2.6096150875091553}\n",
            "epoch 1, batch 180, logs: {'loss': 2.3801834106445314, 'loss_val': 2.6096150875091553}\n",
            "epoch 1, batch 200, logs: {'loss': 2.3549868774414064, 'loss_val': 2.6096150875091553}\n",
            "\n",
            "(input) Please sit down. \n",
            "(target) Bitte setz dich! \n",
            "(prediction) sie sind sind . <end>\n",
            "\n",
            "\n",
            "(input) We study music. \n",
            "(target) Wir studieren Musik. \n",
            "(prediction) wir haben sie uns . <end>\n",
            "\n",
            "\n",
            "(input) I want this one. \n",
            "(target) Ich will dieses. \n",
            "(prediction) ich habe tom nicht . <end>\n",
            "\n",
            "\n",
            "(input) I am not a monster. \n",
            "(target) Ich bin kein Ungeheuer! \n",
            "(prediction) ich bin nicht nicht . <end>\n",
            "\n",
            "\n",
            "(input) Tom succeeded. \n",
            "(target) Tom hatte Erfolg. \n",
            "(prediction) tom hat tom . <end>\n",
            "\n",
            "loss_val improved from 2.6096150875091553 to 2.3537137508392334, saving summarizer to /tmp/summarizer\n",
            "epoch 2, batch 220, logs: {'loss': 2.219897651672363, 'loss_val': 2.3537137508392334}\n",
            "epoch 2, batch 240, logs: {'loss': 2.184505081176758, 'loss_val': 2.3537137508392334}\n",
            "epoch 2, batch 260, logs: {'loss': 2.181831868489583, 'loss_val': 2.3537137508392334}\n",
            "epoch 2, batch 280, logs: {'loss': 2.1522686004638674, 'loss_val': 2.3537137508392334}\n",
            "epoch 2, batch 300, logs: {'loss': 2.131430206298828, 'loss_val': 2.3537137508392334}\n",
            "\n",
            "(input) Please sit down. \n",
            "(target) Bitte setz dich! \n",
            "(prediction) sie haben mich . <end>\n",
            "\n",
            "\n",
            "(input) We study music. \n",
            "(target) Wir studieren Musik. \n",
            "(prediction) wir haben mich . <end>\n",
            "\n",
            "\n",
            "(input) I want this one. \n",
            "(target) Ich will dieses. \n",
            "(prediction) ich kann tom nicht . <end>\n",
            "\n",
            "\n",
            "(input) I am not a monster. \n",
            "(target) Ich bin kein Ungeheuer! \n",
            "(prediction) ich kann nicht nicht . <end>\n",
            "\n",
            "\n",
            "(input) Tom succeeded. \n",
            "(target) Tom hatte Erfolg. \n",
            "(prediction) tom hat es nicht . <end>\n",
            "\n",
            "loss_val improved from 2.3537137508392334 to 2.2164502143859863, saving summarizer to /tmp/summarizer\n",
            "epoch 3, batch 320, logs: {'loss': 1.9961315155029298, 'loss_val': 2.2164502143859863}\n",
            "epoch 3, batch 340, logs: {'loss': 1.980158233642578, 'loss_val': 2.2164502143859863}\n",
            "epoch 3, batch 360, logs: {'loss': 1.962914276123047, 'loss_val': 2.2164502143859863}\n",
            "epoch 3, batch 380, logs: {'loss': 1.9578643798828126, 'loss_val': 2.2164502143859863}\n",
            "epoch 3, batch 400, logs: {'loss': 1.9402359008789063, 'loss_val': 2.2164502143859863}\n",
            "\n",
            "(input) Please sit down. \n",
            "(target) Bitte setz dich! \n",
            "(prediction) bitte bitte bitte ! <end>\n",
            "\n",
            "\n",
            "(input) We study music. \n",
            "(target) Wir studieren Musik. \n",
            "(prediction) wir haben einen entscheidung . <end>\n",
            "\n",
            "\n",
            "(input) I want this one. \n",
            "(target) Ich will dieses. \n",
            "(prediction) ich habe es nicht gesehen . <end>\n",
            "\n",
            "\n",
            "(input) I am not a monster. \n",
            "(target) Ich bin kein Ungeheuer! \n",
            "(prediction) ich habe nicht nicht . <end>\n",
            "\n",
            "\n",
            "(input) Tom succeeded. \n",
            "(target) Tom hatte Erfolg. \n",
            "(prediction) tom hat nicht . <end>\n",
            "\n",
            "loss_val improved from 2.2164502143859863 to 2.0091631412506104, saving summarizer to /tmp/summarizer\n",
            "epoch 4, batch 420, logs: {'loss': 1.8183828353881837, 'loss_val': 2.0091631412506104}\n",
            "epoch 4, batch 440, logs: {'loss': 1.8210479736328125, 'loss_val': 2.0091631412506104}\n",
            "epoch 4, batch 460, logs: {'loss': 1.812646484375, 'loss_val': 2.0091631412506104}\n",
            "finished iterating over dataset, total batches: 467\n",
            "epoch 4, batch 480, logs: {'loss': 1.7803890228271484, 'loss_val': 2.0091631412506104}\n",
            "epoch 4, batch 500, logs: {'loss': 1.7419471740722656, 'loss_val': 2.0091631412506104}\n",
            "\n",
            "(input) Please sit down. \n",
            "(target) Bitte setz dich! \n",
            "(prediction) bitte bitte bitte . <end>\n",
            "\n",
            "\n",
            "(input) We study music. \n",
            "(target) Wir studieren Musik. \n",
            "(prediction) wir haben uns töten . <end>\n",
            "\n",
            "\n",
            "(input) I want this one. \n",
            "(target) Ich will dieses. \n",
            "(prediction) ich will , was du hat tom . <end>\n",
            "\n",
            "\n",
            "(input) I am not a monster. \n",
            "(target) Ich bin kein Ungeheuer! \n",
            "(prediction) ich bin nicht müde . <end>\n",
            "\n",
            "\n",
            "(input) Tom succeeded. \n",
            "(target) Tom hatte Erfolg. \n",
            "(prediction) tom hat mich angerufen . <end>\n",
            "\n",
            "loss_val improved from 2.0091631412506104 to 1.9184008836746216, saving summarizer to /tmp/summarizer\n",
            "epoch 5, batch 520, logs: {'loss': 1.5486298561096192, 'loss_val': 1.9184008836746216}\n",
            "epoch 5, batch 540, logs: {'loss': 1.5530106544494628, 'loss_val': 1.9184008836746216}\n",
            "epoch 5, batch 560, logs: {'loss': 1.5481976826985677, 'loss_val': 1.9184008836746216}\n",
            "epoch 5, batch 580, logs: {'loss': 1.5433114051818848, 'loss_val': 1.9184008836746216}\n",
            "epoch 5, batch 600, logs: {'loss': 1.5303941345214844, 'loss_val': 1.9184008836746216}\n",
            "\n",
            "(input) Please sit down. \n",
            "(target) Bitte setz dich! \n",
            "(prediction) bitte bitte setzen sie mich . <end>\n",
            "\n",
            "\n",
            "(input) We study music. \n",
            "(target) Wir studieren Musik. \n",
            "(prediction) wir müssen mehr zeit . <end>\n",
            "\n",
            "\n",
            "(input) I want this one. \n",
            "(target) Ich will dieses. \n",
            "(prediction) ich will das . <end>\n",
            "\n",
            "\n",
            "(input) I am not a monster. \n",
            "(target) Ich bin kein Ungeheuer! \n",
            "(prediction) ich bin nicht so spät . <end>\n",
            "\n",
            "\n",
            "(input) Tom succeeded. \n",
            "(target) Tom hatte Erfolg. \n",
            "(prediction) tom hat es tom . <end>\n",
            "\n",
            "loss_val improved from 1.9184008836746216 to 1.7987327575683594, saving summarizer to /tmp/summarizer\n",
            "epoch 6, batch 620, logs: {'loss': 1.4699604034423828, 'loss_val': 1.7987327575683594}\n",
            "epoch 6, batch 640, logs: {'loss': 1.4616827011108398, 'loss_val': 1.7987327575683594}\n",
            "epoch 6, batch 660, logs: {'loss': 1.4451844533284506, 'loss_val': 1.7987327575683594}\n",
            "epoch 6, batch 680, logs: {'loss': 1.4316317558288574, 'loss_val': 1.7987327575683594}\n",
            "epoch 6, batch 700, logs: {'loss': 1.4263383483886718, 'loss_val': 1.7987327575683594}\n",
            "\n",
            "(input) Please sit down. \n",
            "(target) Bitte setz dich! \n",
            "(prediction) bitte bitte du willst ! <end>\n",
            "\n",
            "\n",
            "(input) We study music. \n",
            "(target) Wir studieren Musik. \n",
            "(prediction) wir sollten den vertrag . <end>\n",
            "\n",
            "\n",
            "(input) I want this one. \n",
            "(target) Ich will dieses. \n",
            "(prediction) ich will das . <end>\n",
            "\n",
            "\n",
            "(input) I am not a monster. \n",
            "(target) Ich bin kein Ungeheuer! \n",
            "(prediction) ich bin kein gute gesicht . <end>\n",
            "\n",
            "\n",
            "(input) Tom succeeded. \n",
            "(target) Tom hatte Erfolg. \n",
            "(prediction) tom hat es gut . <end>\n",
            "\n",
            "loss_val improved from 1.7987327575683594 to 1.714637279510498, saving summarizer to /tmp/summarizer\n",
            "epoch 7, batch 720, logs: {'loss': 1.4398180007934571, 'loss_val': 1.714637279510498}\n",
            "epoch 7, batch 740, logs: {'loss': 1.3694064140319824, 'loss_val': 1.714637279510498}\n",
            "epoch 7, batch 760, logs: {'loss': 1.3508015950520833, 'loss_val': 1.714637279510498}\n",
            "epoch 7, batch 780, logs: {'loss': 1.3387903213500976, 'loss_val': 1.714637279510498}\n",
            "epoch 7, batch 800, logs: {'loss': 1.3236459350585938, 'loss_val': 1.714637279510498}\n",
            "\n",
            "(input) Please sit down. \n",
            "(target) Bitte setz dich! \n",
            "(prediction) bitte bitte setzen sie mich . <end>\n",
            "\n",
            "\n",
            "(input) We study music. \n",
            "(target) Wir studieren Musik. \n",
            "(prediction) wir müssen gewinnen . <end>\n",
            "\n",
            "\n",
            "(input) I want this one. \n",
            "(target) Ich will dieses. \n",
            "(prediction) ich will das . <end>\n",
            "\n",
            "\n",
            "(input) I am not a monster. \n",
            "(target) Ich bin kein Ungeheuer! \n",
            "(prediction) ich bin kein raucher . <end>\n",
            "\n",
            "\n",
            "(input) Tom succeeded. \n",
            "(target) Tom hatte Erfolg. \n",
            "(prediction) tom hat es gemacht . <end>\n",
            "\n",
            "loss_val improved from 1.714637279510498 to 1.610206961631775, saving summarizer to /tmp/summarizer\n",
            "epoch 8, batch 820, logs: {'loss': 1.295832061767578, 'loss_val': 1.610206961631775}\n",
            "epoch 8, batch 840, logs: {'loss': 1.2875219345092774, 'loss_val': 1.610206961631775}\n",
            "epoch 8, batch 860, logs: {'loss': 1.2629159291585286, 'loss_val': 1.610206961631775}\n",
            "epoch 8, batch 880, logs: {'loss': 1.2458459854125976, 'loss_val': 1.610206961631775}\n",
            "epoch 8, batch 900, logs: {'loss': 1.2431150817871093, 'loss_val': 1.610206961631775}\n",
            "\n",
            "(input) Please sit down. \n",
            "(target) Bitte setz dich! \n",
            "(prediction) bitte bitte du bitte ! <end>\n",
            "\n",
            "\n",
            "(input) We study music. \n",
            "(target) Wir studieren Musik. \n",
            "(prediction) wir müssen den nase . <end>\n",
            "\n",
            "\n",
            "(input) I want this one. \n",
            "(target) Ich will dieses. \n",
            "(prediction) ich will das hier . <end>\n",
            "\n",
            "\n",
            "(input) I am not a monster. \n",
            "(target) Ich bin kein Ungeheuer! \n",
            "(prediction) ich bin ein monster . <end>\n",
            "\n",
            "\n",
            "(input) Tom succeeded. \n",
            "(target) Tom hatte Erfolg. \n",
            "(prediction) tom hat es verdient . <end>\n",
            "\n",
            "loss_val improved from 1.610206961631775 to 1.4771090745925903, saving summarizer to /tmp/summarizer\n",
            "epoch 9, batch 920, logs: {'loss': 1.169743537902832, 'loss_val': 1.4771090745925903}\n",
            "finished iterating over dataset, total batches: 934\n",
            "epoch 9, batch 940, logs: {'loss': 1.1432354927062989, 'loss_val': 1.4771090745925903}\n",
            "epoch 9, batch 960, logs: {'loss': 1.0655312220255533, 'loss_val': 1.4771090745925903}\n",
            "epoch 9, batch 980, logs: {'loss': 1.0360244750976562, 'loss_val': 1.4771090745925903}\n",
            "epoch 9, batch 1000, logs: {'loss': 1.0165045166015625, 'loss_val': 1.4771090745925903}\n",
            "\n",
            "(input) Please sit down. \n",
            "(target) Bitte setz dich! \n",
            "(prediction) bitte setzen sie sich auf . <end>\n",
            "\n",
            "\n",
            "(input) We study music. \n",
            "(target) Wir studieren Musik. \n",
            "(prediction) wir müssen musik . <end>\n",
            "\n",
            "\n",
            "(input) I want this one. \n",
            "(target) Ich will dieses. \n",
            "(prediction) ich will dieses gesehen . <end>\n",
            "\n",
            "\n",
            "(input) I am not a monster. \n",
            "(target) Ich bin kein Ungeheuer! \n",
            "(prediction) ich bin kein monster . <end>\n",
            "\n",
            "\n",
            "(input) Tom succeeded. \n",
            "(target) Tom hatte Erfolg. \n",
            "(prediction) tom hat sich erwähnt . <end>\n",
            "\n",
            "loss_val improved from 1.4771090745925903 to 1.4280983209609985, saving summarizer to /tmp/summarizer\n",
            "finished iterating over dataset, total batches: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKYgDLngooL1",
        "colab_type": "code",
        "outputId": "e5758b7c-3b75-425f-e07f-496174c2fa27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Do some prediction\n",
        "summarizer.predict('How are you?')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wie geht es ? <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}