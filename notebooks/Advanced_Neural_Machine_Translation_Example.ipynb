{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced Neural Machine Translation Example",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vi8tH5xO_x",
        "colab_type": "text"
      },
      "source": [
        "# Advanced Neural Machine Translation Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiQIr4FSloBt",
        "colab_type": "code",
        "outputId": "e75bd370-b02d-40ea-913a-7982375ae26c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "# Install TensorFlow and also our package via PyPI\n",
        "!pip install tensorflow-gpu==2.0.0\n",
        "!pip install headliner"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 56kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.16.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 32.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed tensorboard-2.0.0 tensorflow-estimator-2.0.0 tensorflow-gpu-2.0.0\n",
            "Collecting headliner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/5d/ce41845d2a93ba2bdff560f895fcb231d76c1cd5c4e152ed0c308d0a0054/headliner-0.0.18-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from headliner) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from headliner) (0.21.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from headliner) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->headliner) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->headliner) (1.16.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->headliner) (0.14.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->headliner) (1.3.1)\n",
            "Installing collected packages: headliner\n",
            "Successfully installed headliner-0.0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLWI5oUvJ1St",
        "colab_type": "code",
        "outputId": "f144ca92-9280-46c0-8860-701fbad86425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Download the German-English sentence pairs\n",
        "!wget http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-15 15:22:26--  http://www.manythings.org/anki/deu-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:30::6818:6dc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7612057 (7.3M) [application/zip]\n",
            "Saving to: ‘deu-eng.zip’\n",
            "\n",
            "deu-eng.zip         100%[===================>]   7.26M   803KB/s    in 7.8s    \n",
            "\n",
            "2019-10-15 15:22:34 (950 KB/s) - ‘deu-eng.zip’ saved [7612057/7612057]\n",
            "\n",
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYyOWzeep2lU",
        "colab_type": "code",
        "outputId": "e577c349-1ada-4241-99c3-c66e5bf0aa35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Create the dataset but only take a subset for faster training\n",
        "import io\n",
        "\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    word_pairs = [[w for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
        "    return zip(*word_pairs)\n",
        "\n",
        "eng, ger = create_dataset('deu.txt', 30000)\n",
        "data = list(zip(eng, ger))\n",
        "data[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hi.', 'Hallo!'),\n",
              " ('Hi.', 'Grüß Gott!'),\n",
              " ('Run!', 'Lauf!'),\n",
              " ('Wow!', 'Potzdonner!'),\n",
              " ('Wow!', 'Donnerwetter!'),\n",
              " ('Fire!', 'Feuer!'),\n",
              " ('Help!', 'Hilfe!'),\n",
              " ('Help!', 'Zu Hülf!'),\n",
              " ('Stop!', 'Stopp!'),\n",
              " ('Wait!', 'Warte!')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPiBB8TCzCVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IotVafsR43wF",
        "colab_type": "code",
        "outputId": "1726b8ef-42df-41fc-f21e-01da1e1439f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Define custom preprocessing\n",
        "from headliner.preprocessing import Preprocessor\n",
        "\n",
        "preprocessor = Preprocessor(lower_case=True)\n",
        "train_prep = [preprocessor(t) for t in train]\n",
        "train_prep[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('when are you off ?', '<start> wann hast du nach der arbeit frei ? <end>'),\n",
              " ('tom likes girls .', '<start> tom mag mädchen . <end>'),\n",
              " ('wonderful !', '<start> herrlich ! <end>'),\n",
              " ('the spoon is dirty .', '<start> der löffel ist schmutzig . <end>'),\n",
              " (\"tom doesn't care .\", '<start> tom ist es egal . <end>')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORr35PWw5i1c",
        "colab_type": "code",
        "outputId": "737a817e-2e99-41f7-9765-cb62a0e970b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Fit custom tokenizers for input and target\n",
        "from tensorflow_datasets.core.features.text import SubwordTextEncoder\n",
        "from headliner.preprocessing import Vectorizer\n",
        "\n",
        "inputs_prep = [t[0] for t in train_prep]\n",
        "targets_prep = [t[1] for t in train_prep]\n",
        "tokenizer_input = SubwordTextEncoder.build_from_corpus(\n",
        "    inputs_prep, target_vocab_size=2**13,\n",
        "    reserved_tokens=[preprocessor.start_token, preprocessor.end_token])\n",
        "tokenizer_target = SubwordTextEncoder.build_from_corpus(\n",
        "    targets_prep, target_vocab_size=2**13, \n",
        "    reserved_tokens=[preprocessor.start_token, preprocessor.end_token])\n",
        "\n",
        "vectorizer = Vectorizer(tokenizer_input, tokenizer_target)\n",
        "'vocab size input {}, target {}'.format(\n",
        "    vectorizer.encoding_dim, vectorizer.decoding_dim)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vocab size input 5989, target 9458'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wd1m6SPClNP",
        "colab_type": "code",
        "outputId": "3097c33d-cf3b-49b4-f2a9-62101425d6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "# Start tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /tmp/summarizer_tensorboard"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6006\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKpAXaoT7Hds",
        "colab_type": "code",
        "outputId": "5524c184-d1ee-4b37-9bb2-c8a8ed8f303e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define the model and train it\n",
        "from headliner.model.summarizer_transformer import SummarizerTransformer\n",
        "from headliner.trainer import Trainer\n",
        "\n",
        "summarizer = SummarizerTransformer(num_heads=2,\n",
        "                                   feed_forward_dim=1024,\n",
        "                                   num_layers=1,\n",
        "                                   embedding_size=64,\n",
        "                                   dropout_rate=0.1,\n",
        "                                   max_prediction_len=50)\n",
        "summarizer.init_model(preprocessor, vectorizer)\n",
        "trainer = Trainer(steps_per_epoch=250,\n",
        "                  batch_size=64,\n",
        "                  model_save_path='/tmp/summarizer_transformer',\n",
        "                  tensorboard_dir='/tmp/summarizer_tensorboard',\n",
        "                  steps_to_log=50)\n",
        "trainer.train(summarizer, train, num_epochs=10, val_data=test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training an already initialized model...\n",
            "epoch 0, batch 50, logs: {'loss': 4.847849426269531}\n",
            "epoch 0, batch 100, logs: {'loss': 4.120591430664063}\n",
            "epoch 0, batch 150, logs: {'loss': 3.7478401692708334}\n",
            "epoch 0, batch 200, logs: {'loss': 3.4574810791015627}\n",
            "epoch 0, batch 250, logs: {'loss': 3.24528515625}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom ist ein hause . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind ein zu  . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin ein zu  . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) wir sind nicht zu  . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) sie haben sie zu  . <end>\n",
            "\n",
            "loss_val improved from None to 2.459765911102295, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 1, batch 300, logs: {'loss': 2.2673402404785157, 'loss_val': 2.459765911102295}\n",
            "epoch 1, batch 350, logs: {'loss': 2.1796632385253907, 'loss_val': 2.459765911102295}\n",
            "epoch 1, batch 400, logs: {'loss': 2.1494535319010417, 'loss_val': 2.459765911102295}\n",
            "epoch 1, batch 450, logs: {'loss': 2.101768035888672, 'loss_val': 2.459765911102295}\n",
            "finished iterating over dataset, total batches: 467\n",
            "epoch 1, batch 500, logs: {'loss': 2.0485419921875, 'loss_val': 2.459765911102295}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom ist schnell . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind zu sein . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin ein guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter guter \n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) gib nicht zu zu zu sein . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) gib sie sich aus . <end>\n",
            "\n",
            "loss_val improved from 2.459765911102295 to 2.121305465698242, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 2, batch 550, logs: {'loss': 1.7923690795898437, 'loss_val': 2.121305465698242}\n",
            "epoch 2, batch 600, logs: {'loss': 1.76551513671875, 'loss_val': 2.121305465698242}\n",
            "epoch 2, batch 650, logs: {'loss': 1.761105753580729, 'loss_val': 2.121305465698242}\n",
            "epoch 2, batch 700, logs: {'loss': 1.7367677307128906, 'loss_val': 2.121305465698242}\n",
            "epoch 2, batch 750, logs: {'loss': 1.7044339599609375, 'loss_val': 2.121305465698242}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom ist ungeduldig . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin ein bisschen an . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lass es nicht so hause . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) mach sie sich aus . <end>\n",
            "\n",
            "loss_val improved from 2.121305465698242 to 1.9558559656143188, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 3, batch 800, logs: {'loss': 1.5660623168945313, 'loss_val': 1.9558559656143188}\n",
            "epoch 3, batch 850, logs: {'loss': 1.5638919067382813, 'loss_val': 1.9558559656143188}\n",
            "epoch 3, batch 900, logs: {'loss': 1.549705098470052, 'loss_val': 1.9558559656143188}\n",
            "finished iterating over dataset, total batches: 934\n",
            "epoch 3, batch 950, logs: {'loss': 1.5236862182617188, 'loss_val': 1.9558559656143188}\n",
            "epoch 3, batch 1000, logs: {'loss': 1.4957935791015624, 'loss_val': 1.9558559656143188}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom ist pummelig . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin nur im tag . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) gib nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau sich die tür an . <end>\n",
            "\n",
            "loss_val improved from 1.9558559656143188 to 1.8447048664093018, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 4, batch 1050, logs: {'loss': 1.3124964904785157, 'loss_val': 1.8447048664093018}\n",
            "epoch 4, batch 1100, logs: {'loss': 1.3031134033203124, 'loss_val': 1.8447048664093018}\n",
            "epoch 4, batch 1150, logs: {'loss': 1.3005820719401042, 'loss_val': 1.8447048664093018}\n",
            "epoch 4, batch 1200, logs: {'loss': 1.2824595642089844, 'loss_val': 1.8447048664093018}\n",
            "epoch 4, batch 1250, logs: {'loss': 1.27720068359375, 'loss_val': 1.8447048664093018}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom ist pummelig . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin fast am uhr . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) sei nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau sich auf . <end>\n",
            "\n",
            "loss_val improved from 1.8447048664093018 to 1.7218711376190186, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 5, batch 1300, logs: {'loss': 1.1920172119140624, 'loss_val': 1.7218711376190186}\n",
            "epoch 5, batch 1350, logs: {'loss': 1.203248062133789, 'loss_val': 1.7218711376190186}\n",
            "epoch 5, batch 1400, logs: {'loss': 1.2020086669921874, 'loss_val': 1.7218711376190186}\n",
            "finished iterating over dataset, total batches: 1401\n",
            "epoch 5, batch 1450, logs: {'loss': 1.1596061706542968, 'loss_val': 1.7218711376190186}\n",
            "epoch 5, batch 1500, logs: {'loss': 1.1353394775390624, 'loss_val': 1.7218711376190186}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom zögert . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin fast als nächstes . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) komm nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau dir die tür an . <end>\n",
            "\n",
            "loss_val improved from 1.7218711376190186 to 1.6900994777679443, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 6, batch 1550, logs: {'loss': 1.0270941925048829, 'loss_val': 1.6900994777679443}\n",
            "epoch 6, batch 1600, logs: {'loss': 1.0275889587402345, 'loss_val': 1.6900994777679443}\n",
            "epoch 6, batch 1650, logs: {'loss': 1.02754638671875, 'loss_val': 1.6900994777679443}\n",
            "epoch 6, batch 1700, logs: {'loss': 1.0227529907226562, 'loss_val': 1.6900994777679443}\n",
            "epoch 6, batch 1750, logs: {'loss': 1.013340087890625, 'loss_val': 1.6900994777679443}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom ist lästig . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin arbeitslos . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lach nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau hin . <end>\n",
            "\n",
            "loss_val improved from 1.6900994777679443 to 1.6418373584747314, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 7, batch 1800, logs: {'loss': 0.9886691284179687, 'loss_val': 1.6418373584747314}\n",
            "epoch 7, batch 1850, logs: {'loss': 0.9934121704101563, 'loss_val': 1.6418373584747314}\n",
            "finished iterating over dataset, total batches: 1868\n",
            "epoch 7, batch 1900, logs: {'loss': 0.9493770345052084, 'loss_val': 1.6418373584747314}\n",
            "epoch 7, batch 1950, logs: {'loss': 0.9163150024414063, 'loss_val': 1.6418373584747314}\n",
            "epoch 7, batch 2000, logs: {'loss': 0.8992882690429688, 'loss_val': 1.6418373584747314}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom tanzt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin fast am onkel . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) spring nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau hin . <end>\n",
            "\n",
            "loss_val improved from 1.6418373584747314 to 1.5638813972473145, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 8, batch 2050, logs: {'loss': 0.8338279724121094, 'loss_val': 1.5638813972473145}\n",
            "epoch 8, batch 2100, logs: {'loss': 0.8272604370117187, 'loss_val': 1.5638813972473145}\n",
            "epoch 8, batch 2150, logs: {'loss': 0.8280699666341146, 'loss_val': 1.5638813972473145}\n",
            "epoch 8, batch 2200, logs: {'loss': 0.822716293334961, 'loss_val': 1.5638813972473145}\n",
            "epoch 8, batch 2250, logs: {'loss': 0.8193378295898438, 'loss_val': 1.5638813972473145}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom hat geschummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin arbeitslos . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lach nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau hin . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 9, batch 2300, logs: {'loss': 0.827230224609375, 'loss_val': 1.5894076824188232}\n",
            "finished iterating over dataset, total batches: 2335\n",
            "epoch 9, batch 2350, logs: {'loss': 0.7916562652587891, 'loss_val': 1.5894076824188232}\n",
            "epoch 9, batch 2400, logs: {'loss': 0.7509266662597657, 'loss_val': 1.5894076824188232}\n",
            "epoch 9, batch 2450, logs: {'loss': 0.7328492736816407, 'loss_val': 1.5894076824188232}\n",
            "epoch 9, batch 2500, logs: {'loss': 0.7222935180664063, 'loss_val': 1.5894076824188232}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom fehlt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin genauso einverstanden . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) spring nicht ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau hin . <end>\n",
            "\n",
            "loss_val improved from 1.5638813972473145 to 1.505288004875183, saving summarizer to /tmp/summarizer_transformer\n",
            "finished iterating over dataset, total batches: 2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKYgDLngooL1",
        "colab_type": "code",
        "outputId": "3f1f22ca-1839-45f9-a2b7-c38d28508752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load best model and do some prediction\n",
        "best_summarizer = SummarizerTransformer.load('/tmp/summarizer_transformer')\n",
        "best_summarizer.predict('Do you like robots?')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'magst du roboter ? <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpOX8NweyrHN",
        "colab_type": "code",
        "outputId": "f24aef18-dd55-426a-9b6f-2c9527ebb857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "source": [
        "# Plot attention alignment for a prediction\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_attention_weights(summarizer, pred_vectors, layer_name):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "    input_text, _ = pred_vectors['preprocessed_text']\n",
        "    input_sequence = summarizer.vectorizer.encode_input(input_text)\n",
        "    pred_sequence = pred_vectors['predicted_sequence']\n",
        "    attention = tf.squeeze(pred_vectors['attention_weights'][layer_name])\n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(1, 2, head + 1)\n",
        "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "        fontdict = {'fontsize': 10}\n",
        "        ax.set_xticks(range(len(input_sequence)))\n",
        "        ax.set_yticks(range(len(pred_sequence)))\n",
        "        ax.set_ylim(len(pred_sequence) - 1.5, -0.5)\n",
        "        ax.set_xticklabels(\n",
        "            [summarizer.vectorizer.decode_input([i]) for i in input_sequence],\n",
        "            fontdict=fontdict,\n",
        "            rotation=90)\n",
        "        ax.set_yticklabels([summarizer.vectorizer.decode_output([i]) \n",
        "                            for i in pred_sequence], fontdict=fontdict)\n",
        "        ax.set_xlabel('Head {}'.format(head + 1))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "pred_vectors = best_summarizer.predict_vectors(\n",
        "    'Tom ran out of the house.', '')\n",
        "plot_attention_weights(best_summarizer, pred_vectors, 'decoder_layer1_block2')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABHgAAAH5CAYAAAD6JomZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 1152x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u62rjlcu1AKv",
        "colab_type": "code",
        "outputId": "5820fc64-8954-460a-e180-cdce15d7c260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Continue training to improve the model and check the BLEU score\n",
        "from headliner.evaluation import BleuScorer\n",
        "\n",
        "bleu_scorer = BleuScorer(tokens_to_ignore=[preprocessor.start_token, \n",
        "                                           preprocessor.end_token])\n",
        "trainer.train(best_summarizer, \n",
        "              train, \n",
        "              num_epochs=30, \n",
        "              val_data=test, \n",
        "              scorers={'bleu': bleu_scorer})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training an already initialized model...\n",
            "epoch 0, batch 50, logs: {'loss': 0.6212480926513672}\n",
            "epoch 0, batch 100, logs: {'loss': 0.63480224609375}\n",
            "epoch 0, batch 150, logs: {'loss': 0.6377926127115885}\n",
            "epoch 0, batch 200, logs: {'loss': 0.6386700820922852}\n",
            "epoch 0, batch 250, logs: {'loss': 0.639394287109375}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schreit . <end>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin doppelt so erwachsen . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell . <end>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau zu . <end>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss_val improved from None to 1.4889614582061768, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 1, batch 300, logs: {'loss': 0.6381979370117188, 'bleu': 0.684873107475936, 'loss_val': 1.4889614582061768}\n",
            "epoch 1, batch 350, logs: {'loss': 0.6408485412597656, 'bleu': 0.684873107475936, 'loss_val': 1.4889614582061768}\n",
            "epoch 1, batch 400, logs: {'loss': 0.6411895751953125, 'bleu': 0.684873107475936, 'loss_val': 1.4889614582061768}\n",
            "epoch 1, batch 450, logs: {'loss': 0.6473457336425781, 'bleu': 0.684873107475936, 'loss_val': 1.4889614582061768}\n",
            "finished iterating over dataset, total batches: 467\n",
            "epoch 1, batch 500, logs: {'loss': 0.6295943603515625, 'bleu': 0.684873107475936, 'loss_val': 1.4889614582061768}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin doppelt so weit . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau hin . <end>\n",
            "\n",
            "loss_val improved from 1.4889614582061768 to 1.482133388519287, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 2, batch 550, logs: {'loss': 0.5305867767333985, 'bleu': 0.7124219473222054, 'loss_val': 1.482133388519287}\n",
            "epoch 2, batch 600, logs: {'loss': 0.5359102630615235, 'bleu': 0.7124219473222054, 'loss_val': 1.482133388519287}\n",
            "epoch 2, batch 650, logs: {'loss': 0.5365870157877605, 'bleu': 0.7124219473222054, 'loss_val': 1.482133388519287}\n",
            "epoch 2, batch 700, logs: {'loss': 0.5380923843383789, 'bleu': 0.7124219473222054, 'loss_val': 1.482133388519287}\n",
            "epoch 2, batch 750, logs: {'loss': 0.5406396484375, 'bleu': 0.7124219473222054, 'loss_val': 1.482133388519287}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom überlebt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin doppelt so erwachsen . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau hin . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 3, batch 800, logs: {'loss': 0.5499881744384766, 'bleu': 0.6976308318258426, 'loss_val': 1.4842947721481323}\n",
            "epoch 3, batch 850, logs: {'loss': 0.5534464263916016, 'bleu': 0.6976308318258426, 'loss_val': 1.4842947721481323}\n",
            "epoch 3, batch 900, logs: {'loss': 0.5552583821614583, 'bleu': 0.6976308318258426, 'loss_val': 1.4842947721481323}\n",
            "finished iterating over dataset, total batches: 934\n",
            "epoch 3, batch 950, logs: {'loss': 0.5448128509521485, 'bleu': 0.6976308318258426, 'loss_val': 1.4842947721481323}\n",
            "epoch 3, batch 1000, logs: {'loss': 0.5267890014648438, 'bleu': 0.6976308318258426, 'loss_val': 1.4842947721481323}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin arbeitslos . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau hin . <end>\n",
            "\n",
            "loss_val improved from 1.482133388519287 to 1.4176106452941895, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 4, batch 1050, logs: {'loss': 0.4577713775634766, 'bleu': 0.7186904163683073, 'loss_val': 1.4176106452941895}\n",
            "epoch 4, batch 1100, logs: {'loss': 0.4574295425415039, 'bleu': 0.7186904163683073, 'loss_val': 1.4176106452941895}\n",
            "epoch 4, batch 1150, logs: {'loss': 0.4652721659342448, 'bleu': 0.7186904163683073, 'loss_val': 1.4176106452941895}\n",
            "epoch 4, batch 1200, logs: {'loss': 0.47529701232910154, 'bleu': 0.7186904163683073, 'loss_val': 1.4176106452941895}\n",
            "epoch 4, batch 1250, logs: {'loss': 0.4784554138183594, 'bleu': 0.7186904163683073, 'loss_val': 1.4176106452941895}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom hat geschummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin genauso lieber . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau hin . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 5, batch 1300, logs: {'loss': 0.4898837661743164, 'bleu': 0.7049929402980343, 'loss_val': 1.4848568439483643}\n",
            "epoch 5, batch 1350, logs: {'loss': 0.4902175521850586, 'bleu': 0.7049929402980343, 'loss_val': 1.4848568439483643}\n",
            "epoch 5, batch 1400, logs: {'loss': 0.49054585774739584, 'bleu': 0.7049929402980343, 'loss_val': 1.4848568439483643}\n",
            "finished iterating over dataset, total batches: 1401\n",
            "epoch 5, batch 1450, logs: {'loss': 0.4661066436767578, 'bleu': 0.7049929402980343, 'loss_val': 1.4848568439483643}\n",
            "epoch 5, batch 1500, logs: {'loss': 0.45094784545898436, 'bleu': 0.7049929402980343, 'loss_val': 1.4848568439483643}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom hat geschummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin fernsehsüchtig . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau hin . <end>\n",
            "\n",
            "loss_val improved from 1.4176106452941895 to 1.383309245109558, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 6, batch 1550, logs: {'loss': 0.4336648178100586, 'bleu': 0.7202048257989005, 'loss_val': 1.383309245109558}\n",
            "epoch 6, batch 1600, logs: {'loss': 0.429630012512207, 'bleu': 0.7202048257989005, 'loss_val': 1.383309245109558}\n",
            "epoch 6, batch 1650, logs: {'loss': 0.4302245585123698, 'bleu': 0.7202048257989005, 'loss_val': 1.383309245109558}\n",
            "epoch 6, batch 1700, logs: {'loss': 0.4287827682495117, 'bleu': 0.7202048257989005, 'loss_val': 1.383309245109558}\n",
            "epoch 6, batch 1750, logs: {'loss': 0.4293268737792969, 'bleu': 0.7202048257989005, 'loss_val': 1.383309245109558}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom hat geschummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin tv-menge erfüllt . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schaut genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 7, batch 1800, logs: {'loss': 0.44521297454833986, 'bleu': 0.7369222999946254, 'loss_val': 1.4924256801605225}\n",
            "epoch 7, batch 1850, logs: {'loss': 0.4428987884521484, 'bleu': 0.7369222999946254, 'loss_val': 1.4924256801605225}\n",
            "finished iterating over dataset, total batches: 1868\n",
            "epoch 7, batch 1900, logs: {'loss': 0.4244889322916667, 'bleu': 0.7369222999946254, 'loss_val': 1.4924256801605225}\n",
            "epoch 7, batch 1950, logs: {'loss': 0.40805801391601565, 'bleu': 0.7369222999946254, 'loss_val': 1.4924256801605225}\n",
            "epoch 7, batch 2000, logs: {'loss': 0.39933779907226563, 'bleu': 0.7369222999946254, 'loss_val': 1.4924256801605225}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin genauso erwachsen . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) rennt nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 8, batch 2050, logs: {'loss': 0.3700966644287109, 'bleu': 0.7171872898441944, 'loss_val': 1.469375491142273}\n",
            "epoch 8, batch 2100, logs: {'loss': 0.36865814208984377, 'bleu': 0.7171872898441944, 'loss_val': 1.469375491142273}\n",
            "epoch 8, batch 2150, logs: {'loss': 0.37939163208007814, 'bleu': 0.7171872898441944, 'loss_val': 1.469375491142273}\n",
            "epoch 8, batch 2200, logs: {'loss': 0.3887173843383789, 'bleu': 0.7171872898441944, 'loss_val': 1.469375491142273}\n",
            "epoch 8, batch 2250, logs: {'loss': 0.39206903076171873, 'bleu': 0.7171872898441944, 'loss_val': 1.469375491142273}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom überlebt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin genauso erwachsen . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau hin . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 9, batch 2300, logs: {'loss': 0.4175285339355469, 'bleu': 0.7296547784852084, 'loss_val': 1.493322491645813}\n",
            "finished iterating over dataset, total batches: 2335\n",
            "epoch 9, batch 2350, logs: {'loss': 0.3964054489135742, 'bleu': 0.7296547784852084, 'loss_val': 1.493322491645813}\n",
            "epoch 9, batch 2400, logs: {'loss': 0.3700288391113281, 'bleu': 0.7296547784852084, 'loss_val': 1.493322491645813}\n",
            "epoch 9, batch 2450, logs: {'loss': 0.3642586898803711, 'bleu': 0.7296547784852084, 'loss_val': 1.493322491645813}\n",
            "epoch 9, batch 2500, logs: {'loss': 0.3604920959472656, 'bleu': 0.7296547784852084, 'loss_val': 1.493322491645813}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin arbeitslos . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 10, batch 2550, logs: {'loss': 0.3545438003540039, 'bleu': 0.7184702359428705, 'loss_val': 1.5076167583465576}\n",
            "epoch 10, batch 2600, logs: {'loss': 0.3590167236328125, 'bleu': 0.7184702359428705, 'loss_val': 1.5076167583465576}\n",
            "epoch 10, batch 2650, logs: {'loss': 0.35589253743489585, 'bleu': 0.7184702359428705, 'loss_val': 1.5076167583465576}\n",
            "epoch 10, batch 2700, logs: {'loss': 0.36044429779052733, 'bleu': 0.7184702359428705, 'loss_val': 1.5076167583465576}\n",
            "epoch 10, batch 2750, logs: {'loss': 0.364622802734375, 'bleu': 0.7184702359428705, 'loss_val': 1.5076167583465576}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin arbeitslos . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau hin . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 11, batch 2800, logs: {'loss': 0.3705809783935547, 'bleu': 0.7292065522266131, 'loss_val': 1.485396146774292}\n",
            "finished iterating over dataset, total batches: 2802\n",
            "epoch 11, batch 2850, logs: {'loss': 0.34329666137695314, 'bleu': 0.7292065522266131, 'loss_val': 1.485396146774292}\n",
            "epoch 11, batch 2900, logs: {'loss': 0.3294825236002604, 'bleu': 0.7292065522266131, 'loss_val': 1.485396146774292}\n",
            "epoch 11, batch 2950, logs: {'loss': 0.3286273956298828, 'bleu': 0.7292065522266131, 'loss_val': 1.485396146774292}\n",
            "epoch 11, batch 3000, logs: {'loss': 0.33004669189453123, 'bleu': 0.7292065522266131, 'loss_val': 1.485396146774292}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich habe keine siebten himmel . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 12, batch 3050, logs: {'loss': 0.3443166732788086, 'bleu': 0.7339099682018256, 'loss_val': 1.472660779953003}\n",
            "epoch 12, batch 3100, logs: {'loss': 0.3445756149291992, 'bleu': 0.7339099682018256, 'loss_val': 1.472660779953003}\n",
            "epoch 12, batch 3150, logs: {'loss': 0.3461367797851562, 'bleu': 0.7339099682018256, 'loss_val': 1.472660779953003}\n",
            "epoch 12, batch 3200, logs: {'loss': 0.3442848205566406, 'bleu': 0.7339099682018256, 'loss_val': 1.472660779953003}\n",
            "epoch 12, batch 3250, logs: {'loss': 0.3494396057128906, 'bleu': 0.7339099682018256, 'loss_val': 1.472660779953003}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) pass auf . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "finished iterating over dataset, total batches: 3269\n",
            "epoch 13, batch 3300, logs: {'loss': 0.3067830657958984, 'bleu': 0.7111771988345562, 'loss_val': 1.5417207479476929}\n",
            "epoch 13, batch 3350, logs: {'loss': 0.29824081420898435, 'bleu': 0.7111771988345562, 'loss_val': 1.5417207479476929}\n",
            "epoch 13, batch 3400, logs: {'loss': 0.29661412556966144, 'bleu': 0.7111771988345562, 'loss_val': 1.5417207479476929}\n",
            "epoch 13, batch 3450, logs: {'loss': 0.3024469947814941, 'bleu': 0.7111771988345562, 'loss_val': 1.5417207479476929}\n",
            "epoch 13, batch 3500, logs: {'loss': 0.3049110717773437, 'bleu': 0.7111771988345562, 'loss_val': 1.5417207479476929}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) rennt nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 14, batch 3550, logs: {'loss': 0.33330257415771486, 'bleu': 0.7244227127330725, 'loss_val': 1.6029106378555298}\n",
            "epoch 14, batch 3600, logs: {'loss': 0.3254917907714844, 'bleu': 0.7244227127330725, 'loss_val': 1.6029106378555298}\n",
            "epoch 14, batch 3650, logs: {'loss': 0.32708951314290363, 'bleu': 0.7244227127330725, 'loss_val': 1.6029106378555298}\n",
            "epoch 14, batch 3700, logs: {'loss': 0.32631175994873046, 'bleu': 0.7244227127330725, 'loss_val': 1.6029106378555298}\n",
            "finished iterating over dataset, total batches: 3736\n",
            "epoch 14, batch 3750, logs: {'loss': 0.3247098999023437, 'bleu': 0.7244227127330725, 'loss_val': 1.6029106378555298}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) rennt nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 15, batch 3800, logs: {'loss': 0.27654714584350587, 'bleu': 0.7361939944764474, 'loss_val': 1.4747116565704346}\n",
            "epoch 15, batch 3850, logs: {'loss': 0.2725899124145508, 'bleu': 0.7361939944764474, 'loss_val': 1.4747116565704346}\n",
            "epoch 15, batch 3900, logs: {'loss': 0.28193580627441406, 'bleu': 0.7361939944764474, 'loss_val': 1.4747116565704346}\n",
            "epoch 15, batch 3950, logs: {'loss': 0.28707715988159177, 'bleu': 0.7361939944764474, 'loss_val': 1.4747116565704346}\n",
            "epoch 15, batch 4000, logs: {'loss': 0.2932083435058594, 'bleu': 0.7361939944764474, 'loss_val': 1.4747116565704346}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) rennt nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau hin . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 16, batch 4050, logs: {'loss': 0.3063184356689453, 'bleu': 0.7245678159437674, 'loss_val': 1.5305821895599365}\n",
            "epoch 16, batch 4100, logs: {'loss': 0.31138168334960936, 'bleu': 0.7245678159437674, 'loss_val': 1.5305821895599365}\n",
            "epoch 16, batch 4150, logs: {'loss': 0.3162423706054687, 'bleu': 0.7245678159437674, 'loss_val': 1.5305821895599365}\n",
            "epoch 16, batch 4200, logs: {'loss': 0.31853786468505857, 'bleu': 0.7245678159437674, 'loss_val': 1.5305821895599365}\n",
            "finished iterating over dataset, total batches: 4203\n",
            "epoch 16, batch 4250, logs: {'loss': 0.3081563720703125, 'bleu': 0.7245678159437674, 'loss_val': 1.5305821895599365}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) rennen sie nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schaut genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 17, batch 4300, logs: {'loss': 0.2708150482177734, 'bleu': 0.742318889409482, 'loss_val': 1.4916431903839111}\n",
            "epoch 17, batch 4350, logs: {'loss': 0.2727382659912109, 'bleu': 0.742318889409482, 'loss_val': 1.4916431903839111}\n",
            "epoch 17, batch 4400, logs: {'loss': 0.27573758443196617, 'bleu': 0.742318889409482, 'loss_val': 1.4916431903839111}\n",
            "epoch 17, batch 4450, logs: {'loss': 0.2791232490539551, 'bleu': 0.742318889409482, 'loss_val': 1.4916431903839111}\n",
            "epoch 17, batch 4500, logs: {'loss': 0.28291229248046873, 'bleu': 0.742318889409482, 'loss_val': 1.4916431903839111}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin genauso erwachsen . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau hin . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 18, batch 4550, logs: {'loss': 0.3093565368652344, 'bleu': 0.7381651764787358, 'loss_val': 1.5463677644729614}\n",
            "epoch 18, batch 4600, logs: {'loss': 0.3142294692993164, 'bleu': 0.7381651764787358, 'loss_val': 1.5463677644729614}\n",
            "epoch 18, batch 4650, logs: {'loss': 0.31578511555989586, 'bleu': 0.7381651764787358, 'loss_val': 1.5463677644729614}\n",
            "finished iterating over dataset, total batches: 4670\n",
            "epoch 18, batch 4700, logs: {'loss': 0.3064955520629883, 'bleu': 0.7381651764787358, 'loss_val': 1.5463677644729614}\n",
            "epoch 18, batch 4750, logs: {'loss': 0.2977115478515625, 'bleu': 0.7381651764787358, 'loss_val': 1.5463677644729614}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) rennen sie nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau hin . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 19, batch 4800, logs: {'loss': 0.2659957504272461, 'bleu': 0.7445447511215773, 'loss_val': 1.5318710803985596}\n",
            "epoch 19, batch 4850, logs: {'loss': 0.2711930274963379, 'bleu': 0.7445447511215773, 'loss_val': 1.5318710803985596}\n",
            "epoch 19, batch 4900, logs: {'loss': 0.2722443644205729, 'bleu': 0.7445447511215773, 'loss_val': 1.5318710803985596}\n",
            "epoch 19, batch 4950, logs: {'loss': 0.27383563995361326, 'bleu': 0.7445447511215773, 'loss_val': 1.5318710803985596}\n",
            "epoch 19, batch 5000, logs: {'loss': 0.27887429809570313, 'bleu': 0.7445447511215773, 'loss_val': 1.5318710803985596}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) ich bin tv-sager . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau hin . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 20, batch 5050, logs: {'loss': 0.3027042007446289, 'bleu': 0.7389010951321814, 'loss_val': 1.5252989530563354}\n",
            "epoch 20, batch 5100, logs: {'loss': 0.30146978378295897, 'bleu': 0.7389010951321814, 'loss_val': 1.5252989530563354}\n",
            "finished iterating over dataset, total batches: 5137\n",
            "epoch 20, batch 5150, logs: {'loss': 0.29324918111165366, 'bleu': 0.7389010951321814, 'loss_val': 1.5252989530563354}\n",
            "epoch 20, batch 5200, logs: {'loss': 0.2774674606323242, 'bleu': 0.7389010951321814, 'loss_val': 1.5252989530563354}\n",
            "epoch 20, batch 5250, logs: {'loss': 0.2735796203613281, 'bleu': 0.7389010951321814, 'loss_val': 1.5252989530563354}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 21, batch 5300, logs: {'loss': 0.2644134712219238, 'bleu': 0.7517291406545273, 'loss_val': 1.5472182035446167}\n",
            "epoch 21, batch 5350, logs: {'loss': 0.26410694122314454, 'bleu': 0.7517291406545273, 'loss_val': 1.5472182035446167}\n",
            "epoch 21, batch 5400, logs: {'loss': 0.2648760986328125, 'bleu': 0.7517291406545273, 'loss_val': 1.5472182035446167}\n",
            "epoch 21, batch 5450, logs: {'loss': 0.27033411026000975, 'bleu': 0.7517291406545273, 'loss_val': 1.5472182035446167}\n",
            "epoch 21, batch 5500, logs: {'loss': 0.27539950561523435, 'bleu': 0.7517291406545273, 'loss_val': 1.5472182035446167}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) rennt nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schau genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 22, batch 5550, logs: {'loss': 0.2937948226928711, 'bleu': 0.7334741155580301, 'loss_val': 1.586303472518921}\n",
            "epoch 22, batch 5600, logs: {'loss': 0.2919683074951172, 'bleu': 0.7334741155580301, 'loss_val': 1.586303472518921}\n",
            "finished iterating over dataset, total batches: 5604\n",
            "epoch 22, batch 5650, logs: {'loss': 0.2723842366536458, 'bleu': 0.7334741155580301, 'loss_val': 1.586303472518921}\n",
            "epoch 22, batch 5700, logs: {'loss': 0.26302036285400393, 'bleu': 0.7334741155580301, 'loss_val': 1.586303472518921}\n",
            "epoch 22, batch 5750, logs: {'loss': 0.2621115112304688, 'bleu': 0.7334741155580301, 'loss_val': 1.586303472518921}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) rennen sie nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 23, batch 5800, logs: {'loss': 0.2650888442993164, 'bleu': 0.7320130189155565, 'loss_val': 1.5468677282333374}\n",
            "epoch 23, batch 5850, logs: {'loss': 0.2647309494018555, 'bleu': 0.7320130189155565, 'loss_val': 1.5468677282333374}\n",
            "epoch 23, batch 5900, logs: {'loss': 0.26511500040690106, 'bleu': 0.7320130189155565, 'loss_val': 1.5468677282333374}\n",
            "epoch 23, batch 5950, logs: {'loss': 0.2686612129211426, 'bleu': 0.7320130189155565, 'loss_val': 1.5468677282333374}\n",
            "epoch 23, batch 6000, logs: {'loss': 0.273745849609375, 'bleu': 0.7320130189155565, 'loss_val': 1.5468677282333374}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) rennt nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 24, batch 6050, logs: {'loss': 0.2945831680297852, 'bleu': 0.7184561389829079, 'loss_val': 1.4791797399520874}\n",
            "finished iterating over dataset, total batches: 6071\n",
            "epoch 24, batch 6100, logs: {'loss': 0.2710887336730957, 'bleu': 0.7184561389829079, 'loss_val': 1.4791797399520874}\n",
            "epoch 24, batch 6150, logs: {'loss': 0.26071935017903647, 'bleu': 0.7184561389829079, 'loss_val': 1.4791797399520874}\n",
            "epoch 24, batch 6200, logs: {'loss': 0.25175161361694337, 'bleu': 0.7184561389829079, 'loss_val': 1.4791797399520874}\n",
            "epoch 24, batch 6250, logs: {'loss': 0.2543775634765625, 'bleu': 0.7184561389829079, 'loss_val': 1.4791797399520874}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 25, batch 6300, logs: {'loss': 0.2467753219604492, 'bleu': 0.7266632453470588, 'loss_val': 1.547176480293274}\n",
            "epoch 25, batch 6350, logs: {'loss': 0.2559062385559082, 'bleu': 0.7266632453470588, 'loss_val': 1.547176480293274}\n",
            "epoch 25, batch 6400, logs: {'loss': 0.25850001017252605, 'bleu': 0.7266632453470588, 'loss_val': 1.547176480293274}\n",
            "epoch 25, batch 6450, logs: {'loss': 0.2617708969116211, 'bleu': 0.7266632453470588, 'loss_val': 1.547176480293274}\n",
            "epoch 25, batch 6500, logs: {'loss': 0.2655138854980469, 'bleu': 0.7266632453470588, 'loss_val': 1.547176480293274}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell . <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "finished iterating over dataset, total batches: 6538\n",
            "epoch 26, batch 6550, logs: {'loss': 0.26719429016113283, 'bleu': 0.7391339649831419, 'loss_val': 1.4648536443710327}\n",
            "epoch 26, batch 6600, logs: {'loss': 0.24301500320434571, 'bleu': 0.7391339649831419, 'loss_val': 1.4648536443710327}\n",
            "epoch 26, batch 6650, logs: {'loss': 0.24089566548665364, 'bleu': 0.7391339649831419, 'loss_val': 1.4648536443710327}\n",
            "epoch 26, batch 6700, logs: {'loss': 0.24068975448608398, 'bleu': 0.7391339649831419, 'loss_val': 1.4648536443710327}\n",
            "epoch 26, batch 6750, logs: {'loss': 0.24264845275878907, 'bleu': 0.7391339649831419, 'loss_val': 1.4648536443710327}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) rennt nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 27, batch 6800, logs: {'loss': 0.24427263259887697, 'bleu': 0.7170531246673052, 'loss_val': 1.6027475595474243}\n",
            "epoch 27, batch 6850, logs: {'loss': 0.2506506729125977, 'bleu': 0.7170531246673052, 'loss_val': 1.6027475595474243}\n",
            "epoch 27, batch 6900, logs: {'loss': 0.2562866465250651, 'bleu': 0.7170531246673052, 'loss_val': 1.6027475595474243}\n",
            "epoch 27, batch 6950, logs: {'loss': 0.2591534996032715, 'bleu': 0.7170531246673052, 'loss_val': 1.6027475595474243}\n",
            "epoch 27, batch 7000, logs: {'loss': 0.2620638732910156, 'bleu': 0.7170531246673052, 'loss_val': 1.6027475595474243}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "finished iterating over dataset, total batches: 7005\n",
            "epoch 28, batch 7050, logs: {'loss': 0.2241122055053711, 'bleu': 0.7365892579227342, 'loss_val': 1.5729097127914429}\n",
            "epoch 28, batch 7100, logs: {'loss': 0.22594600677490234, 'bleu': 0.7365892579227342, 'loss_val': 1.5729097127914429}\n",
            "epoch 28, batch 7150, logs: {'loss': 0.22656578063964844, 'bleu': 0.7365892579227342, 'loss_val': 1.5729097127914429}\n",
            "epoch 28, batch 7200, logs: {'loss': 0.2311421203613281, 'bleu': 0.7365892579227342, 'loss_val': 1.5729097127914429}\n",
            "epoch 28, batch 7250, logs: {'loss': 0.23489260864257813, 'bleu': 0.7365892579227342, 'loss_val': 1.5729097127914429}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) rennt nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 29, batch 7300, logs: {'loss': 0.2539756965637207, 'bleu': 0.7206417078228224, 'loss_val': 1.5685365200042725}\n",
            "epoch 29, batch 7350, logs: {'loss': 0.25826272964477537, 'bleu': 0.7206417078228224, 'loss_val': 1.5685365200042725}\n",
            "epoch 29, batch 7400, logs: {'loss': 0.2568682098388672, 'bleu': 0.7206417078228224, 'loss_val': 1.5685365200042725}\n",
            "epoch 29, batch 7450, logs: {'loss': 0.26000070571899414, 'bleu': 0.7206417078228224, 'loss_val': 1.5685365200042725}\n",
            "finished iterating over dataset, total batches: 7472\n",
            "epoch 29, batch 7500, logs: {'loss': 0.25596554565429686, 'bleu': 0.7206417078228224, 'loss_val': 1.5685365200042725}\n",
            "\n",
            "(input) Tom is cheating. \n",
            "(target) Tom betrügt. \n",
            "(prediction) tom schummelt . <end>\n",
            "\n",
            "\n",
            "(input) They're happy. \n",
            "(target) Sie sind glücklich. \n",
            "(prediction) sie sind glücklich . <end>\n",
            "\n",
            "\n",
            "(input) I'm freaking out. \n",
            "(target) Ich werd noch wahnsinnig. \n",
            "(prediction) l .a . <end>\n",
            "\n",
            "\n",
            "(input) Don't run so fast. \n",
            "(target) Renn nicht so schnell! \n",
            "(prediction) lauf nicht so schnell ! <end>\n",
            "\n",
            "\n",
            "(input) Watch closely. \n",
            "(target) Schau gut zu. \n",
            "(prediction) schauen sie genau zu . <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "finished iterating over dataset, total batches: 7500\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}