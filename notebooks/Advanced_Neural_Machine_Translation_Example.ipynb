{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced Neural Machine Translation Example",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vi8tH5xO_x",
        "colab_type": "text"
      },
      "source": [
        "# Advanced Neural Machine Translation Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiQIr4FSloBt",
        "colab_type": "code",
        "outputId": "aeec15cc-0d03-4422-d3e6-1182a18f7aab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "# Install TensorFlow and also our package via PyPI\n",
        "!pip install tensorflow-gpu==2.0.0\n",
        "!pip install headliner"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 79kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.16.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 30.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed tensorboard-2.0.0 tensorflow-estimator-2.0.0 tensorflow-gpu-2.0.0\n",
            "Collecting headliner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/e4/12578b729080ac8f84fff98a2082fd2a5a8e4043c120e08ec96e6ec0865c/headliner-0.0.13-py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from headliner) (0.21.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from headliner) (3.13)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from headliner) (3.2.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->headliner) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->headliner) (1.16.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->headliner) (0.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->headliner) (1.12.0)\n",
            "Installing collected packages: headliner\n",
            "Successfully installed headliner-0.0.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLWI5oUvJ1St",
        "colab_type": "code",
        "outputId": "18086369-79e9-4e1d-a57c-066752df7289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Download the German-English sentence pairs\n",
        "!wget http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-11 07:44:52--  http://www.manythings.org/anki/deu-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:30::6818:6dc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4541707 (4.3M) [application/zip]\n",
            "Saving to: ‘deu-eng.zip’\n",
            "\n",
            "\rdeu-eng.zip           0%[                    ]       0  --.-KB/s               \rdeu-eng.zip          36%[======>             ]   1.58M  7.91MB/s               \rdeu-eng.zip         100%[===================>]   4.33M  17.0MB/s    in 0.3s    \n",
            "\n",
            "2019-10-11 07:44:52 (17.0 MB/s) - ‘deu-eng.zip’ saved [4541707/4541707]\n",
            "\n",
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYyOWzeep2lU",
        "colab_type": "code",
        "outputId": "0c4859d3-993a-403d-bfe0-35ed03e74d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Create the dataset but only take a subset for faster training\n",
        "import io\n",
        "\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    word_pairs = [[w for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    return zip(*word_pairs)\n",
        "\n",
        "eng, ger = create_dataset('deu.txt', 30000)\n",
        "data = list(zip(ger, eng))\n",
        "data[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hallo!', 'Hi.'),\n",
              " ('Grüß Gott!', 'Hi.'),\n",
              " ('Lauf!', 'Run!'),\n",
              " ('Potzdonner!', 'Wow!'),\n",
              " ('Donnerwetter!', 'Wow!'),\n",
              " ('Feuer!', 'Fire!'),\n",
              " ('Hilfe!', 'Help!'),\n",
              " ('Zu Hülf!', 'Help!'),\n",
              " ('Stopp!', 'Stop!'),\n",
              " ('Warte!', 'Wait!')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPiBB8TCzCVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IotVafsR43wF",
        "colab_type": "code",
        "outputId": "d565ea96-57ee-4760-f666-ca0fbc8b9161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Define custom preprocessing\n",
        "from headliner.preprocessing import Preprocessor\n",
        "\n",
        "preprocessor = Preprocessor(lower_case=True)\n",
        "train_prep = [preprocessor(t) for t in train]\n",
        "train_prep[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ist es nicht lecker ?', \"<start> isn't it delicious ? <end>\"),\n",
              " ('uns wird es gut gehen .', \"<start> we'll be fine . <end>\"),\n",
              " ('das passiert immer wieder .', '<start> it keeps happening . <end>'),\n",
              " ('tom mag bohnen .', '<start> tom likes beans . <end>'),\n",
              " ('ist das ein bär ?', '<start> is that a bear ? <end>')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORr35PWw5i1c",
        "colab_type": "code",
        "outputId": "0a8aab7e-8cf3-4ebb-d048-737a0effe5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Fit custom tokenizers for input and target\n",
        "from tensorflow_datasets.core.features.text import SubwordTextEncoder\n",
        "from headliner.preprocessing import Vectorizer\n",
        "\n",
        "inputs_prep = [t[0] for t in train_prep]\n",
        "targets_prep = [t[1] for t in train_prep]\n",
        "tokenizer_input = SubwordTextEncoder.build_from_corpus(\n",
        "    inputs_prep, target_vocab_size=2**13)\n",
        "tokenizer_target = SubwordTextEncoder.build_from_corpus(\n",
        "    targets_prep, target_vocab_size=2**13, \n",
        "    reserved_tokens=[preprocessor.start_token, preprocessor.end_token])\n",
        "\n",
        "vectorizer = Vectorizer(tokenizer_input, tokenizer_target)\n",
        "'vocab size input {}, target {}'.format(\n",
        "    vectorizer.encoding_dim, vectorizer.decoding_dim)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vocab size input 7311, target 6021'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wd1m6SPClNP",
        "colab_type": "code",
        "outputId": "14cf8882-3517-4a38-ad6c-f656c62ef1ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "# Start tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /tmp/summarizer_tensorboard"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6006\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKpAXaoT7Hds",
        "colab_type": "code",
        "outputId": "4b1edb08-f893-4dda-b83c-a4ea7fff761b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define the model and train it\n",
        "from headliner.model.summarizer_transformer import SummarizerTransformer\n",
        "from headliner.trainer import Trainer\n",
        "\n",
        "summarizer = SummarizerTransformer(num_heads=2,\n",
        "                                   feed_forward_dim=1024,\n",
        "                                   num_layers=1,\n",
        "                                   embedding_size=64,\n",
        "                                   dropout_rate=0.1,\n",
        "                                   max_prediction_len=50)\n",
        "summarizer.init_model(preprocessor, vectorizer)\n",
        "trainer = Trainer(steps_per_epoch=250,\n",
        "                  batch_size=64,\n",
        "                  model_save_path='/tmp/summarizer_transformer',\n",
        "                  tensorboard_dir='/tmp/summarizer_tensorboard',\n",
        "                  steps_to_log=50)\n",
        "trainer.train(summarizer, train, num_epochs=10, val_data=test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training an already initialized model...\n",
            "epoch 0, batch 50, logs: {'loss': 5.006421494483948}\n",
            "epoch 0, batch 100, logs: {'loss': 4.161925711631775}\n",
            "epoch 0, batch 150, logs: {'loss': 3.700190437634786}\n",
            "epoch 0, batch 200, logs: {'loss': 3.361472927927971}\n",
            "epoch 0, batch 250, logs: {'loss': 3.1242762908935546}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i can't you you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) you you you  ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we can't you a  . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom is very  . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who is it  ? <end>\n",
            "\n",
            "loss_val improved from None to 1.7789506912231445, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 1, batch 300, logs: {'loss': 2.9458431458473204, 'loss_val': 1.7789506912231445}\n",
            "epoch 1, batch 350, logs: {'loss': 2.796235908440181, 'loss_val': 1.7789506912231445}\n",
            "epoch 1, batch 400, logs: {'loss': 2.669507239460945, 'loss_val': 1.7789506912231445}\n",
            "epoch 1, batch 450, logs: {'loss': 2.562228073279063, 'loss_val': 1.7789506912231445}\n",
            "finished iterating over dataset, total batches: 467\n",
            "epoch 1, batch 500, logs: {'loss': 2.463265746831894, 'loss_val': 1.7789506912231445}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i have a lot . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you like it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we don't be too fast . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom looks upset . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who is he a lot ? <end>\n",
            "\n",
            "loss_val improved from 1.7789506912231445 to 1.4671252965927124, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 2, batch 550, logs: {'loss': 2.376291231458837, 'loss_val': 1.4671252965927124}\n",
            "epoch 2, batch 600, logs: {'loss': 2.30264226436615, 'loss_val': 1.4671252965927124}\n",
            "epoch 2, batch 650, logs: {'loss': 2.2364292102593644, 'loss_val': 1.4671252965927124}\n",
            "epoch 2, batch 700, logs: {'loss': 2.1778743953364237, 'loss_val': 1.4671252965927124}\n",
            "epoch 2, batch 750, logs: {'loss': 2.123750707467397, 'loss_val': 1.4671252965927124}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i saw you a lot . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you do it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we don't be too fast . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom is wounded . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who is a lot ? <end>\n",
            "\n",
            "loss_val improved from 1.4671252965927124 to 1.2867461442947388, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 3, batch 800, logs: {'loss': 2.072648555189371, 'loss_val': 1.2867461442947388}\n",
            "epoch 3, batch 850, logs: {'loss': 2.0288429798799403, 'loss_val': 1.2867461442947388}\n",
            "epoch 3, batch 900, logs: {'loss': 1.984775690370136, 'loss_val': 1.2867461442947388}\n",
            "finished iterating over dataset, total batches: 934\n",
            "epoch 3, batch 950, logs: {'loss': 1.9434494260737771, 'loss_val': 1.2867461442947388}\n",
            "epoch 3, batch 1000, logs: {'loss': 1.899536397099495, 'loss_val': 1.2867461442947388}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i don't help you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you look it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we're not crazy . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom died off . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who is a lot ? <end>\n",
            "\n",
            "loss_val improved from 1.2867461442947388 to 1.242181420326233, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 4, batch 1050, logs: {'loss': 1.860140148174195, 'loss_val': 1.242181420326233}\n",
            "epoch 4, batch 1100, logs: {'loss': 1.8247337637706236, 'loss_val': 1.242181420326233}\n",
            "epoch 4, batch 1150, logs: {'loss': 1.7894607831602511, 'loss_val': 1.242181420326233}\n",
            "epoch 4, batch 1200, logs: {'loss': 1.7559678439795972, 'loss_val': 1.242181420326233}\n",
            "epoch 4, batch 1250, logs: {'loss': 1.7263502437591554, 'loss_val': 1.242181420326233}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you look it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no more . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom died of fish . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who is on a student ? <end>\n",
            "\n",
            "loss_val improved from 1.242181420326233 to 1.1078895330429077, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 5, batch 1300, logs: {'loss': 1.695499902826089, 'loss_val': 1.1078895330429077}\n",
            "epoch 5, batch 1350, logs: {'loss': 1.6676966820822823, 'loss_val': 1.1078895330429077}\n",
            "epoch 5, batch 1400, logs: {'loss': 1.6419969955938203, 'loss_val': 1.1078895330429077}\n",
            "finished iterating over dataset, total batches: 1401\n",
            "epoch 5, batch 1450, logs: {'loss': 1.6123528718126232, 'loss_val': 1.1078895330429077}\n",
            "epoch 5, batch 1500, logs: {'loss': 1.5847502884467444, 'loss_val': 1.1078895330429077}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no crazy . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom died out . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who is a bit child ? <end>\n",
            "\n",
            "loss_val improved from 1.1078895330429077 to 1.0749753713607788, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 6, batch 1550, logs: {'loss': 1.5585406379930435, 'loss_val': 1.0749753713607788}\n",
            "epoch 6, batch 1600, logs: {'loss': 1.5339924963749945, 'loss_val': 1.0749753713607788}\n",
            "epoch 6, batch 1650, logs: {'loss': 1.5104518979426587, 'loss_val': 1.0749753713607788}\n",
            "epoch 6, batch 1700, logs: {'loss': 1.4873170433500233, 'loss_val': 1.0749753713607788}\n",
            "epoch 6, batch 1750, logs: {'loss': 1.4662253492389405, 'loss_val': 1.0749753713607788}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we don't have tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom threw up . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who is on mary going ? <end>\n",
            "\n",
            "loss_val improved from 1.0749753713607788 to 1.0551031827926636, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 7, batch 1800, logs: {'loss': 1.4461373987793922, 'loss_val': 1.0551031827926636}\n",
            "epoch 7, batch 1850, logs: {'loss': 1.4255273054902617, 'loss_val': 1.0551031827926636}\n",
            "finished iterating over dataset, total batches: 1868\n",
            "epoch 7, batch 1900, logs: {'loss': 1.4048986061309514, 'loss_val': 1.0551031827926636}\n",
            "epoch 7, batch 1950, logs: {'loss': 1.3840673714876175, 'loss_val': 1.0551031827926636}\n",
            "epoch 7, batch 2000, logs: {'loss': 1.3639887059777975, 'loss_val': 1.0551031827926636}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom threw up . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val improved from 1.0551031827926636 to 1.0373717546463013, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 8, batch 2050, logs: {'loss': 1.3454356492583344, 'loss_val': 1.0373717546463013}\n",
            "epoch 8, batch 2100, logs: {'loss': 1.3273204937293417, 'loss_val': 1.0373717546463013}\n",
            "epoch 8, batch 2150, logs: {'loss': 1.3104525678934054, 'loss_val': 1.0373717546463013}\n",
            "epoch 8, batch 2200, logs: {'loss': 1.2933452883227305, 'loss_val': 1.0373717546463013}\n",
            "epoch 8, batch 2250, logs: {'loss': 1.277444358944893, 'loss_val': 1.0373717546463013}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-549e62321a1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                   \u001b[0mtensorboard_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/tmp/summarizer_tensorboard'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                   steps_to_log=50)\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/headliner/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, summarizer, train_data, val_data, num_epochs, scorers, callbacks)\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m                     \u001b[0mepoch_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mepoch_count\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/headliner/callbacks/evaluation_callback.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mcount_val\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount_val\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_num_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 self.logger.info('\\n(input) {} \\n(target) {} \\n(prediction) {}\\n'.format(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/headliner/model/summarizer_transformer.py\u001b[0m in \u001b[0;36mpredict_vectors\u001b[0;34m(self, input_text, target_text)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                                               \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                                               \u001b[0mcombined_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                                                               dec_padding_mask)\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/headliner/model/summarizer_transformer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         dec_output, attention_weights = self.decoder(\n\u001b[0;32m--> 286\u001b[0;31m             tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, tar_seq_len, target_vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/headliner/model/summarizer_transformer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, enc_output, training, look_ahead_mask, padding_mask)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n\u001b[0;32m--> 241\u001b[0;31m                                                    look_ahead_mask, padding_mask)\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mattention_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder_layer{}_block1'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mattention_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder_layer{}_block2'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/headliner/model/summarizer_transformer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, enc_output, training, look_ahead_mask, padding_mask)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mattn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mattn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights_block2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mattn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/headliner/model/summarizer_transformer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, v, k, q, mask)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mscaled_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mscaled_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mconcat_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/headliner/model/summarizer_transformer.py\u001b[0m in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(q, k, v, mask)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmatmul_qk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mdk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mscaled_attention_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul_qk\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m    702\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[1;32m   2197\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cast\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DstT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDstT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Truncate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2199\u001b[0;31m         Truncate)\n\u001b[0m\u001b[1;32m   2200\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2201\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKYgDLngooL1",
        "colab_type": "code",
        "outputId": "db39c5d8-affb-427e-d419-77627bf14c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load best model and do some prediction\n",
        "best_summarizer = SummarizerTransformer.load('/tmp/summarizer_transformer')\n",
        "best_summarizer.predict('Mögen Sie Roboter?')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'do you like robots ? <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpOX8NweyrHN",
        "colab_type": "code",
        "outputId": "7145cb07-52d8-4645-dabb-04da425b71bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        }
      },
      "source": [
        "# Plot attention alignment for a prediction\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_attention_weights(summarizer, pred_vectors, layer_name):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "    input_text, _ = pred_vectors['preprocessed_text']\n",
        "    input_sequence = summarizer.vectorizer.encode_input(input_text)\n",
        "    pred_sequence = pred_vectors['predicted_sequence']\n",
        "    attention = tf.squeeze(pred_vectors['attention_weights'][layer_name])\n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(1, 2, head + 1)\n",
        "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "        fontdict = {'fontsize': 10}\n",
        "        ax.set_xticks(range(len(input_sequence)))\n",
        "        ax.set_yticks(range(len(pred_sequence)))\n",
        "        ax.set_ylim(len(pred_sequence) - 1.5, -0.5)\n",
        "        ax.set_xticklabels(\n",
        "            [summarizer.vectorizer.decode_input([i]) for i in input_sequence],\n",
        "            fontdict=fontdict,\n",
        "            rotation=90)\n",
        "        ax.set_yticklabels([summarizer.vectorizer.decode_output([i]) \n",
        "                            for i in pred_sequence], fontdict=fontdict)\n",
        "        ax.set_xlabel('Head {}'.format(head + 1))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "pred_vectors = best_summarizer.predict_vectors(\n",
        "    'Tom rannte aus dem brennenden Haus.', '')\n",
        "plot_attention_weights(best_summarizer, pred_vectors, 'decoder_layer1_block2')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABHgAAAITCAYAAAB8EavgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCh4AAACAySl4AAAAACb3vwEn5OtLDkECfgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1152x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u62rjlcu1AKv",
        "colab_type": "code",
        "outputId": "e881d42e-2b74-4d40-a967-b93056425573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Continue training to improve the model and check the BLEU score\n",
        "from headliner.evaluation import BleuScorer\n",
        "\n",
        "bleu_scorer = BleuScorer(tokens_to_ignore=[preprocessor.start_token, \n",
        "                                           preprocessor.end_token])\n",
        "trainer.train(best_summarizer, \n",
        "              train, \n",
        "              num_epochs=30, \n",
        "              val_data=test, \n",
        "              scorers={'bleu': bleu_scorer})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training an already initialized model...\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0, batch 50, logs: {'loss': 0.5587309688329697}\n",
            "epoch 0, batch 100, logs: {'loss': 0.556186862885952}\n",
            "epoch 0, batch 150, logs: {'loss': 0.5621454254786173}\n",
            "epoch 0, batch 200, logs: {'loss': 0.5577866177260876}\n",
            "epoch 0, batch 250, logs: {'loss': 0.5561979472637176}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom vomited down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on on facebook ? <end>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss_val improved from None to 1.0644575357437134, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 1, batch 300, logs: {'loss': 0.5549514476458232, 'bleu': 0.7600504209318474, 'loss_val': 1.0644575357437134}\n",
            "epoch 1, batch 350, logs: {'loss': 0.5535186570031302, 'bleu': 0.7600504209318474, 'loss_val': 1.0644575357437134}\n",
            "epoch 1, batch 400, logs: {'loss': 0.5539342339336872, 'bleu': 0.7600504209318474, 'loss_val': 1.0644575357437134}\n",
            "epoch 1, batch 450, logs: {'loss': 0.5505019794570075, 'bleu': 0.7600504209318474, 'loss_val': 1.0644575357437134}\n",
            "finished iterating over dataset, total batches: 467\n",
            "epoch 1, batch 500, logs: {'loss': 0.5432471603751182, 'bleu': 0.7600504209318474, 'loss_val': 1.0644575357437134}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we don't have tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom threw up . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val improved from 1.0644575357437134 to 1.0220704078674316, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 2, batch 550, logs: {'loss': 0.5343290437893434, 'bleu': 0.7404074347913476, 'loss_val': 1.0220704078674316}\n",
            "epoch 2, batch 600, logs: {'loss': 0.5257346551616987, 'bleu': 0.7404074347913476, 'loss_val': 1.0220704078674316}\n",
            "epoch 2, batch 650, logs: {'loss': 0.5188830205568901, 'bleu': 0.7404074347913476, 'loss_val': 1.0220704078674316}\n",
            "epoch 2, batch 700, logs: {'loss': 0.5128824406010764, 'bleu': 0.7404074347913476, 'loss_val': 1.0220704078674316}\n",
            "epoch 2, batch 750, logs: {'loss': 0.5084660832484563, 'bleu': 0.7404074347913476, 'loss_val': 1.0220704078674316}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no more . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom threw up . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 3, batch 800, logs: {'loss': 0.5069817974418401, 'bleu': 0.7327558442919055, 'loss_val': 1.0762815475463867}\n",
            "epoch 3, batch 850, logs: {'loss': 0.5028255539781907, 'bleu': 0.7327558442919055, 'loss_val': 1.0762815475463867}\n",
            "epoch 3, batch 900, logs: {'loss': 0.4986840520964728, 'bleu': 0.7327558442919055, 'loss_val': 1.0762815475463867}\n",
            "finished iterating over dataset, total batches: 934\n",
            "epoch 3, batch 950, logs: {'loss': 0.4940117668634967, 'bleu': 0.7327558442919055, 'loss_val': 1.0762815475463867}\n",
            "epoch 3, batch 1000, logs: {'loss': 0.4861641818881035, 'bleu': 0.7327558442919055, 'loss_val': 1.0762815475463867}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom blacked out . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val improved from 1.0220704078674316 to 1.0210126638412476, saving summarizer to /tmp/summarizer_transformer\n",
            "epoch 4, batch 1050, logs: {'loss': 0.479444660118648, 'bleu': 0.7726316181090864, 'loss_val': 1.0210126638412476}\n",
            "epoch 4, batch 1100, logs: {'loss': 0.47443264795975254, 'bleu': 0.7726316181090864, 'loss_val': 1.0210126638412476}\n",
            "epoch 4, batch 1150, logs: {'loss': 0.4702241519223089, 'bleu': 0.7726316181090864, 'loss_val': 1.0210126638412476}\n",
            "epoch 4, batch 1200, logs: {'loss': 0.4663383820280433, 'bleu': 0.7726316181090864, 'loss_val': 1.0210126638412476}\n",
            "epoch 4, batch 1250, logs: {'loss': 0.4625765904426575, 'bleu': 0.7726316181090864, 'loss_val': 1.0210126638412476}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom passed out . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 5, batch 1300, logs: {'loss': 0.45881528332829474, 'bleu': 0.7616286693351025, 'loss_val': 1.039923071861267}\n",
            "epoch 5, batch 1350, logs: {'loss': 0.45628740347094005, 'bleu': 0.7616286693351025, 'loss_val': 1.039923071861267}\n",
            "epoch 5, batch 1400, logs: {'loss': 0.4534576527029276, 'bleu': 0.7616286693351025, 'loss_val': 1.039923071861267}\n",
            "finished iterating over dataset, total batches: 1401\n",
            "epoch 5, batch 1450, logs: {'loss': 0.44829773808347767, 'bleu': 0.7616286693351025, 'loss_val': 1.039923071861267}\n",
            "epoch 5, batch 1500, logs: {'loss': 0.4439090789953868, 'bleu': 0.7616286693351025, 'loss_val': 1.039923071861267}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's dieting ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 6, batch 1550, logs: {'loss': 0.43907742500305175, 'bleu': 0.7570796522702701, 'loss_val': 1.0751205682754517}\n",
            "epoch 6, batch 1600, logs: {'loss': 0.4348169266153127, 'bleu': 0.7570796522702701, 'loss_val': 1.0751205682754517}\n",
            "epoch 6, batch 1650, logs: {'loss': 0.43066693271651413, 'bleu': 0.7570796522702701, 'loss_val': 1.0751205682754517}\n",
            "epoch 6, batch 1700, logs: {'loss': 0.42732976582996984, 'bleu': 0.7570796522702701, 'loss_val': 1.0751205682754517}\n",
            "epoch 6, batch 1750, logs: {'loss': 0.42414534550053734, 'bleu': 0.7570796522702701, 'loss_val': 1.0751205682754517}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom blacked out . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who is on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 7, batch 1800, logs: {'loss': 0.42160764257113137, 'bleu': 0.756232813833424, 'loss_val': 1.0726872682571411}\n",
            "epoch 7, batch 1850, logs: {'loss': 0.41939051172217806, 'bleu': 0.756232813833424, 'loss_val': 1.0726872682571411}\n",
            "finished iterating over dataset, total batches: 1868\n",
            "epoch 7, batch 1900, logs: {'loss': 0.41558517718785687, 'bleu': 0.756232813833424, 'loss_val': 1.0726872682571411}\n",
            "epoch 7, batch 1950, logs: {'loss': 0.41137015927296416, 'bleu': 0.756232813833424, 'loss_val': 1.0726872682571411}\n",
            "epoch 7, batch 2000, logs: {'loss': 0.4074586843252182, 'bleu': 0.756232813833424, 'loss_val': 1.0726872682571411}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we're out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom blacked out . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 8, batch 2050, logs: {'loss': 0.4040225774558579, 'bleu': 0.7651573119802528, 'loss_val': 1.0824254751205444}\n",
            "epoch 8, batch 2100, logs: {'loss': 0.4008005027331057, 'bleu': 0.7651573119802528, 'loss_val': 1.0824254751205444}\n",
            "epoch 8, batch 2150, logs: {'loss': 0.39807122579147647, 'bleu': 0.7651573119802528, 'loss_val': 1.0824254751205444}\n",
            "epoch 8, batch 2200, logs: {'loss': 0.3959856840832667, 'bleu': 0.7651573119802528, 'loss_val': 1.0824254751205444}\n",
            "epoch 8, batch 2250, logs: {'loss': 0.39339953710635506, 'bleu': 0.7651573119802528, 'loss_val': 1.0824254751205444}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we're out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's dieting ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 9, batch 2300, logs: {'loss': 0.391100864339134, 'bleu': 0.7701075881046161, 'loss_val': 1.0965017080307007}\n",
            "finished iterating over dataset, total batches: 2335\n",
            "epoch 9, batch 2350, logs: {'loss': 0.3887223929420431, 'bleu': 0.7701075881046161, 'loss_val': 1.0965017080307007}\n",
            "epoch 9, batch 2400, logs: {'loss': 0.3853697891657551, 'bleu': 0.7701075881046161, 'loss_val': 1.0965017080307007}\n",
            "epoch 9, batch 2450, logs: {'loss': 0.3821075283872838, 'bleu': 0.7701075881046161, 'loss_val': 1.0965017080307007}\n",
            "epoch 9, batch 2500, logs: {'loss': 0.37903268740177154, 'bleu': 0.7701075881046161, 'loss_val': 1.0965017080307007}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom aced the seat . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 10, batch 2550, logs: {'loss': 0.376129190483514, 'bleu': 0.7810953939743716, 'loss_val': 1.0927358865737915}\n",
            "epoch 10, batch 2600, logs: {'loss': 0.3736852291398324, 'bleu': 0.7810953939743716, 'loss_val': 1.0927358865737915}\n",
            "epoch 10, batch 2650, logs: {'loss': 0.3714516196498331, 'bleu': 0.7810953939743716, 'loss_val': 1.0927358865737915}\n",
            "epoch 10, batch 2700, logs: {'loss': 0.3694960246704243, 'bleu': 0.7810953939743716, 'loss_val': 1.0927358865737915}\n",
            "epoch 10, batch 2750, logs: {'loss': 0.3675096230506897, 'bleu': 0.7810953939743716, 'loss_val': 1.0927358865737915}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy a lot . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we're out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom is squatting . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 11, batch 2800, logs: {'loss': 0.3658851916928377, 'bleu': 0.7850323565201647, 'loss_val': 1.1331713199615479}\n",
            "finished iterating over dataset, total batches: 2802\n",
            "epoch 11, batch 2850, logs: {'loss': 0.3627376305037423, 'bleu': 0.7850323565201647, 'loss_val': 1.1331713199615479}\n",
            "epoch 11, batch 2900, logs: {'loss': 0.36010506581386614, 'bleu': 0.7850323565201647, 'loss_val': 1.1331713199615479}\n",
            "epoch 11, batch 2950, logs: {'loss': 0.35758864090856857, 'bleu': 0.7850323565201647, 'loss_val': 1.1331713199615479}\n",
            "epoch 11, batch 3000, logs: {'loss': 0.3553238876735171, 'bleu': 0.7850323565201647, 'loss_val': 1.1331713199615479}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's dieting ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 12, batch 3050, logs: {'loss': 0.3533243194833154, 'bleu': 0.7731955036122234, 'loss_val': 1.1058826446533203}\n",
            "epoch 12, batch 3100, logs: {'loss': 0.3514732309334701, 'bleu': 0.7731955036122234, 'loss_val': 1.1058826446533203}\n",
            "epoch 12, batch 3150, logs: {'loss': 0.34948994903573916, 'bleu': 0.7731955036122234, 'loss_val': 1.1058826446533203}\n",
            "epoch 12, batch 3200, logs: {'loss': 0.34770682113012297, 'bleu': 0.7731955036122234, 'loss_val': 1.1058826446533203}\n",
            "epoch 12, batch 3250, logs: {'loss': 0.3460257792312365, 'bleu': 0.7731955036122234, 'loss_val': 1.1058826446533203}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we don't have tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who is dieting ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "finished iterating over dataset, total batches: 3269\n",
            "epoch 13, batch 3300, logs: {'loss': 0.34401260943349565, 'bleu': 0.7920555476628286, 'loss_val': 1.1219788789749146}\n",
            "epoch 13, batch 3350, logs: {'loss': 0.3416761301457882, 'bleu': 0.7920555476628286, 'loss_val': 1.1219788789749146}\n",
            "epoch 13, batch 3400, logs: {'loss': 0.33955187011510135, 'bleu': 0.7920555476628286, 'loss_val': 1.1219788789749146}\n",
            "epoch 13, batch 3450, logs: {'loss': 0.33754207839352496, 'bleu': 0.7920555476628286, 'loss_val': 1.1219788789749146}\n",
            "epoch 13, batch 3500, logs: {'loss': 0.33572514821588995, 'bleu': 0.7920555476628286, 'loss_val': 1.1219788789749146}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 14, batch 3550, logs: {'loss': 0.33393511200244996, 'bleu': 0.7813728461040277, 'loss_val': 1.1328518390655518}\n",
            "epoch 14, batch 3600, logs: {'loss': 0.33235272804275157, 'bleu': 0.7813728461040277, 'loss_val': 1.1328518390655518}\n",
            "epoch 14, batch 3650, logs: {'loss': 0.3308610851572801, 'bleu': 0.7813728461040277, 'loss_val': 1.1328518390655518}\n",
            "epoch 14, batch 3700, logs: {'loss': 0.32935419369589636, 'bleu': 0.7813728461040277, 'loss_val': 1.1328518390655518}\n",
            "finished iterating over dataset, total batches: 3736\n",
            "epoch 14, batch 3750, logs: {'loss': 0.32771197078824044, 'bleu': 0.7813728461040277, 'loss_val': 1.1328518390655518}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's dieting ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 15, batch 3800, logs: {'loss': 0.32573771980248, 'bleu': 0.7785154960610141, 'loss_val': 1.1486603021621704}\n",
            "epoch 15, batch 3850, logs: {'loss': 0.3239137635525171, 'bleu': 0.7785154960610141, 'loss_val': 1.1486603021621704}\n",
            "epoch 15, batch 3900, logs: {'loss': 0.3221649607251852, 'bleu': 0.7785154960610141, 'loss_val': 1.1486603021621704}\n",
            "epoch 15, batch 3950, logs: {'loss': 0.32049906844579723, 'bleu': 0.7785154960610141, 'loss_val': 1.1486603021621704}\n",
            "epoch 15, batch 4000, logs: {'loss': 0.3191118339449167, 'bleu': 0.7785154960610141, 'loss_val': 1.1486603021621704}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we're out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 16, batch 4050, logs: {'loss': 0.3176610787120866, 'bleu': 0.8050788591266254, 'loss_val': 1.1492104530334473}\n",
            "epoch 16, batch 4100, logs: {'loss': 0.3163202376300242, 'bleu': 0.8050788591266254, 'loss_val': 1.1492104530334473}\n",
            "epoch 16, batch 4150, logs: {'loss': 0.3149902349495026, 'bleu': 0.8050788591266254, 'loss_val': 1.1492104530334473}\n",
            "epoch 16, batch 4200, logs: {'loss': 0.31374079038344677, 'bleu': 0.8050788591266254, 'loss_val': 1.1492104530334473}\n",
            "finished iterating over dataset, total batches: 4203\n",
            "epoch 16, batch 4250, logs: {'loss': 0.3120031962885576, 'bleu': 0.8050788591266254, 'loss_val': 1.1492104530334473}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we're out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 17, batch 4300, logs: {'loss': 0.3103059775260992, 'bleu': 0.8023974267142049, 'loss_val': 1.139438509941101}\n",
            "epoch 17, batch 4350, logs: {'loss': 0.3087831651410837, 'bleu': 0.8023974267142049, 'loss_val': 1.139438509941101}\n",
            "epoch 17, batch 4400, logs: {'loss': 0.3072723699632016, 'bleu': 0.8023974267142049, 'loss_val': 1.139438509941101}\n",
            "epoch 17, batch 4450, logs: {'loss': 0.30585171021604807, 'bleu': 0.8023974267142049, 'loss_val': 1.139438509941101}\n",
            "epoch 17, batch 4500, logs: {'loss': 0.3046315490321981, 'bleu': 0.8023974267142049, 'loss_val': 1.139438509941101}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 18, batch 4550, logs: {'loss': 0.30354311025568415, 'bleu': 0.7913934429589637, 'loss_val': 1.1805062294006348}\n",
            "epoch 18, batch 4600, logs: {'loss': 0.302455000916253, 'bleu': 0.7913934429589637, 'loss_val': 1.1805062294006348}\n",
            "epoch 18, batch 4650, logs: {'loss': 0.301323772101633, 'bleu': 0.7913934429589637, 'loss_val': 1.1805062294006348}\n",
            "finished iterating over dataset, total batches: 4670\n",
            "epoch 18, batch 4700, logs: {'loss': 0.2999312330750709, 'bleu': 0.7913934429589637, 'loss_val': 1.1805062294006348}\n",
            "epoch 18, batch 4750, logs: {'loss': 0.29846170394514737, 'bleu': 0.7913934429589637, 'loss_val': 1.1805062294006348}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 19, batch 4800, logs: {'loss': 0.29702145266967517, 'bleu': 0.797567586253561, 'loss_val': 1.1835784912109375}\n",
            "epoch 19, batch 4850, logs: {'loss': 0.29570448390755455, 'bleu': 0.797567586253561, 'loss_val': 1.1835784912109375}\n",
            "epoch 19, batch 4900, logs: {'loss': 0.29452142490264105, 'bleu': 0.797567586253561, 'loss_val': 1.1835784912109375}\n",
            "epoch 19, batch 4950, logs: {'loss': 0.2933111530617632, 'bleu': 0.797567586253561, 'loss_val': 1.1835784912109375}\n",
            "epoch 19, batch 5000, logs: {'loss': 0.29220969214588405, 'bleu': 0.797567586253561, 'loss_val': 1.1835784912109375}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's dieting ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 20, batch 5050, logs: {'loss': 0.29126607010860256, 'bleu': 0.7967150469461848, 'loss_val': 1.1803663969039917}\n",
            "epoch 20, batch 5100, logs: {'loss': 0.2903460413772686, 'bleu': 0.7967150469461848, 'loss_val': 1.1803663969039917}\n",
            "finished iterating over dataset, total batches: 5137\n",
            "epoch 20, batch 5150, logs: {'loss': 0.28933645481912834, 'bleu': 0.7967150469461848, 'loss_val': 1.1803663969039917}\n",
            "epoch 20, batch 5200, logs: {'loss': 0.28798310216372974, 'bleu': 0.7967150469461848, 'loss_val': 1.1803663969039917}\n",
            "epoch 20, batch 5250, logs: {'loss': 0.28671339238683385, 'bleu': 0.7967150469461848, 'loss_val': 1.1803663969039917}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you a lot . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we're out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's dieting ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 21, batch 5300, logs: {'loss': 0.285587624336751, 'bleu': 0.7919103285295571, 'loss_val': 1.2154202461242676}\n",
            "epoch 21, batch 5350, logs: {'loss': 0.2845068480617532, 'bleu': 0.7919103285295571, 'loss_val': 1.2154202461242676}\n",
            "epoch 21, batch 5400, logs: {'loss': 0.28346650528135126, 'bleu': 0.7919103285295571, 'loss_val': 1.2154202461242676}\n",
            "epoch 21, batch 5450, logs: {'loss': 0.2824321189153632, 'bleu': 0.7919103285295571, 'loss_val': 1.2154202461242676}\n",
            "epoch 21, batch 5500, logs: {'loss': 0.28147063005647877, 'bleu': 0.7919103285295571, 'loss_val': 1.2154202461242676}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we don't have tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 22, batch 5550, logs: {'loss': 0.28048882486315463, 'bleu': 0.7803222311480553, 'loss_val': 1.2287248373031616}\n",
            "epoch 22, batch 5600, logs: {'loss': 0.2796354146408183, 'bleu': 0.7803222311480553, 'loss_val': 1.2287248373031616}\n",
            "finished iterating over dataset, total batches: 5604\n",
            "epoch 22, batch 5650, logs: {'loss': 0.2784420133586478, 'bleu': 0.7803222311480553, 'loss_val': 1.2287248373031616}\n",
            "epoch 22, batch 5700, logs: {'loss': 0.2773968308013782, 'bleu': 0.7803222311480553, 'loss_val': 1.2287248373031616}\n",
            "epoch 22, batch 5750, logs: {'loss': 0.27635498902460803, 'bleu': 0.7803222311480553, 'loss_val': 1.2287248373031616}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we have no tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 23, batch 5800, logs: {'loss': 0.27539178899778377, 'bleu': 0.7810055514126248, 'loss_val': 1.23611581325531}\n",
            "epoch 23, batch 5850, logs: {'loss': 0.27438031504042126, 'bleu': 0.7810055514126248, 'loss_val': 1.23611581325531}\n",
            "epoch 23, batch 5900, logs: {'loss': 0.27338720315593784, 'bleu': 0.7810055514126248, 'loss_val': 1.23611581325531}\n",
            "epoch 23, batch 5950, logs: {'loss': 0.27251359566920946, 'bleu': 0.7810055514126248, 'loss_val': 1.23611581325531}\n",
            "epoch 23, batch 6000, logs: {'loss': 0.2717206691702207, 'bleu': 0.7810055514126248, 'loss_val': 1.23611581325531}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we're out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's dieting ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 24, batch 6050, logs: {'loss': 0.27093918885697016, 'bleu': 0.8053236933337257, 'loss_val': 1.2507812976837158}\n",
            "finished iterating over dataset, total batches: 6071\n",
            "epoch 24, batch 6100, logs: {'loss': 0.2699716776304069, 'bleu': 0.8053236933337257, 'loss_val': 1.2507812976837158}\n",
            "epoch 24, batch 6150, logs: {'loss': 0.2688882511296893, 'bleu': 0.8053236933337257, 'loss_val': 1.2507812976837158}\n",
            "epoch 24, batch 6200, logs: {'loss': 0.26792069883836855, 'bleu': 0.8053236933337257, 'loss_val': 1.2507812976837158}\n",
            "epoch 24, batch 6250, logs: {'loss': 0.2669989850139618, 'bleu': 0.8053236933337257, 'loss_val': 1.2507812976837158}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we're out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's dieting ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 25, batch 6300, logs: {'loss': 0.26607421661653213, 'bleu': 0.785600680723945, 'loss_val': 1.2620222568511963}\n",
            "epoch 25, batch 6350, logs: {'loss': 0.2652746983165816, 'bleu': 0.785600680723945, 'loss_val': 1.2620222568511963}\n",
            "epoch 25, batch 6400, logs: {'loss': 0.26455234809662215, 'bleu': 0.785600680723945, 'loss_val': 1.2620222568511963}\n",
            "epoch 25, batch 6450, logs: {'loss': 0.2638601735615453, 'bleu': 0.785600680723945, 'loss_val': 1.2620222568511963}\n",
            "epoch 25, batch 6500, logs: {'loss': 0.2631987105642374, 'bleu': 0.785600680723945, 'loss_val': 1.2620222568511963}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you a lot . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we're out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "finished iterating over dataset, total batches: 6538\n",
            "epoch 26, batch 6550, logs: {'loss': 0.26246723919196896, 'bleu': 0.7773721401973812, 'loss_val': 1.2859407663345337}\n",
            "epoch 26, batch 6600, logs: {'loss': 0.2615115770810481, 'bleu': 0.7773721401973812, 'loss_val': 1.2859407663345337}\n",
            "epoch 26, batch 6650, logs: {'loss': 0.2605641796311041, 'bleu': 0.7773721401973812, 'loss_val': 1.2859407663345337}\n",
            "epoch 26, batch 6700, logs: {'loss': 0.25971575281290865, 'bleu': 0.7773721401973812, 'loss_val': 1.2859407663345337}\n",
            "epoch 26, batch 6750, logs: {'loss': 0.2588886762658755, 'bleu': 0.7773721401973812, 'loss_val': 1.2859407663345337}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we ran out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 27, batch 6800, logs: {'loss': 0.25809315662831067, 'bleu': 0.792597890021153, 'loss_val': 1.2567238807678223}\n",
            "epoch 27, batch 6850, logs: {'loss': 0.2573762638159912, 'bleu': 0.792597890021153, 'loss_val': 1.2567238807678223}\n",
            "epoch 27, batch 6900, logs: {'loss': 0.25672591163736325, 'bleu': 0.792597890021153, 'loss_val': 1.2567238807678223}\n",
            "epoch 27, batch 6950, logs: {'loss': 0.2561472928063046, 'bleu': 0.792597890021153, 'loss_val': 1.2567238807678223}\n",
            "epoch 27, batch 7000, logs: {'loss': 0.2555091213041118, 'bleu': 0.792597890021153, 'loss_val': 1.2567238807678223}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we ran out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who is dieting ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "finished iterating over dataset, total batches: 7005\n",
            "epoch 28, batch 7050, logs: {'loss': 0.25469513151873935, 'bleu': 0.7889861295425203, 'loss_val': 1.2314354181289673}\n",
            "epoch 28, batch 7100, logs: {'loss': 0.2538859010172982, 'bleu': 0.7889861295425203, 'loss_val': 1.2314354181289673}\n",
            "epoch 28, batch 7150, logs: {'loss': 0.25310151039928824, 'bleu': 0.7889861295425203, 'loss_val': 1.2314354181289673}\n",
            "epoch 28, batch 7200, logs: {'loss': 0.2523275102747397, 'bleu': 0.7889861295425203, 'loss_val': 1.2314354181289673}\n",
            "epoch 28, batch 7250, logs: {'loss': 0.25163009591452007, 'bleu': 0.7889861295425203, 'loss_val': 1.2314354181289673}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you a lot . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we're out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "epoch 29, batch 7300, logs: {'loss': 0.25095251871298435, 'bleu': 0.7897653157863425, 'loss_val': 1.2487125396728516}\n",
            "epoch 29, batch 7350, logs: {'loss': 0.25034167379749067, 'bleu': 0.7897653157863425, 'loss_val': 1.2487125396728516}\n",
            "epoch 29, batch 7400, logs: {'loss': 0.24969921670552042, 'bleu': 0.7897653157863425, 'loss_val': 1.2487125396728516}\n",
            "epoch 29, batch 7450, logs: {'loss': 0.2491327713699949, 'bleu': 0.7897653157863425, 'loss_val': 1.2487125396728516}\n",
            "finished iterating over dataset, total batches: 7472\n",
            "epoch 29, batch 7500, logs: {'loss': 0.24840269303917883, 'bleu': 0.7897653157863425, 'loss_val': 1.2487125396728516}\n",
            "\n",
            "(input) Ich beneide dich sehr. \n",
            "(target) I envy you so much. \n",
            "(prediction) i envy you . <end>\n",
            "\n",
            "\n",
            "(input) Siehst du es? \n",
            "(target) Do you see it? \n",
            "(prediction) do you see it ? <end>\n",
            "\n",
            "\n",
            "(input) Wir haben keinen Tee mehr. \n",
            "(target) We're out of tea. \n",
            "(prediction) we're out of tea . <end>\n",
            "\n",
            "\n",
            "(input) Tom hockte sich hin. \n",
            "(target) Tom scrunched down. \n",
            "(prediction) tom squatted down . <end>\n",
            "\n",
            "\n",
            "(input) Wer ist auf Diät? \n",
            "(target) Who's dieting? \n",
            "(prediction) who's on a diet ? <end>\n",
            "\n",
            "loss_val did not improve.\n",
            "finished iterating over dataset, total batches: 7500\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}