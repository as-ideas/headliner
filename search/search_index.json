{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Headliner Headliner is a sequence modeling library that eases the training and in particular, the deployment of custom sequence models for both researchers and developers. You can very easily deploy your models in a few lines of code. It was originally built for our own research to generate headlines from Welt news articles (see figure 1). That's why we chose the name, Headliner. Figure 1: One example from our Welt.de headline generator. Update 21.01.2020 The library now supports fine-tuning pre-trained BERT models with custom preprocessing as in Text Summarization with Pretrained Encoders ! check out this tutorial on colab! \ud83e\udde0 Internals We use sequence-to-sequence (seq2seq) under the hood, an encoder-decoder framework (see figure 2). We provide a very simple interface to train and deploy seq2seq models. Although this library was created internally to generate headlines, you can also use it for other tasks like machine translations, text summarization and many more. Figure 2: Encoder-decoder sequence-to-sequence model. Why Headliner? You may ask why another seq2seq library? There are a couple of them out there already. For example, Facebook has fairseq , Google has seq2seq and there is also OpenNMT . Although those libraries are great, they have a few drawbacks for our use case e.g. the former doesn't focus much on production whereas the Google one is not actively maintained. OpenNMT was the closest one to match our requirements i.e. it has a strong focus on production. However, we didn't like that their workflow (preparing data, training and evaluation) is mainly done via the command line. They also expose a well-defined API though but the complexity there is still too high with too much custom code (see their minimal transformer training example ). Therefore, we built this library for us with the following goals in mind: Easy-to-use API for training and deployment (only a few lines of code) Uses TensorFlow 2.0 with all its new features ( tf.function , tf.keras.layers etc.) Modular classes: text preprocessing, modeling, evaluation Extensible for different encoder-decoder models Works on large text data For more details on the library, read the documentation at: https://as-ideas.github.io/headliner/ Headliner is compatible with Python 3.6 and is distributed under the MIT license. \u2699\ufe0f Installation \u26a0\ufe0f Before installing Headliner, you need to install TensorFlow as we use this as our deep learning framework. For more details on how to install it, have a look at the TensorFlow installation instructions . Then you can install Headliner itself. There are two ways to install Headliner: Install Headliner from PyPI (recommended): pip install headliner Install Headliner from the GitHub source: git clone https://github.com/as-ideas/headliner.git cd headliner python setup.py install \ud83d\udcd6 Usage Training For the training, you need to import one of our provided models or create your own custom one. Then you need to create the dataset, a tuple of input-output sequences, and then train it: from headliner.trainer import Trainer from headliner.model.transformer_summarizer import TransformerSummarizer data = [( 'You are the stars, earth and sky for me!' , 'I love you.' ), ( 'You are great, but I have other plans.' , 'I like you.' )] summarizer = TransformerSummarizer ( embedding_size = 64 , max_prediction_len = 20 ) trainer = Trainer ( batch_size = 2 , steps_per_epoch = 100 ) trainer . train ( summarizer , data , num_epochs = 2 ) summarizer . save ( '/tmp/summarizer' ) Prediction The prediction can be done in a few lines of code: from headliner.model.transformer_summarizer import TransformerSummarizer summarizer = TransformerSummarizer . load ( '/tmp/summarizer' ) summarizer . predict ( 'You are the stars, earth and sky for me!' ) Models Currently available models include a basic encoder-decoder, an encoder-decoder with Luong attention, the transformer and a transformer on top of a pre-trained BERT-model: from headliner.model.basic_summarizer import BasicSummarizer from headliner.model.attention_summarizer import AttentionSummarizer from headliner.model.transformer_summarizer import TransformerSummarizer from headliner.model.bert_summarizer import BertSummarizer basic_summarizer = BasicSummarizer () attention_summarizer = AttentionSummarizer () transformer_summarizer = TransformerSummarizer () bert_summarizer = BertSummarizer () Advanced training Training using a validation split and model checkpointing: from headliner.model.transformer_summarizer import TransformerSummarizer from headliner.trainer import Trainer train_data = [( 'You are the stars, earth and sky for me!' , 'I love you.' ), ( 'You are great, but I have other plans.' , 'I like you.' )] val_data = [( 'You are great, but I have other plans.' , 'I like you.' )] summarizer = TransformerSummarizer ( num_heads = 1 , feed_forward_dim = 512 , num_layers = 1 , embedding_size = 64 , max_prediction_len = 50 ) trainer = Trainer ( batch_size = 8 , steps_per_epoch = 50 , max_vocab_size_encoder = 10000 , max_vocab_size_decoder = 10000 , tensorboard_dir = '/tmp/tensorboard' , model_save_path = '/tmp/summarizer' ) trainer . train ( summarizer , train_data , val_data = val_data , num_epochs = 3 ) Advanced prediction Prediction information such as attention weights and logits can be accessed via predict_vectors returning a dictionary: from headliner.model.transformer_summarizer import TransformerSummarizer summarizer = TransformerSummarizer . load ( '/tmp/summarizer' ) summarizer . predict_vectors ( 'You are the stars, earth and sky for me!' ) Resume training A previously trained summarizer can be loaded and then retrained. In this case the data preprocessing and vectorization is loaded from the model. train_data = [( 'Some new training data.' , 'New data.' )] * 10 summarizer_loaded = TransformerSummarizer . load ( '/tmp/summarizer' ) trainer = Trainer ( batch_size = 2 ) trainer . train ( summarizer_loaded , train_data ) summarizer_loaded . save ( '/tmp/summarizer_retrained' ) Use pretrained GloVe embeddings Embeddings in GloVe format can be injected in to the trainer as follows. Optionally, set the embedding to non-trainable. trainer = Trainer ( embedding_path_encoder = '/tmp/embedding_encoder.txt' , embedding_path_decoder = '/tmp/embedding_decoder.txt' ) # make sure the embedding size matches to the embedding size of the files summarizer = TransformerSummarizer ( embedding_size = 64 , embedding_encoder_trainable = False , embedding_decoder_trainable = False ) Custom preprocessing A model can be initialized with custom preprocessing and tokenization: from headliner.preprocessing.preprocessor import Preprocessor train_data = [( 'Some inputs.' , 'Some outputs.' )] * 10 preprocessor = Preprocessor ( filter_pattern = '' , lower_case = True , hash_numbers = False ) train_prep = [ preprocessor ( t ) for t in train_data ] inputs_prep = [ t [ 0 ] for t in train_prep ] targets_prep = [ t [ 1 ] for t in train_prep ] # Build tf subword tokenizers. Other custom tokenizers can be implemented # by subclassing headliner.preprocessing.Tokenizer from tensorflow_datasets.core.features.text import SubwordTextEncoder tokenizer_input = SubwordTextEncoder . build_from_corpus ( inputs_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) tokenizer_target = SubwordTextEncoder . build_from_corpus ( targets_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) vectorizer = Vectorizer ( tokenizer_input , tokenizer_target ) summarizer = TransformerSummarizer ( embedding_size = 64 , max_prediction_len = 50 ) summarizer . init_model ( preprocessor , vectorizer ) trainer = Trainer ( batch_size = 2 ) trainer . train ( summarizer , train_data , num_epochs = 3 ) Use pre-trained BERT embeddings Pre-trained BERT models can be included as follows. Be aware that pre-trained BERT models are expensive to train and require custom preprocessing! from headliner.preprocessing.bert_preprocessor import BertPreprocessor from spacy.lang.en import English train_data = [( 'Some inputs.' , 'Some outputs.' )] * 10 # use BERT-specific start and end token preprocessor = BertPreprocessor ( nlp = English () train_prep = [ preprocessor ( t ) for t in train_data ] targets_prep = [ t [ 1 ] for t in train_prep ] from tensorflow_datasets.core.features.text import SubwordTextEncoder from transformers import BertTokenizer from headliner.model.bert_summarizer import BertSummarizer # Use a pre-trained BERT embedding and BERT tokenizer for the encoder tokenizer_input = BertTokenizer . from_pretrained ( 'bert-base-uncased' ) tokenizer_target = SubwordTextEncoder . build_from_corpus ( targets_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) vectorizer = BertVectorizer ( tokenizer_input , tokenizer_target ) summarizer = BertSummarizer ( num_heads = 2 , feed_forward_dim = 512 , num_layers_encoder = 0 , num_layers_decoder = 4 , bert_embedding_encoder = 'bert-base-uncased' , embedding_size_encoder = 768 , embedding_size_decoder = 768 , dropout_rate = 0.1 , max_prediction_len = 50 )) summarizer . init_model ( preprocessor , vectorizer ) trainer = Trainer ( batch_size = 2 ) trainer . train ( summarizer , train_data , num_epochs = 3 ) Training on large datasets Large datasets can be handled by using an iterator: def read_data_iteratively (): return (( 'Some inputs.' , 'Some outputs.' ) for _ in range ( 1000 )) class DataIterator : def __iter__ ( self ): return read_data_iteratively () data_iter = DataIterator () summarizer = TransformerSummarizer ( embedding_size = 10 , max_prediction_len = 20 ) trainer = Trainer ( batch_size = 16 , steps_per_epoch = 1000 ) trainer . train ( summarizer , data_iter , num_epochs = 3 ) \ud83e\udd1d Contribute We welcome all kinds of contributions such as new models, new examples and many more. See the Contribution guide for more details. \ud83d\udcdd Cite this work Please cite Headliner in your publications if this is useful for your research. Here is an example BibTeX entry: @misc { axelspringerai2019headliners , title = {Headliner} , author = {Christian Sch\u00e4fer & Dat Tran} , year = {2019} , howpublished = {\\url{https://github.com/as-ideas/headliner}} , } \ud83c\udfd7 Maintainers Christian Sch\u00e4fer, github: cschaefer26 Dat Tran, github: datitran \u00a9 Copyright See LICENSE for details. References Text Summarization with Pretrained Encoders Effective Approaches to Attention-based Neural Machine Translation Acknowlegements https://www.tensorflow.org/tutorials/text/transformer https://github.com/huggingface/transformers https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/","title":"Home"},{"location":"#headliner","text":"Headliner is a sequence modeling library that eases the training and in particular, the deployment of custom sequence models for both researchers and developers. You can very easily deploy your models in a few lines of code. It was originally built for our own research to generate headlines from Welt news articles (see figure 1). That's why we chose the name, Headliner. Figure 1: One example from our Welt.de headline generator.","title":"Headliner"},{"location":"#update-21012020","text":"The library now supports fine-tuning pre-trained BERT models with custom preprocessing as in Text Summarization with Pretrained Encoders ! check out this tutorial on colab!","title":"Update 21.01.2020"},{"location":"#internals","text":"We use sequence-to-sequence (seq2seq) under the hood, an encoder-decoder framework (see figure 2). We provide a very simple interface to train and deploy seq2seq models. Although this library was created internally to generate headlines, you can also use it for other tasks like machine translations, text summarization and many more. Figure 2: Encoder-decoder sequence-to-sequence model.","title":"\ud83e\udde0 Internals"},{"location":"#why-headliner","text":"You may ask why another seq2seq library? There are a couple of them out there already. For example, Facebook has fairseq , Google has seq2seq and there is also OpenNMT . Although those libraries are great, they have a few drawbacks for our use case e.g. the former doesn't focus much on production whereas the Google one is not actively maintained. OpenNMT was the closest one to match our requirements i.e. it has a strong focus on production. However, we didn't like that their workflow (preparing data, training and evaluation) is mainly done via the command line. They also expose a well-defined API though but the complexity there is still too high with too much custom code (see their minimal transformer training example ). Therefore, we built this library for us with the following goals in mind: Easy-to-use API for training and deployment (only a few lines of code) Uses TensorFlow 2.0 with all its new features ( tf.function , tf.keras.layers etc.) Modular classes: text preprocessing, modeling, evaluation Extensible for different encoder-decoder models Works on large text data For more details on the library, read the documentation at: https://as-ideas.github.io/headliner/ Headliner is compatible with Python 3.6 and is distributed under the MIT license.","title":"Why Headliner?"},{"location":"#installation","text":"\u26a0\ufe0f Before installing Headliner, you need to install TensorFlow as we use this as our deep learning framework. For more details on how to install it, have a look at the TensorFlow installation instructions . Then you can install Headliner itself. There are two ways to install Headliner: Install Headliner from PyPI (recommended): pip install headliner Install Headliner from the GitHub source: git clone https://github.com/as-ideas/headliner.git cd headliner python setup.py install","title":"\u2699\ufe0f Installation"},{"location":"#usage","text":"","title":"\ud83d\udcd6 Usage"},{"location":"#training","text":"For the training, you need to import one of our provided models or create your own custom one. Then you need to create the dataset, a tuple of input-output sequences, and then train it: from headliner.trainer import Trainer from headliner.model.transformer_summarizer import TransformerSummarizer data = [( 'You are the stars, earth and sky for me!' , 'I love you.' ), ( 'You are great, but I have other plans.' , 'I like you.' )] summarizer = TransformerSummarizer ( embedding_size = 64 , max_prediction_len = 20 ) trainer = Trainer ( batch_size = 2 , steps_per_epoch = 100 ) trainer . train ( summarizer , data , num_epochs = 2 ) summarizer . save ( '/tmp/summarizer' )","title":"Training"},{"location":"#prediction","text":"The prediction can be done in a few lines of code: from headliner.model.transformer_summarizer import TransformerSummarizer summarizer = TransformerSummarizer . load ( '/tmp/summarizer' ) summarizer . predict ( 'You are the stars, earth and sky for me!' )","title":"Prediction"},{"location":"#models","text":"Currently available models include a basic encoder-decoder, an encoder-decoder with Luong attention, the transformer and a transformer on top of a pre-trained BERT-model: from headliner.model.basic_summarizer import BasicSummarizer from headliner.model.attention_summarizer import AttentionSummarizer from headliner.model.transformer_summarizer import TransformerSummarizer from headliner.model.bert_summarizer import BertSummarizer basic_summarizer = BasicSummarizer () attention_summarizer = AttentionSummarizer () transformer_summarizer = TransformerSummarizer () bert_summarizer = BertSummarizer ()","title":"Models"},{"location":"#advanced-training","text":"Training using a validation split and model checkpointing: from headliner.model.transformer_summarizer import TransformerSummarizer from headliner.trainer import Trainer train_data = [( 'You are the stars, earth and sky for me!' , 'I love you.' ), ( 'You are great, but I have other plans.' , 'I like you.' )] val_data = [( 'You are great, but I have other plans.' , 'I like you.' )] summarizer = TransformerSummarizer ( num_heads = 1 , feed_forward_dim = 512 , num_layers = 1 , embedding_size = 64 , max_prediction_len = 50 ) trainer = Trainer ( batch_size = 8 , steps_per_epoch = 50 , max_vocab_size_encoder = 10000 , max_vocab_size_decoder = 10000 , tensorboard_dir = '/tmp/tensorboard' , model_save_path = '/tmp/summarizer' ) trainer . train ( summarizer , train_data , val_data = val_data , num_epochs = 3 )","title":"Advanced training"},{"location":"#advanced-prediction","text":"Prediction information such as attention weights and logits can be accessed via predict_vectors returning a dictionary: from headliner.model.transformer_summarizer import TransformerSummarizer summarizer = TransformerSummarizer . load ( '/tmp/summarizer' ) summarizer . predict_vectors ( 'You are the stars, earth and sky for me!' )","title":"Advanced prediction"},{"location":"#resume-training","text":"A previously trained summarizer can be loaded and then retrained. In this case the data preprocessing and vectorization is loaded from the model. train_data = [( 'Some new training data.' , 'New data.' )] * 10 summarizer_loaded = TransformerSummarizer . load ( '/tmp/summarizer' ) trainer = Trainer ( batch_size = 2 ) trainer . train ( summarizer_loaded , train_data ) summarizer_loaded . save ( '/tmp/summarizer_retrained' )","title":"Resume training"},{"location":"#use-pretrained-glove-embeddings","text":"Embeddings in GloVe format can be injected in to the trainer as follows. Optionally, set the embedding to non-trainable. trainer = Trainer ( embedding_path_encoder = '/tmp/embedding_encoder.txt' , embedding_path_decoder = '/tmp/embedding_decoder.txt' ) # make sure the embedding size matches to the embedding size of the files summarizer = TransformerSummarizer ( embedding_size = 64 , embedding_encoder_trainable = False , embedding_decoder_trainable = False )","title":"Use pretrained GloVe embeddings"},{"location":"#custom-preprocessing","text":"A model can be initialized with custom preprocessing and tokenization: from headliner.preprocessing.preprocessor import Preprocessor train_data = [( 'Some inputs.' , 'Some outputs.' )] * 10 preprocessor = Preprocessor ( filter_pattern = '' , lower_case = True , hash_numbers = False ) train_prep = [ preprocessor ( t ) for t in train_data ] inputs_prep = [ t [ 0 ] for t in train_prep ] targets_prep = [ t [ 1 ] for t in train_prep ] # Build tf subword tokenizers. Other custom tokenizers can be implemented # by subclassing headliner.preprocessing.Tokenizer from tensorflow_datasets.core.features.text import SubwordTextEncoder tokenizer_input = SubwordTextEncoder . build_from_corpus ( inputs_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) tokenizer_target = SubwordTextEncoder . build_from_corpus ( targets_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) vectorizer = Vectorizer ( tokenizer_input , tokenizer_target ) summarizer = TransformerSummarizer ( embedding_size = 64 , max_prediction_len = 50 ) summarizer . init_model ( preprocessor , vectorizer ) trainer = Trainer ( batch_size = 2 ) trainer . train ( summarizer , train_data , num_epochs = 3 )","title":"Custom preprocessing"},{"location":"#use-pre-trained-bert-embeddings","text":"Pre-trained BERT models can be included as follows. Be aware that pre-trained BERT models are expensive to train and require custom preprocessing! from headliner.preprocessing.bert_preprocessor import BertPreprocessor from spacy.lang.en import English train_data = [( 'Some inputs.' , 'Some outputs.' )] * 10 # use BERT-specific start and end token preprocessor = BertPreprocessor ( nlp = English () train_prep = [ preprocessor ( t ) for t in train_data ] targets_prep = [ t [ 1 ] for t in train_prep ] from tensorflow_datasets.core.features.text import SubwordTextEncoder from transformers import BertTokenizer from headliner.model.bert_summarizer import BertSummarizer # Use a pre-trained BERT embedding and BERT tokenizer for the encoder tokenizer_input = BertTokenizer . from_pretrained ( 'bert-base-uncased' ) tokenizer_target = SubwordTextEncoder . build_from_corpus ( targets_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) vectorizer = BertVectorizer ( tokenizer_input , tokenizer_target ) summarizer = BertSummarizer ( num_heads = 2 , feed_forward_dim = 512 , num_layers_encoder = 0 , num_layers_decoder = 4 , bert_embedding_encoder = 'bert-base-uncased' , embedding_size_encoder = 768 , embedding_size_decoder = 768 , dropout_rate = 0.1 , max_prediction_len = 50 )) summarizer . init_model ( preprocessor , vectorizer ) trainer = Trainer ( batch_size = 2 ) trainer . train ( summarizer , train_data , num_epochs = 3 )","title":"Use pre-trained BERT embeddings"},{"location":"#training-on-large-datasets","text":"Large datasets can be handled by using an iterator: def read_data_iteratively (): return (( 'Some inputs.' , 'Some outputs.' ) for _ in range ( 1000 )) class DataIterator : def __iter__ ( self ): return read_data_iteratively () data_iter = DataIterator () summarizer = TransformerSummarizer ( embedding_size = 10 , max_prediction_len = 20 ) trainer = Trainer ( batch_size = 16 , steps_per_epoch = 1000 ) trainer . train ( summarizer , data_iter , num_epochs = 3 )","title":"Training on large datasets"},{"location":"#contribute","text":"We welcome all kinds of contributions such as new models, new examples and many more. See the Contribution guide for more details.","title":"\ud83e\udd1d Contribute"},{"location":"#cite-this-work","text":"Please cite Headliner in your publications if this is useful for your research. Here is an example BibTeX entry: @misc { axelspringerai2019headliners , title = {Headliner} , author = {Christian Sch\u00e4fer & Dat Tran} , year = {2019} , howpublished = {\\url{https://github.com/as-ideas/headliner}} , }","title":"\ud83d\udcdd Cite this work"},{"location":"#maintainers","text":"Christian Sch\u00e4fer, github: cschaefer26 Dat Tran, github: datitran","title":"\ud83c\udfd7 Maintainers"},{"location":"#copyright","text":"See LICENSE for details.","title":"\u00a9 Copyright"},{"location":"#references","text":"Text Summarization with Pretrained Encoders Effective Approaches to Attention-based Neural Machine Translation","title":"References"},{"location":"#acknowlegements","text":"https://www.tensorflow.org/tutorials/text/transformer https://github.com/huggingface/transformers https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/","title":"Acknowlegements"},{"location":"CONTRIBUTING/","text":"Contribution Guide We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions. Submit Feedback The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels. Fix Bugs: You may look through the GitHub issues for bugs. Implement Features You may look through the GitHub issues for feature requests. Pull Requests (PR) Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our master branch of the original Headliner repo. Documentation Make sure any new function or class you introduce has proper docstrings. Testing We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes. Main Contributor List We maintain a list of main contributors to appreciate all the contributions.","title":"Contribution"},{"location":"CONTRIBUTING/#contribution-guide","text":"We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions.","title":"Contribution Guide"},{"location":"CONTRIBUTING/#submit-feedback","text":"The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels.","title":"Submit Feedback"},{"location":"CONTRIBUTING/#fix-bugs","text":"You may look through the GitHub issues for bugs.","title":"Fix Bugs:"},{"location":"CONTRIBUTING/#implement-features","text":"You may look through the GitHub issues for feature requests.","title":"Implement Features"},{"location":"CONTRIBUTING/#pull-requests-pr","text":"Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our master branch of the original Headliner repo.","title":"Pull Requests (PR)"},{"location":"CONTRIBUTING/#documentation","text":"Make sure any new function or class you introduce has proper docstrings.","title":"Documentation"},{"location":"CONTRIBUTING/#testing","text":"We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes.","title":"Testing"},{"location":"CONTRIBUTING/#main-contributor-list","text":"We maintain a list of main contributors to appreciate all the contributions.","title":"Main Contributor List"},{"location":"LICENSE/","text":"COPYRIGHT Copyright (c) 2019 Axel Springer AI. All rights reserved. LICENSE The MIT License (MIT) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"embeddings/","text":"read_embedding def read_embedding ( file_path , vector_dim ) Reads an embedding file in glove format into a dictionary mapping tokens to vectors. embedding_to_matrix def embedding_to_matrix ( embedding , token_index , embedding_dim ) Converts an embedding dictionary into a weights matrix used to initialize an embedding layer. It ensures that all tokens in the token_index dictionare are mapped to a row, even those that are not contained in the provided embedding dictionary. Unknown tokens are initialized with a random vector with entries between -1 and 1. Args embedding : dictionary mapping tokens to embedding vectors token_index : dictionary mapping tokens to indices that are fed into the embedding layer embedding_dim : size of the embedding vectors","title":"Embeddings"},{"location":"embeddings/#read_embedding","text":"def read_embedding ( file_path , vector_dim ) Reads an embedding file in glove format into a dictionary mapping tokens to vectors.","title":"read_embedding"},{"location":"embeddings/#embedding_to_matrix","text":"def embedding_to_matrix ( embedding , token_index , embedding_dim ) Converts an embedding dictionary into a weights matrix used to initialize an embedding layer. It ensures that all tokens in the token_index dictionare are mapped to a row, even those that are not contained in the provided embedding dictionary. Unknown tokens are initialized with a random vector with entries between -1 and 1.","title":"embedding_to_matrix"},{"location":"embeddings/#args","text":"embedding : dictionary mapping tokens to embedding vectors token_index : dictionary mapping tokens to indices that are fed into the embedding layer embedding_dim : size of the embedding vectors","title":"Args"},{"location":"losses/","text":"masked_crossentropy def masked_crossentropy ( targets , logits )","title":"Losses"},{"location":"losses/#masked_crossentropy","text":"def masked_crossentropy ( targets , logits )","title":"masked_crossentropy"},{"location":"trainer/","text":"class Trainer __init__ def __init__ ( max_input_len , max_output_len , batch_size , max_vocab_size_encoder , max_vocab_size_decoder , embedding_path_encoder , embedding_path_decoder , steps_per_epoch , tensorboard_dir , model_save_path , shuffle_buffer_size , use_bucketing , bucketing_buffer_size_batches , bucketing_batches_to_bucket , logging_level , num_print_predictions , steps_to_log , preprocessor ) Initializes the trainer. Args max_input_len (output) : Maximum length of input sequences, longer sequences will be truncated. max_output_len (output) : Maximum length of output sequences, longer sequences will be truncated. batch_size : Size of mini-batches for stochastic gradient descent. max_vocab_size_encoder : Maximum number of unique tokens to consider for encoder embeddings. max_vocab_size_decoder : Maximum number of unique tokens to consider for decoder embeddings. embedding_path_encoder : Path to embedding file for the encoder. embedding_path_decoder : Path to embedding file for the decoder. steps_per_epoch : Number of steps to train until callbacks are invoked. tensorboard_dir : Directory for saving tensorboard logs. model_save_path : Directory for saving the best model. shuffle_buffer_size : Size of the buffer for shuffling the files before batching. use_bucketing : Whether to bucket the sequences by length to reduce the amount of padding. bucketing_buffer_size_batches : Number of batches to buffer when bucketing sequences. bucketing_batches_to_bucket : Number of buffered batches from which sequences are collected for bucketing. logging_level : Level of logging to use, e.g. logging.INFO or logging.DEBUG. num_print_predictions : Number of sample predictions to print in each evaluation. steps_to_log : Number of steps to wait for logging output. preprocessor (optional) : custom preprocessor, if None a standard preprocessor will be created. from_config def from_config ( cls , file_path , ** kwargs ) train def train ( summarizer , train_data , val_data , num_epochs , scorers , callbacks ) Trains a summarizer or resumes training of a previously initialized summarizer. Args summarizer : Model to train, can be either a freshly created model or a loaded model. train_data : Data to train the model on. val_data (optional) : Validation data. num_epochs : Number of epochs to train. scorers (optional) : Dictionary with {score_name, scorer} to add validation scores to the logs. callbacks (optional) : Additional custom callbacks.","title":"Trainer"},{"location":"trainer/#class-trainer","text":"","title":"class Trainer"},{"location":"trainer/#__init__","text":"def __init__ ( max_input_len , max_output_len , batch_size , max_vocab_size_encoder , max_vocab_size_decoder , embedding_path_encoder , embedding_path_decoder , steps_per_epoch , tensorboard_dir , model_save_path , shuffle_buffer_size , use_bucketing , bucketing_buffer_size_batches , bucketing_batches_to_bucket , logging_level , num_print_predictions , steps_to_log , preprocessor ) Initializes the trainer.","title":"__init__"},{"location":"trainer/#args","text":"max_input_len (output) : Maximum length of input sequences, longer sequences will be truncated. max_output_len (output) : Maximum length of output sequences, longer sequences will be truncated. batch_size : Size of mini-batches for stochastic gradient descent. max_vocab_size_encoder : Maximum number of unique tokens to consider for encoder embeddings. max_vocab_size_decoder : Maximum number of unique tokens to consider for decoder embeddings. embedding_path_encoder : Path to embedding file for the encoder. embedding_path_decoder : Path to embedding file for the decoder. steps_per_epoch : Number of steps to train until callbacks are invoked. tensorboard_dir : Directory for saving tensorboard logs. model_save_path : Directory for saving the best model. shuffle_buffer_size : Size of the buffer for shuffling the files before batching. use_bucketing : Whether to bucket the sequences by length to reduce the amount of padding. bucketing_buffer_size_batches : Number of batches to buffer when bucketing sequences. bucketing_batches_to_bucket : Number of buffered batches from which sequences are collected for bucketing. logging_level : Level of logging to use, e.g. logging.INFO or logging.DEBUG. num_print_predictions : Number of sample predictions to print in each evaluation. steps_to_log : Number of steps to wait for logging output. preprocessor (optional) : custom preprocessor, if None a standard preprocessor will be created.","title":"Args"},{"location":"trainer/#from_config","text":"def from_config ( cls , file_path , ** kwargs )","title":"from_config"},{"location":"trainer/#train","text":"def train ( summarizer , train_data , val_data , num_epochs , scorers , callbacks ) Trains a summarizer or resumes training of a previously initialized summarizer.","title":"train"},{"location":"trainer/#args_1","text":"summarizer : Model to train, can be either a freshly created model or a loaded model. train_data : Data to train the model on. val_data (optional) : Validation data. num_epochs : Number of epochs to train. scorers (optional) : Dictionary with {score_name, scorer} to add validation scores to the logs. callbacks (optional) : Additional custom callbacks.","title":"Args"},{"location":"callbacks/evaluation_callback/","text":"class EvaluationCallback Callback for custom scoring methods. __init__ def __init__ ( summarizer , scorers , val_data , print_num_examples ) Initializes the Callback. Args summarizer : Summarizer that predicts over the validation data. scorers : Dictionary of {scorer_name val_data : Raw validation data to predict on. print_num_examples : Number of prediction examples to output for eyeballing the prediction quality. on_epoch_end def on_epoch_end ( batch , logs )","title":"EvaluationCallback"},{"location":"callbacks/evaluation_callback/#class-evaluationcallback","text":"Callback for custom scoring methods.","title":"class EvaluationCallback"},{"location":"callbacks/evaluation_callback/#__init__","text":"def __init__ ( summarizer , scorers , val_data , print_num_examples ) Initializes the Callback.","title":"__init__"},{"location":"callbacks/evaluation_callback/#args","text":"summarizer : Summarizer that predicts over the validation data. scorers : Dictionary of {scorer_name val_data : Raw validation data to predict on. print_num_examples : Number of prediction examples to output for eyeballing the prediction quality.","title":"Args"},{"location":"callbacks/evaluation_callback/#on_epoch_end","text":"def on_epoch_end ( batch , logs )","title":"on_epoch_end"},{"location":"callbacks/model_checkpoint_callback/","text":"class ModelCheckpointCallback Callback for checkpointing summarizer models. __init__ def __init__ ( file_path , summarizer , monitor , mode ) Initializes the Callback. Args file_path : Path for saving the model (a directory). If existing, the model will be overwritten. summarizer : Summarizer to checkpoint. monitor : Name of the score monitor for improvements. mode : If set to 'min' a decrease of the monitored score is seen as an improvement, otherwise an increase. on_epoch_end def on_epoch_end ( batch , logs )","title":"ModelCheckpointCallback"},{"location":"callbacks/model_checkpoint_callback/#class-modelcheckpointcallback","text":"Callback for checkpointing summarizer models.","title":"class ModelCheckpointCallback"},{"location":"callbacks/model_checkpoint_callback/#__init__","text":"def __init__ ( file_path , summarizer , monitor , mode ) Initializes the Callback.","title":"__init__"},{"location":"callbacks/model_checkpoint_callback/#args","text":"file_path : Path for saving the model (a directory). If existing, the model will be overwritten. summarizer : Summarizer to checkpoint. monitor : Name of the score monitor for improvements. mode : If set to 'min' a decrease of the monitored score is seen as an improvement, otherwise an increase.","title":"Args"},{"location":"callbacks/model_checkpoint_callback/#on_epoch_end","text":"def on_epoch_end ( batch , logs )","title":"on_epoch_end"},{"location":"callbacks/tensorboard_callback/","text":"class TensorboardCallback Callback for validation loss. __init__ def __init__ ( log_dir ) Initializes the Callback. Args log_dir : Tensorboard log directory to write to. on_epoch_end def on_epoch_end ( batch , logs )","title":"Tensorboard callback"},{"location":"callbacks/tensorboard_callback/#class-tensorboardcallback","text":"Callback for validation loss.","title":"class TensorboardCallback"},{"location":"callbacks/tensorboard_callback/#__init__","text":"def __init__ ( log_dir ) Initializes the Callback.","title":"__init__"},{"location":"callbacks/tensorboard_callback/#args","text":"log_dir : Tensorboard log directory to write to.","title":"Args"},{"location":"callbacks/tensorboard_callback/#on_epoch_end","text":"def on_epoch_end ( batch , logs )","title":"on_epoch_end"},{"location":"callbacks/validation_callback/","text":"class ValidationCallback Callback for validation loss. __init__ def __init__ ( summarizer , val_dataset , loss_function , batch_size ) Initializes the Callback. Args summarizer : Summarizer to validate. val_dataset : Validation dataset to validate the model on. loss_function : Loss function to apply to calculate the validation score. batch_size : Batch size of the validation dataset, needed for initializing the model. on_epoch_end def on_epoch_end ( batch , logs )","title":"ValidationCallback"},{"location":"callbacks/validation_callback/#class-validationcallback","text":"Callback for validation loss.","title":"class ValidationCallback"},{"location":"callbacks/validation_callback/#__init__","text":"def __init__ ( summarizer , val_dataset , loss_function , batch_size ) Initializes the Callback.","title":"__init__"},{"location":"callbacks/validation_callback/#args","text":"summarizer : Summarizer to validate. val_dataset : Validation dataset to validate the model on. loss_function : Loss function to apply to calculate the validation score. batch_size : Batch size of the validation dataset, needed for initializing the model.","title":"Args"},{"location":"callbacks/validation_callback/#on_epoch_end","text":"def on_epoch_end ( batch , logs )","title":"on_epoch_end"},{"location":"evaluation/bleu_scorer/","text":"class BleuScorer Provides BLEU score for a model prediction. __init__ def __init__ ( tokens_to_ignore , weights ) Initializes the scorer. Args tokens_to_ignore : Tokens to be removed before comparing input and output text. weights : Custom weights for 1,2,3,4 grams, e.g. (1, 0, 0, 0) will only measure 1-gram overlaps. __call__ def __call__ ( prediction )","title":"BleuScorer"},{"location":"evaluation/bleu_scorer/#class-bleuscorer","text":"Provides BLEU score for a model prediction.","title":"class BleuScorer"},{"location":"evaluation/bleu_scorer/#__init__","text":"def __init__ ( tokens_to_ignore , weights ) Initializes the scorer.","title":"__init__"},{"location":"evaluation/bleu_scorer/#args","text":"tokens_to_ignore : Tokens to be removed before comparing input and output text. weights : Custom weights for 1,2,3,4 grams, e.g. (1, 0, 0, 0) will only measure 1-gram overlaps.","title":"Args"},{"location":"evaluation/bleu_scorer/#__call__","text":"def __call__ ( prediction )","title":"__call__"},{"location":"evaluation/scorer/","text":"class Scorer __call__ def __call__ ( prediction ) Evaluates prediction. Args prediction : Dictionary providing all information about a model prediction such as","title":"Scorer"},{"location":"evaluation/scorer/#class-scorer","text":"","title":"class Scorer"},{"location":"evaluation/scorer/#__call__","text":"def __call__ ( prediction ) Evaluates prediction.","title":"__call__"},{"location":"evaluation/scorer/#args","text":"prediction : Dictionary providing all information about a model prediction such as","title":"Args"},{"location":"examples/advanced_nmt_example/","text":"Advanced Neural Machine Translation Example Upgrade grpcio which is needed by tensorboard 2.0.2 !pip install --upgrade grpcio Install TensorFlow and also our package via PyPI pip install tensorflow-gpu == 2 .0.0 pip install headliner Download the German-English sentence pairs wget http://www.manythings.org/anki/deu-eng.zip unzip deu-eng.zip Create the dataset but only take a subset for faster training import io def create_dataset ( path , num_examples ): lines = io . open ( path , encoding = 'UTF-8' ) . read () . strip () . split ( ' \\n ' ) word_pairs = [[ w for w in l . split ( ' \\t ' )[: 2 ]] for l in lines [: num_examples ]] return zip ( * word_pairs ) eng , ger , meta = create_dataset ( 'deu.txt' , 30000 ) data = list ( zip ( eng , ger )) Split the dataset into train and test from sklearn.model_selection import train_test_split train , test = train_test_split ( data , test_size = 100 ) Define custom preprocessing from headliner.preprocessing import Preprocessor preprocessor = Preprocessor ( lower_case = True ) train_prep = [ preprocessor ( t ) for t in train ] Fit custom tokenizers for input and target from tensorflow_datasets.core.features.text import SubwordTextEncoder from headliner.preprocessing import Vectorizer inputs_prep = [ t [ 0 ] for t in train_prep ] targets_prep = [ t [ 1 ] for t in train_prep ] tokenizer_input = SubwordTextEncoder . build_from_corpus ( inputs_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) tokenizer_target = SubwordTextEncoder . build_from_corpus ( targets_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) vectorizer = Vectorizer ( tokenizer_input , tokenizer_target ) 'vocab size input {} , target {} ' . format ( vectorizer . encoding_dim , vectorizer . decoding_dim ) Start tensorboard %load_ext tensorboard %tensorboard -- logdir / tmp / transformer_tensorboard Define the model and train it from headliner.model.transformer_summarizer import TransformerSummarizer from headliner.trainer import Trainer summarizer = TransformerSummarizer ( num_heads = 4 , feed_forward_dim = 1024 , num_layers = 1 , embedding_size = 256 , dropout_rate = 0.1 , max_prediction_len = 50 ) summarizer . init_model ( preprocessor , vectorizer ) trainer = Trainer ( steps_per_epoch = 250 , batch_size = 32 , model_save_path = '/tmp/transformer_summarizer' , tensorboard_dir = '/tmp/transformer_tensorboard' , steps_to_log = 50 ) trainer . train ( summarizer , train , num_epochs = 10 , val_data = test ) Load best model and do some prediction best_summarizer = TransformerSummarizer . load ( '/tmp/transformer_summarizer' ) best_summarizer . predict ( 'Do you like robots?' ) Plot attention alignment for a prediction import tensorflow as tf import matplotlib.pyplot as plt def plot_attention_weights ( summarizer , pred_vectors , layer_name ): fig = plt . figure ( figsize = ( 16 , 8 )) input_text , _ = pred_vectors [ 'preprocessed_text' ] input_sequence = summarizer . vectorizer . encode_input ( input_text ) pred_sequence = pred_vectors [ 'predicted_sequence' ] attention = tf . squeeze ( pred_vectors [ 'attention_weights' ][ layer_name ]) for head in range ( attention . shape [ 0 ]): ax = fig . add_subplot ( 1 , 2 , head + 1 ) ax . matshow ( attention [ head ][: - 1 , :], cmap = 'viridis' ) fontdict = { 'fontsize' : 10 } ax . set_xticks ( range ( len ( input_sequence ))) ax . set_yticks ( range ( len ( pred_sequence ))) ax . set_ylim ( len ( pred_sequence ) - 1.5 , - 0.5 ) ax . set_xticklabels ( [ summarizer . vectorizer . decode_input ([ i ]) for i in input_sequence ], fontdict = fontdict , rotation = 90 ) ax . set_yticklabels ([ summarizer . vectorizer . decode_output ([ i ]) for i in pred_sequence ], fontdict = fontdict ) ax . set_xlabel ( 'Head {} ' . format ( head + 1 )) plt . tight_layout () plt . show () pred_vectors = best_summarizer . predict_vectors ( 'Tom ran out of the burning house.' , '' ) plot_attention_weights ( best_summarizer , pred_vectors , 'decoder_layer1_block2' ) Continue training to improve the model and check the BLEU score from headliner.evaluation import BleuScorer bleu_scorer = BleuScorer ( tokens_to_ignore = [ preprocessor . start_token , preprocessor . end_token ]) trainer . train ( best_summarizer , train , num_epochs = 30 , val_data = test , scorers = { 'bleu' : bleu_scorer })","title":"Advanced Neural Machine Translation"},{"location":"examples/advanced_nmt_example/#advanced-neural-machine-translation-example","text":"","title":"Advanced Neural Machine Translation Example"},{"location":"examples/advanced_nmt_example/#upgrade-grpcio-which-is-needed-by-tensorboard-202","text":"!pip install --upgrade grpcio","title":"Upgrade grpcio which is needed by tensorboard 2.0.2"},{"location":"examples/advanced_nmt_example/#install-tensorflow-and-also-our-package-via-pypi","text":"pip install tensorflow-gpu == 2 .0.0 pip install headliner","title":"Install TensorFlow and also our package via PyPI"},{"location":"examples/advanced_nmt_example/#download-the-german-english-sentence-pairs","text":"wget http://www.manythings.org/anki/deu-eng.zip unzip deu-eng.zip","title":"Download the German-English sentence pairs"},{"location":"examples/advanced_nmt_example/#create-the-dataset-but-only-take-a-subset-for-faster-training","text":"import io def create_dataset ( path , num_examples ): lines = io . open ( path , encoding = 'UTF-8' ) . read () . strip () . split ( ' \\n ' ) word_pairs = [[ w for w in l . split ( ' \\t ' )[: 2 ]] for l in lines [: num_examples ]] return zip ( * word_pairs ) eng , ger , meta = create_dataset ( 'deu.txt' , 30000 ) data = list ( zip ( eng , ger ))","title":"Create the dataset but only take a subset for faster training"},{"location":"examples/advanced_nmt_example/#split-the-dataset-into-train-and-test","text":"from sklearn.model_selection import train_test_split train , test = train_test_split ( data , test_size = 100 )","title":"Split the dataset into train and test"},{"location":"examples/advanced_nmt_example/#define-custom-preprocessing","text":"from headliner.preprocessing import Preprocessor preprocessor = Preprocessor ( lower_case = True ) train_prep = [ preprocessor ( t ) for t in train ]","title":"Define custom preprocessing"},{"location":"examples/advanced_nmt_example/#fit-custom-tokenizers-for-input-and-target","text":"from tensorflow_datasets.core.features.text import SubwordTextEncoder from headliner.preprocessing import Vectorizer inputs_prep = [ t [ 0 ] for t in train_prep ] targets_prep = [ t [ 1 ] for t in train_prep ] tokenizer_input = SubwordTextEncoder . build_from_corpus ( inputs_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) tokenizer_target = SubwordTextEncoder . build_from_corpus ( targets_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) vectorizer = Vectorizer ( tokenizer_input , tokenizer_target ) 'vocab size input {} , target {} ' . format ( vectorizer . encoding_dim , vectorizer . decoding_dim )","title":"Fit custom tokenizers for input and target"},{"location":"examples/advanced_nmt_example/#start-tensorboard","text":"%load_ext tensorboard %tensorboard -- logdir / tmp / transformer_tensorboard","title":"Start tensorboard"},{"location":"examples/advanced_nmt_example/#define-the-model-and-train-it","text":"from headliner.model.transformer_summarizer import TransformerSummarizer from headliner.trainer import Trainer summarizer = TransformerSummarizer ( num_heads = 4 , feed_forward_dim = 1024 , num_layers = 1 , embedding_size = 256 , dropout_rate = 0.1 , max_prediction_len = 50 ) summarizer . init_model ( preprocessor , vectorizer ) trainer = Trainer ( steps_per_epoch = 250 , batch_size = 32 , model_save_path = '/tmp/transformer_summarizer' , tensorboard_dir = '/tmp/transformer_tensorboard' , steps_to_log = 50 ) trainer . train ( summarizer , train , num_epochs = 10 , val_data = test )","title":"Define the model and train it"},{"location":"examples/advanced_nmt_example/#load-best-model-and-do-some-prediction","text":"best_summarizer = TransformerSummarizer . load ( '/tmp/transformer_summarizer' ) best_summarizer . predict ( 'Do you like robots?' )","title":"Load best model and do some prediction"},{"location":"examples/advanced_nmt_example/#plot-attention-alignment-for-a-prediction","text":"import tensorflow as tf import matplotlib.pyplot as plt def plot_attention_weights ( summarizer , pred_vectors , layer_name ): fig = plt . figure ( figsize = ( 16 , 8 )) input_text , _ = pred_vectors [ 'preprocessed_text' ] input_sequence = summarizer . vectorizer . encode_input ( input_text ) pred_sequence = pred_vectors [ 'predicted_sequence' ] attention = tf . squeeze ( pred_vectors [ 'attention_weights' ][ layer_name ]) for head in range ( attention . shape [ 0 ]): ax = fig . add_subplot ( 1 , 2 , head + 1 ) ax . matshow ( attention [ head ][: - 1 , :], cmap = 'viridis' ) fontdict = { 'fontsize' : 10 } ax . set_xticks ( range ( len ( input_sequence ))) ax . set_yticks ( range ( len ( pred_sequence ))) ax . set_ylim ( len ( pred_sequence ) - 1.5 , - 0.5 ) ax . set_xticklabels ( [ summarizer . vectorizer . decode_input ([ i ]) for i in input_sequence ], fontdict = fontdict , rotation = 90 ) ax . set_yticklabels ([ summarizer . vectorizer . decode_output ([ i ]) for i in pred_sequence ], fontdict = fontdict ) ax . set_xlabel ( 'Head {} ' . format ( head + 1 )) plt . tight_layout () plt . show () pred_vectors = best_summarizer . predict_vectors ( 'Tom ran out of the burning house.' , '' ) plot_attention_weights ( best_summarizer , pred_vectors , 'decoder_layer1_block2' )","title":"Plot attention alignment for a prediction"},{"location":"examples/advanced_nmt_example/#continue-training-to-improve-the-model-and-check-the-bleu-score","text":"from headliner.evaluation import BleuScorer bleu_scorer = BleuScorer ( tokens_to_ignore = [ preprocessor . start_token , preprocessor . end_token ]) trainer . train ( best_summarizer , train , num_epochs = 30 , val_data = test , scorers = { 'bleu' : bleu_scorer })","title":"Continue training to improve the model and check the BLEU score"},{"location":"examples/bert_example/","text":"BERT Neural Machine Translation Example Upgrade grpcio which is needed by tensorboard 2.0.2 !pip install --upgrade grpcio Install TensorFlow and also our package via PyPI pip install tensorflow-gpu == 2 .0.0 pip install headliner Download the German-English sentence pairs wget http://www.manythings.org/anki/deu-eng.zip unzip deu-eng.zip Create the dataset but only take a subset for faster training import io def create_dataset ( path , num_examples ): lines = io . open ( path , encoding = 'UTF-8' ) . read () . strip () . split ( ' \\n ' ) word_pairs = [[ w for w in l . split ( ' \\t ' )[: 2 ]] for l in lines [: num_examples ]] return zip ( * word_pairs ) eng , ger , meta = create_dataset ( 'deu.txt' , 200000 ) data = list ( zip ( eng , ger )) Split the dataset into train and test from sklearn.model_selection import train_test_split train , test = train_test_split ( data , test_size = 100 ) Define custom preprocessing from headliner.preprocessing.bert_preprocessor import BertPreprocessor from spacy.lang.en import English preprocessor = BertPreprocessor ( nlp = English ()) train_prep = [ preprocessor ( t ) for t in train ] train_prep [: 5 ] Create custom tokenizers for input and target from tensorflow_datasets.core.features.text import SubwordTextEncoder from transformers import BertTokenizer from headliner.preprocessing.bert_vectorizer import BertVectorizer targets_prep = [ t [ 1 ] for t in train_prep ] tokenizer_input = BertTokenizer . from_pretrained ( 'bert-base-uncased' ) tokenizer_target = SubwordTextEncoder . build_from_corpus ( targets_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) vectorizer = BertVectorizer ( tokenizer_input , tokenizer_target ) 'vocab size input {} , target {} ' . format ( vectorizer . encoding_dim , vectorizer . decoding_dim ) Start tensorboard %load_ext tensorboard %tensorboard -- logdir / tmp / bert_tensorboard Define the model and train it # Define the model and train it # You need to be quite patient, since the model has a lot of params import tensorflow as tf from headliner.model.bert_summarizer import BertSummarizer from headliner.trainer import Trainer summarizer = BertSummarizer ( num_heads = 8 , feed_forward_dim = 1024 , num_layers_encoder = 0 , num_layers_decoder = 4 , bert_embedding_encoder = 'bert-base-uncased' , embedding_encoder_trainable = False , embedding_size_encoder = 768 , embedding_size_decoder = 768 , dropout_rate = 0 , max_prediction_len = 50 ) # Adjust learning rates of encoder and decoder optimizer schedules # You may want to try different learning rates and observe the loss summarizer . optimizer_decoder = BertSummarizer . new_optimizer_decoder ( learning_rate_start = 2e-2 ) summarizer . optimizer_encoder = BertSummarizer . new_optimizer_encoder ( learning_rate_start = 5e-4 ) summarizer . init_model ( preprocessor , vectorizer ) trainer = Trainer ( steps_per_epoch = 5000 , batch_size = 16 , model_save_path = '/tmp/bert_summarizer' , tensorboard_dir = '/tmp/bert_tensorboard' , steps_to_log = 10 ) trainer . train ( summarizer , train , num_epochs = 200 , val_data = test ) Load best model and do some prediction best_summarizer = BertSummarize . load ( '/tmp/bert_summarizer' ) best_summarizer . predict ( 'Do you like robots?' ) Plot attention alignment for a prediction import tensorflow as tf import matplotlib.pyplot as plt def plot_attention_weights ( summarizer , pred_vectors , layer_name ): fig = plt . figure ( figsize = ( 16 , 8 )) input_text , _ = pred_vectors [ 'preprocessed_text' ] input_sequence = summarizer . vectorizer . encode_input ( input_text ) pred_sequence = pred_vectors [ 'predicted_sequence' ] attention = tf . squeeze ( pred_vectors [ 'attention_weights' ][ layer_name ]) for head in range ( attention . shape [ 0 ]): ax = fig . add_subplot ( 1 , 2 , head + 1 ) ax . matshow ( attention [ head ][: - 1 , :], cmap = 'viridis' ) fontdict = { 'fontsize' : 10 } ax . set_xticks ( range ( len ( input_sequence ))) ax . set_yticks ( range ( len ( pred_sequence ))) ax . set_ylim ( len ( pred_sequence ) - 1.5 , - 0.5 ) ax . set_xticklabels ( [ summarizer . vectorizer . decode_input ([ i ]) for i in input_sequence ], fontdict = fontdict , rotation = 90 ) ax . set_yticklabels ([ summarizer . vectorizer . decode_output ([ i ]) for i in pred_sequence ], fontdict = fontdict ) ax . set_xlabel ( 'Head {} ' . format ( head + 1 )) plt . tight_layout () plt . show () pred_vectors = best_summarizer . predict_vectors ( 'Tom ran out of the burning house.' , '' ) plot_attention_weights ( best_summarizer , pred_vectors , 'decoder_layer4_block2' )","title":"BERT Machine Translation"},{"location":"examples/bert_example/#bert-neural-machine-translation-example","text":"","title":"BERT Neural Machine Translation Example"},{"location":"examples/bert_example/#upgrade-grpcio-which-is-needed-by-tensorboard-202","text":"!pip install --upgrade grpcio","title":"Upgrade grpcio which is needed by tensorboard 2.0.2"},{"location":"examples/bert_example/#install-tensorflow-and-also-our-package-via-pypi","text":"pip install tensorflow-gpu == 2 .0.0 pip install headliner","title":"Install TensorFlow and also our package via PyPI"},{"location":"examples/bert_example/#download-the-german-english-sentence-pairs","text":"wget http://www.manythings.org/anki/deu-eng.zip unzip deu-eng.zip","title":"Download the German-English sentence pairs"},{"location":"examples/bert_example/#create-the-dataset-but-only-take-a-subset-for-faster-training","text":"import io def create_dataset ( path , num_examples ): lines = io . open ( path , encoding = 'UTF-8' ) . read () . strip () . split ( ' \\n ' ) word_pairs = [[ w for w in l . split ( ' \\t ' )[: 2 ]] for l in lines [: num_examples ]] return zip ( * word_pairs ) eng , ger , meta = create_dataset ( 'deu.txt' , 200000 ) data = list ( zip ( eng , ger ))","title":"Create the dataset but only take a subset for faster training"},{"location":"examples/bert_example/#split-the-dataset-into-train-and-test","text":"from sklearn.model_selection import train_test_split train , test = train_test_split ( data , test_size = 100 )","title":"Split the dataset into train and test"},{"location":"examples/bert_example/#define-custom-preprocessing","text":"from headliner.preprocessing.bert_preprocessor import BertPreprocessor from spacy.lang.en import English preprocessor = BertPreprocessor ( nlp = English ()) train_prep = [ preprocessor ( t ) for t in train ] train_prep [: 5 ]","title":"Define custom preprocessing"},{"location":"examples/bert_example/#create-custom-tokenizers-for-input-and-target","text":"from tensorflow_datasets.core.features.text import SubwordTextEncoder from transformers import BertTokenizer from headliner.preprocessing.bert_vectorizer import BertVectorizer targets_prep = [ t [ 1 ] for t in train_prep ] tokenizer_input = BertTokenizer . from_pretrained ( 'bert-base-uncased' ) tokenizer_target = SubwordTextEncoder . build_from_corpus ( targets_prep , target_vocab_size = 2 ** 13 , reserved_tokens = [ preprocessor . start_token , preprocessor . end_token ]) vectorizer = BertVectorizer ( tokenizer_input , tokenizer_target ) 'vocab size input {} , target {} ' . format ( vectorizer . encoding_dim , vectorizer . decoding_dim )","title":"Create custom tokenizers for input and target"},{"location":"examples/bert_example/#start-tensorboard","text":"%load_ext tensorboard %tensorboard -- logdir / tmp / bert_tensorboard","title":"Start tensorboard"},{"location":"examples/bert_example/#define-the-model-and-train-it","text":"# Define the model and train it # You need to be quite patient, since the model has a lot of params import tensorflow as tf from headliner.model.bert_summarizer import BertSummarizer from headliner.trainer import Trainer summarizer = BertSummarizer ( num_heads = 8 , feed_forward_dim = 1024 , num_layers_encoder = 0 , num_layers_decoder = 4 , bert_embedding_encoder = 'bert-base-uncased' , embedding_encoder_trainable = False , embedding_size_encoder = 768 , embedding_size_decoder = 768 , dropout_rate = 0 , max_prediction_len = 50 ) # Adjust learning rates of encoder and decoder optimizer schedules # You may want to try different learning rates and observe the loss summarizer . optimizer_decoder = BertSummarizer . new_optimizer_decoder ( learning_rate_start = 2e-2 ) summarizer . optimizer_encoder = BertSummarizer . new_optimizer_encoder ( learning_rate_start = 5e-4 ) summarizer . init_model ( preprocessor , vectorizer ) trainer = Trainer ( steps_per_epoch = 5000 , batch_size = 16 , model_save_path = '/tmp/bert_summarizer' , tensorboard_dir = '/tmp/bert_tensorboard' , steps_to_log = 10 ) trainer . train ( summarizer , train , num_epochs = 200 , val_data = test )","title":"Define the model and train it"},{"location":"examples/bert_example/#load-best-model-and-do-some-prediction","text":"best_summarizer = BertSummarize . load ( '/tmp/bert_summarizer' ) best_summarizer . predict ( 'Do you like robots?' )","title":"Load best model and do some prediction"},{"location":"examples/bert_example/#plot-attention-alignment-for-a-prediction","text":"import tensorflow as tf import matplotlib.pyplot as plt def plot_attention_weights ( summarizer , pred_vectors , layer_name ): fig = plt . figure ( figsize = ( 16 , 8 )) input_text , _ = pred_vectors [ 'preprocessed_text' ] input_sequence = summarizer . vectorizer . encode_input ( input_text ) pred_sequence = pred_vectors [ 'predicted_sequence' ] attention = tf . squeeze ( pred_vectors [ 'attention_weights' ][ layer_name ]) for head in range ( attention . shape [ 0 ]): ax = fig . add_subplot ( 1 , 2 , head + 1 ) ax . matshow ( attention [ head ][: - 1 , :], cmap = 'viridis' ) fontdict = { 'fontsize' : 10 } ax . set_xticks ( range ( len ( input_sequence ))) ax . set_yticks ( range ( len ( pred_sequence ))) ax . set_ylim ( len ( pred_sequence ) - 1.5 , - 0.5 ) ax . set_xticklabels ( [ summarizer . vectorizer . decode_input ([ i ]) for i in input_sequence ], fontdict = fontdict , rotation = 90 ) ax . set_yticklabels ([ summarizer . vectorizer . decode_output ([ i ]) for i in pred_sequence ], fontdict = fontdict ) ax . set_xlabel ( 'Head {} ' . format ( head + 1 )) plt . tight_layout () plt . show () pred_vectors = best_summarizer . predict_vectors ( 'Tom ran out of the burning house.' , '' ) plot_attention_weights ( best_summarizer , pred_vectors , 'decoder_layer4_block2' )","title":"Plot attention alignment for a prediction"},{"location":"examples/nmt_example/","text":"Neural Machine Translation Example Install TensorFlow and also our package via PyPI pip install tensorflow-gpu == 2 .0.0 pip install headliner Download the German-English sentence pairs wget http://www.manythings.org/anki/deu-eng.zip unzip deu-eng.zip Create the dataset but only take a subset for faster training import io def create_dataset ( path , num_examples ): lines = io . open ( path , encoding = 'UTF-8' ) . read () . strip () . split ( ' \\n ' ) word_pairs = [[ w for w in l . split ( ' \\t ' )[: 2 ]] for l in lines [: num_examples ]] return zip ( * word_pairs ) eng , ger , meta = create_dataset ( 'deu.txt' , 30000 ) data = list ( zip ( eng , ger )) Split the dataset into train and test from sklearn.model_selection import train_test_split train , test = train_test_split ( data , test_size = 100 ) Define the model and train it from headliner.trainer import Trainer from headliner.model.attention_summarizer import AttentionSummarizer summarizer = AttentionSummarizer ( lstm_size = 1024 , embedding_size = 256 ) trainer = Trainer ( batch_size = 64 , steps_per_epoch = 100 , steps_to_log = 20 , max_output_len = 10 , model_save_path = '/tmp/summarizer' ) trainer . train ( summarizer , train , num_epochs = 10 , val_data = test ) Do some prediction summarizer . predict ( 'How are you?' )","title":"Neural Machine Translation"},{"location":"examples/nmt_example/#neural-machine-translation-example","text":"","title":"Neural Machine Translation Example"},{"location":"examples/nmt_example/#install-tensorflow-and-also-our-package-via-pypi","text":"pip install tensorflow-gpu == 2 .0.0 pip install headliner","title":"Install TensorFlow and also our package via PyPI"},{"location":"examples/nmt_example/#download-the-german-english-sentence-pairs","text":"wget http://www.manythings.org/anki/deu-eng.zip unzip deu-eng.zip","title":"Download the German-English sentence pairs"},{"location":"examples/nmt_example/#create-the-dataset-but-only-take-a-subset-for-faster-training","text":"import io def create_dataset ( path , num_examples ): lines = io . open ( path , encoding = 'UTF-8' ) . read () . strip () . split ( ' \\n ' ) word_pairs = [[ w for w in l . split ( ' \\t ' )[: 2 ]] for l in lines [: num_examples ]] return zip ( * word_pairs ) eng , ger , meta = create_dataset ( 'deu.txt' , 30000 ) data = list ( zip ( eng , ger ))","title":"Create the dataset but only take a subset for faster training"},{"location":"examples/nmt_example/#split-the-dataset-into-train-and-test","text":"from sklearn.model_selection import train_test_split train , test = train_test_split ( data , test_size = 100 )","title":"Split the dataset into train and test"},{"location":"examples/nmt_example/#define-the-model-and-train-it","text":"from headliner.trainer import Trainer from headliner.model.attention_summarizer import AttentionSummarizer summarizer = AttentionSummarizer ( lstm_size = 1024 , embedding_size = 256 ) trainer = Trainer ( batch_size = 64 , steps_per_epoch = 100 , steps_to_log = 20 , max_output_len = 10 , model_save_path = '/tmp/summarizer' ) trainer . train ( summarizer , train , num_epochs = 10 , val_data = test )","title":"Define the model and train it"},{"location":"examples/nmt_example/#do-some-prediction","text":"summarizer . predict ( 'How are you?' )","title":"Do some prediction"},{"location":"model/attention_model/","text":"class Encoder __init__ def __init__ ( embedding_shape , lstm_size , embedding_trainable , embedding_weights ) call def call ( sequence , states ) init_states def init_states ( batch_size ) class LuongAttention __init__ def __init__ ( rnn_size ) call def call ( decoder_output , encoder_output ) class Decoder __init__ def __init__ ( embedding_shape , lstm_size , embedding_trainable , embedding_weights ) call def call ( sequence , state , encoder_output ) init_states def init_states ( batch_size )","title":"Attention model"},{"location":"model/attention_model/#class-encoder","text":"","title":"class Encoder"},{"location":"model/attention_model/#__init__","text":"def __init__ ( embedding_shape , lstm_size , embedding_trainable , embedding_weights )","title":"__init__"},{"location":"model/attention_model/#call","text":"def call ( sequence , states )","title":"call"},{"location":"model/attention_model/#init_states","text":"def init_states ( batch_size )","title":"init_states"},{"location":"model/attention_model/#class-luongattention","text":"","title":"class LuongAttention"},{"location":"model/attention_model/#__init___1","text":"def __init__ ( rnn_size )","title":"__init__"},{"location":"model/attention_model/#call_1","text":"def call ( decoder_output , encoder_output )","title":"call"},{"location":"model/attention_model/#class-decoder","text":"","title":"class Decoder"},{"location":"model/attention_model/#__init___2","text":"def __init__ ( embedding_shape , lstm_size , embedding_trainable , embedding_weights )","title":"__init__"},{"location":"model/attention_model/#call_2","text":"def call ( sequence , state , encoder_output )","title":"call"},{"location":"model/attention_model/#init_states_1","text":"def init_states ( batch_size )","title":"init_states"},{"location":"model/attention_summarizer/","text":"class AttentionSummarizer __init__ def __init__ ( lstm_size , max_prediction_len , embedding_size , embedding_encoder_trainable , embedding_decoder_trainable ) init_model def init_model ( preprocessor , vectorizer , embedding_weights_encoder , embedding_weights_decoder ) __getstate__ def __getstate__ () Prevents pickle from serializing encoder and decoder predict def predict ( text ) predict_vectors def predict_vectors ( input_text , target_text ) new_train_step def new_train_step ( loss_function , batch_size , apply_gradients ) save def save ( out_path ) load def load ( in_path )","title":"Attention summarizer"},{"location":"model/attention_summarizer/#class-attentionsummarizer","text":"","title":"class AttentionSummarizer"},{"location":"model/attention_summarizer/#__init__","text":"def __init__ ( lstm_size , max_prediction_len , embedding_size , embedding_encoder_trainable , embedding_decoder_trainable )","title":"__init__"},{"location":"model/attention_summarizer/#init_model","text":"def init_model ( preprocessor , vectorizer , embedding_weights_encoder , embedding_weights_decoder )","title":"init_model"},{"location":"model/attention_summarizer/#__getstate__","text":"def __getstate__ () Prevents pickle from serializing encoder and decoder","title":"__getstate__"},{"location":"model/attention_summarizer/#predict","text":"def predict ( text )","title":"predict"},{"location":"model/attention_summarizer/#predict_vectors","text":"def predict_vectors ( input_text , target_text )","title":"predict_vectors"},{"location":"model/attention_summarizer/#new_train_step","text":"def new_train_step ( loss_function , batch_size , apply_gradients )","title":"new_train_step"},{"location":"model/attention_summarizer/#save","text":"def save ( out_path )","title":"save"},{"location":"model/attention_summarizer/#load","text":"def load ( in_path )","title":"load"},{"location":"model/basic_model/","text":"class Encoder __init__ def __init__ ( embedding_shape , lstm_size , embedding_weights , embedding_trainable ) call def call ( sequence , states ) init_states def init_states ( batch_size ) class Decoder __init__ def __init__ ( embedding_shape , lstm_size , embedding_weights , embedding_trainable ) call def call ( sequence , state )","title":"Basic model"},{"location":"model/basic_model/#class-encoder","text":"","title":"class Encoder"},{"location":"model/basic_model/#__init__","text":"def __init__ ( embedding_shape , lstm_size , embedding_weights , embedding_trainable )","title":"__init__"},{"location":"model/basic_model/#call","text":"def call ( sequence , states )","title":"call"},{"location":"model/basic_model/#init_states","text":"def init_states ( batch_size )","title":"init_states"},{"location":"model/basic_model/#class-decoder","text":"","title":"class Decoder"},{"location":"model/basic_model/#__init___1","text":"def __init__ ( embedding_shape , lstm_size , embedding_weights , embedding_trainable )","title":"__init__"},{"location":"model/basic_model/#call_1","text":"def call ( sequence , state )","title":"call"},{"location":"model/basic_summarizer/","text":"class BasicSummarizer __init__ def __init__ ( lstm_size , max_prediction_len , embedding_size , embedding_encoder_trainable , embedding_decoder_trainable ) init_model def init_model ( preprocessor , vectorizer , embedding_weights_encoder , embedding_weights_decoder ) __getstate__ def __getstate__ () Prevents pickle from serializing encoder and decoder. predict def predict ( text ) predict_vectors def predict_vectors ( input_text , target_text ) new_train_step def new_train_step ( loss_function , batch_size , apply_gradients ) save def save ( out_path ) load def load ( in_path )","title":"Basic summarizer"},{"location":"model/basic_summarizer/#class-basicsummarizer","text":"","title":"class BasicSummarizer"},{"location":"model/basic_summarizer/#__init__","text":"def __init__ ( lstm_size , max_prediction_len , embedding_size , embedding_encoder_trainable , embedding_decoder_trainable )","title":"__init__"},{"location":"model/basic_summarizer/#init_model","text":"def init_model ( preprocessor , vectorizer , embedding_weights_encoder , embedding_weights_decoder )","title":"init_model"},{"location":"model/basic_summarizer/#__getstate__","text":"def __getstate__ () Prevents pickle from serializing encoder and decoder.","title":"__getstate__"},{"location":"model/basic_summarizer/#predict","text":"def predict ( text )","title":"predict"},{"location":"model/basic_summarizer/#predict_vectors","text":"def predict_vectors ( input_text , target_text )","title":"predict_vectors"},{"location":"model/basic_summarizer/#new_train_step","text":"def new_train_step ( loss_function , batch_size , apply_gradients )","title":"new_train_step"},{"location":"model/basic_summarizer/#save","text":"def save ( out_path )","title":"save"},{"location":"model/basic_summarizer/#load","text":"def load ( in_path )","title":"load"},{"location":"model/bert_model/","text":"class Encoder __init__ def __init__ ( num_layers , num_heads , feed_forward_dim , embedding_shape , bert_embedding_name , embedding_trainable , embedding_weights , dropout_rate , max_seq_len ) call def call ( x , sent_ids , training , mask ) class Decoder __init__ def __init__ ( num_layers , num_heads , feed_forward_dim , embedding_shape , embedding_trainable , embedding_weights , dropout_rate , max_seq_len ) call def call ( x , enc_output , training , look_ahead_mask , padding_mask ) class Transformer __init__ def __init__ ( num_layers_encoder , num_layers_decoder , num_heads , feed_forward_dim , embedding_shape_encoder , embedding_shape_decoder , bert_embedding_encoder , embedding_encoder_trainable , embedding_decoder_trainable , embedding_weights_encoder , embedding_weights_decoder , dropout_rate , max_seq_len ) call def call ( inp , sent_ids , tar , training , enc_padding_mask , look_ahead_mask , dec_padding_mask )","title":"Bert model"},{"location":"model/bert_model/#class-encoder","text":"","title":"class Encoder"},{"location":"model/bert_model/#__init__","text":"def __init__ ( num_layers , num_heads , feed_forward_dim , embedding_shape , bert_embedding_name , embedding_trainable , embedding_weights , dropout_rate , max_seq_len )","title":"__init__"},{"location":"model/bert_model/#call","text":"def call ( x , sent_ids , training , mask )","title":"call"},{"location":"model/bert_model/#class-decoder","text":"","title":"class Decoder"},{"location":"model/bert_model/#__init___1","text":"def __init__ ( num_layers , num_heads , feed_forward_dim , embedding_shape , embedding_trainable , embedding_weights , dropout_rate , max_seq_len )","title":"__init__"},{"location":"model/bert_model/#call_1","text":"def call ( x , enc_output , training , look_ahead_mask , padding_mask )","title":"call"},{"location":"model/bert_model/#class-transformer","text":"","title":"class Transformer"},{"location":"model/bert_model/#__init___2","text":"def __init__ ( num_layers_encoder , num_layers_decoder , num_heads , feed_forward_dim , embedding_shape_encoder , embedding_shape_decoder , bert_embedding_encoder , embedding_encoder_trainable , embedding_decoder_trainable , embedding_weights_encoder , embedding_weights_decoder , dropout_rate , max_seq_len )","title":"__init__"},{"location":"model/bert_model/#call_2","text":"def call ( inp , sent_ids , tar , training , enc_padding_mask , look_ahead_mask , dec_padding_mask )","title":"call"},{"location":"model/bert_summarizer/","text":"class BertSummarizer __init__ def __init__ ( max_prediction_len , num_layers_encoder , num_layers_decoder , num_heads , feed_forward_dim , dropout_rate , embedding_size_encoder , embedding_size_decoder , bert_embedding_encoder , bert_embedding_decoder , embedding_encoder_trainable , embedding_decoder_trainable , max_sequence_len ) __getstate__ def __getstate__ () Prevents pickle from serializing the transformer and optimizer init_model def init_model ( preprocessor , vectorizer , embedding_weights_encoder , embedding_weights_decoder ) new_train_step def new_train_step ( loss_function , batch_size , apply_gradients ) predict def predict ( text ) predict_vectors def predict_vectors ( input_text , target_text ) save def save ( out_path ) load def load ( in_path ) new_optimizer_decoder def new_optimizer_decoder ( learning_rate_start , warmup_steps ) new_optimizer_encoder def new_optimizer_encoder ( learning_rate_start , warmup_steps ) class CustomSchedule __init__ def __init__ ( warmup_steps , learning_rate_start ) __call__ def __call__ ( step )","title":"Bert summarizer"},{"location":"model/bert_summarizer/#class-bertsummarizer","text":"","title":"class BertSummarizer"},{"location":"model/bert_summarizer/#__init__","text":"def __init__ ( max_prediction_len , num_layers_encoder , num_layers_decoder , num_heads , feed_forward_dim , dropout_rate , embedding_size_encoder , embedding_size_decoder , bert_embedding_encoder , bert_embedding_decoder , embedding_encoder_trainable , embedding_decoder_trainable , max_sequence_len )","title":"__init__"},{"location":"model/bert_summarizer/#__getstate__","text":"def __getstate__ () Prevents pickle from serializing the transformer and optimizer","title":"__getstate__"},{"location":"model/bert_summarizer/#init_model","text":"def init_model ( preprocessor , vectorizer , embedding_weights_encoder , embedding_weights_decoder )","title":"init_model"},{"location":"model/bert_summarizer/#new_train_step","text":"def new_train_step ( loss_function , batch_size , apply_gradients )","title":"new_train_step"},{"location":"model/bert_summarizer/#predict","text":"def predict ( text )","title":"predict"},{"location":"model/bert_summarizer/#predict_vectors","text":"def predict_vectors ( input_text , target_text )","title":"predict_vectors"},{"location":"model/bert_summarizer/#save","text":"def save ( out_path )","title":"save"},{"location":"model/bert_summarizer/#load","text":"def load ( in_path )","title":"load"},{"location":"model/bert_summarizer/#new_optimizer_decoder","text":"def new_optimizer_decoder ( learning_rate_start , warmup_steps )","title":"new_optimizer_decoder"},{"location":"model/bert_summarizer/#new_optimizer_encoder","text":"def new_optimizer_encoder ( learning_rate_start , warmup_steps )","title":"new_optimizer_encoder"},{"location":"model/bert_summarizer/#class-customschedule","text":"","title":"class CustomSchedule"},{"location":"model/bert_summarizer/#__init___1","text":"def __init__ ( warmup_steps , learning_rate_start )","title":"__init__"},{"location":"model/bert_summarizer/#__call__","text":"def __call__ ( step )","title":"__call__"},{"location":"model/summarizer/","text":"class Summarizer __init__ def __init__ () init_model def init_model ( preprocessor , vectorizer , embedding_weights_encoder , embedding_weights_decoder ) Initializes the model and provides necessary information for compilation. Args preprocessor : Preprocessor object that preprocesses text for training and prediction. vectorizer : Vectorizer object that performs vectorization of the text. embedding_weights_encoder (optional) : Matrix to initialize the encoder embedding. embedding_weights_decoder (optional) : Matrix to initialize the decoder embedding. predict def predict ( text ) Predicts summary of an input text. predict_vectors def predict_vectors ( input_text , target_text ) Predicts summary of an input text and outputs information needed for evaluation: output logits, input tokens, output tokens, predicted tokens, preprocessed text, attention alignment. Args input_text : Text used as input for prediction. target_text : Text used for evaluation. Returns new_train_step def new_train_step ( loss_function , batch_size , apply_gradients ) Initializes the train_step function to train the model on batches of data. Args loss_function : Loss function to perform backprop on. batch_size : Batch size to use for training. apply_gradients : Whether to apply the gradients, i.e. False if you want to validate the model on test data. save def save ( out_path ) Saves the model to a file. Args out_path : Path to directory for saving the model. load def load ( in_path ) Loads the model from a file. Args in_path : Path to the model directory.","title":"Summarizer"},{"location":"model/summarizer/#class-summarizer","text":"","title":"class Summarizer"},{"location":"model/summarizer/#__init__","text":"def __init__ ()","title":"__init__"},{"location":"model/summarizer/#init_model","text":"def init_model ( preprocessor , vectorizer , embedding_weights_encoder , embedding_weights_decoder ) Initializes the model and provides necessary information for compilation.","title":"init_model"},{"location":"model/summarizer/#args","text":"preprocessor : Preprocessor object that preprocesses text for training and prediction. vectorizer : Vectorizer object that performs vectorization of the text. embedding_weights_encoder (optional) : Matrix to initialize the encoder embedding. embedding_weights_decoder (optional) : Matrix to initialize the decoder embedding.","title":"Args"},{"location":"model/summarizer/#predict","text":"def predict ( text ) Predicts summary of an input text.","title":"predict"},{"location":"model/summarizer/#predict_vectors","text":"def predict_vectors ( input_text , target_text ) Predicts summary of an input text and outputs information needed for evaluation: output logits, input tokens, output tokens, predicted tokens, preprocessed text, attention alignment.","title":"predict_vectors"},{"location":"model/summarizer/#args_1","text":"input_text : Text used as input for prediction. target_text : Text used for evaluation.","title":"Args"},{"location":"model/summarizer/#returns","text":"","title":"Returns"},{"location":"model/summarizer/#new_train_step","text":"def new_train_step ( loss_function , batch_size , apply_gradients ) Initializes the train_step function to train the model on batches of data.","title":"new_train_step"},{"location":"model/summarizer/#args_2","text":"loss_function : Loss function to perform backprop on. batch_size : Batch size to use for training. apply_gradients : Whether to apply the gradients, i.e. False if you want to validate the model on test data.","title":"Args"},{"location":"model/summarizer/#save","text":"def save ( out_path ) Saves the model to a file.","title":"save"},{"location":"model/summarizer/#args_3","text":"out_path : Path to directory for saving the model.","title":"Args"},{"location":"model/summarizer/#load","text":"def load ( in_path ) Loads the model from a file.","title":"load"},{"location":"model/summarizer/#args_4","text":"in_path : Path to the model directory.","title":"Args"},{"location":"model/transformer_model/","text":"class Encoder __init__ def __init__ ( num_layers , num_heads , feed_forward_dim , embedding_shape , embedding_trainable , embedding_weights , dropout_rate , max_seq_len ) call def call ( x , training , mask ) class Decoder __init__ def __init__ ( num_layers , num_heads , feed_forward_dim , embedding_shape , embedding_trainable , embedding_weights , dropout_rate , max_seq_len ) call def call ( x , enc_output , training , look_ahead_mask , padding_mask ) class Transformer __init__ def __init__ ( num_layers , num_heads , feed_forward_dim , embedding_shape_encoder , embedding_shape_decoder , embedding_encoder_trainable , embedding_decoder_trainable , embedding_weights_encoder , embedding_weights_decoder , dropout_rate , max_sequence_len ) call def call ( inp , tar , training , enc_padding_mask , look_ahead_mask , dec_padding_mask )","title":"Transformer model"},{"location":"model/transformer_model/#class-encoder","text":"","title":"class Encoder"},{"location":"model/transformer_model/#__init__","text":"def __init__ ( num_layers , num_heads , feed_forward_dim , embedding_shape , embedding_trainable , embedding_weights , dropout_rate , max_seq_len )","title":"__init__"},{"location":"model/transformer_model/#call","text":"def call ( x , training , mask )","title":"call"},{"location":"model/transformer_model/#class-decoder","text":"","title":"class Decoder"},{"location":"model/transformer_model/#__init___1","text":"def __init__ ( num_layers , num_heads , feed_forward_dim , embedding_shape , embedding_trainable , embedding_weights , dropout_rate , max_seq_len )","title":"__init__"},{"location":"model/transformer_model/#call_1","text":"def call ( x , enc_output , training , look_ahead_mask , padding_mask )","title":"call"},{"location":"model/transformer_model/#class-transformer","text":"","title":"class Transformer"},{"location":"model/transformer_model/#__init___2","text":"def __init__ ( num_layers , num_heads , feed_forward_dim , embedding_shape_encoder , embedding_shape_decoder , embedding_encoder_trainable , embedding_decoder_trainable , embedding_weights_encoder , embedding_weights_decoder , dropout_rate , max_sequence_len )","title":"__init__"},{"location":"model/transformer_model/#call_2","text":"def call ( inp , tar , training , enc_padding_mask , look_ahead_mask , dec_padding_mask )","title":"call"},{"location":"model/transformer_summarizer/","text":"class TransformerSummarizer __init__ def __init__ ( max_prediction_len , num_layers , num_heads , feed_forward_dim , dropout_rate , embedding_size , embedding_encoder_trainable , embedding_decoder_trainable , max_sequence_len ) __getstate__ def __getstate__ () Prevents pickle from serializing the transformer and optimizer init_model def init_model ( preprocessor , vectorizer , embedding_weights_encoder , embedding_weights_decoder ) new_train_step def new_train_step ( loss_function , batch_size , apply_gradients ) predict def predict ( text ) predict_vectors def predict_vectors ( input_text , target_text ) save def save ( out_path ) load def load ( in_path ) new_optimizer def new_optimizer ()","title":"Transformer summarizer"},{"location":"model/transformer_summarizer/#class-transformersummarizer","text":"","title":"class TransformerSummarizer"},{"location":"model/transformer_summarizer/#__init__","text":"def __init__ ( max_prediction_len , num_layers , num_heads , feed_forward_dim , dropout_rate , embedding_size , embedding_encoder_trainable , embedding_decoder_trainable , max_sequence_len )","title":"__init__"},{"location":"model/transformer_summarizer/#__getstate__","text":"def __getstate__ () Prevents pickle from serializing the transformer and optimizer","title":"__getstate__"},{"location":"model/transformer_summarizer/#init_model","text":"def init_model ( preprocessor , vectorizer , embedding_weights_encoder , embedding_weights_decoder )","title":"init_model"},{"location":"model/transformer_summarizer/#new_train_step","text":"def new_train_step ( loss_function , batch_size , apply_gradients )","title":"new_train_step"},{"location":"model/transformer_summarizer/#predict","text":"def predict ( text )","title":"predict"},{"location":"model/transformer_summarizer/#predict_vectors","text":"def predict_vectors ( input_text , target_text )","title":"predict_vectors"},{"location":"model/transformer_summarizer/#save","text":"def save ( out_path )","title":"save"},{"location":"model/transformer_summarizer/#load","text":"def load ( in_path )","title":"load"},{"location":"model/transformer_summarizer/#new_optimizer","text":"def new_optimizer ()","title":"new_optimizer"},{"location":"model/transformer_util/","text":"get_angles def get_angles ( pos , i , embedding_size ) positional_encoding def positional_encoding ( max_len , embedding_size ) create_padding_mask def create_padding_mask ( seq ) create_look_ahead_mask def create_look_ahead_mask ( size ) scaled_dot_product_attention def scaled_dot_product_attention ( q , k , v , mask ) point_wise_feed_forward_network def point_wise_feed_forward_network ( embedding_size , feed_forward_dim ) create_masks def create_masks ( inp , tar ) class MultiHeadAttention __init__ def __init__ ( embedding_size , num_heads ) split_heads def split_heads ( x , batch_size ) call def call ( v , k , q , mask ) class EncoderLayer __init__ def __init__ ( embedding_size , num_heads , feed_forward_dim , dropout_rate ) call def call ( x , training , mask ) class DecoderLayer __init__ def __init__ ( embedding_size , num_heads , feed_forward_dim , dropout_rate ) call def call ( x , enc_output , training , look_ahead_mask , padding_mask )","title":"Transformer util"},{"location":"model/transformer_util/#get_angles","text":"def get_angles ( pos , i , embedding_size )","title":"get_angles"},{"location":"model/transformer_util/#positional_encoding","text":"def positional_encoding ( max_len , embedding_size )","title":"positional_encoding"},{"location":"model/transformer_util/#create_padding_mask","text":"def create_padding_mask ( seq )","title":"create_padding_mask"},{"location":"model/transformer_util/#create_look_ahead_mask","text":"def create_look_ahead_mask ( size )","title":"create_look_ahead_mask"},{"location":"model/transformer_util/#scaled_dot_product_attention","text":"def scaled_dot_product_attention ( q , k , v , mask )","title":"scaled_dot_product_attention"},{"location":"model/transformer_util/#point_wise_feed_forward_network","text":"def point_wise_feed_forward_network ( embedding_size , feed_forward_dim )","title":"point_wise_feed_forward_network"},{"location":"model/transformer_util/#create_masks","text":"def create_masks ( inp , tar )","title":"create_masks"},{"location":"model/transformer_util/#class-multiheadattention","text":"","title":"class MultiHeadAttention"},{"location":"model/transformer_util/#__init__","text":"def __init__ ( embedding_size , num_heads )","title":"__init__"},{"location":"model/transformer_util/#split_heads","text":"def split_heads ( x , batch_size )","title":"split_heads"},{"location":"model/transformer_util/#call","text":"def call ( v , k , q , mask )","title":"call"},{"location":"model/transformer_util/#class-encoderlayer","text":"","title":"class EncoderLayer"},{"location":"model/transformer_util/#__init___1","text":"def __init__ ( embedding_size , num_heads , feed_forward_dim , dropout_rate )","title":"__init__"},{"location":"model/transformer_util/#call_1","text":"def call ( x , training , mask )","title":"call"},{"location":"model/transformer_util/#class-decoderlayer","text":"","title":"class DecoderLayer"},{"location":"model/transformer_util/#__init___2","text":"def __init__ ( embedding_size , num_heads , feed_forward_dim , dropout_rate )","title":"__init__"},{"location":"model/transformer_util/#call_2","text":"def call ( x , enc_output , training , look_ahead_mask , padding_mask )","title":"call"},{"location":"preprocessing/bert_preprocessor/","text":"class BertPreprocessor __init__ def __init__ ( nlp ) Initializes the preprocessor. Args nlp : Spacy natural language processing pipeline. __call__ def __call__ ( data ) Splits input text into sentences and adds start and end token to each sentence.","title":"Bert preprocessor"},{"location":"preprocessing/bert_preprocessor/#class-bertpreprocessor","text":"","title":"class BertPreprocessor"},{"location":"preprocessing/bert_preprocessor/#__init__","text":"def __init__ ( nlp ) Initializes the preprocessor.","title":"__init__"},{"location":"preprocessing/bert_preprocessor/#args","text":"nlp : Spacy natural language processing pipeline.","title":"Args"},{"location":"preprocessing/bert_preprocessor/#__call__","text":"def __call__ ( data ) Splits input text into sentences and adds start and end token to each sentence.","title":"__call__"},{"location":"preprocessing/bert_vectorizer/","text":"class BertVectorizer Transforms tuples of text into tuples of vector sequences. __init__ def __init__ ( tokenizer_encoder , tokenizer_decoder , max_input_len , max_output_len ) Initializes the vectorizer. Args tokenizer_encoder : Tokenizer that encodes the input text. tokenizer_decoder : Tokenizer that encodes the target text. max_input_len (optional) : Maximum length of input sequence, longer sequences will be truncated. max_output_len (optional) : Maximum length of target sequence, longer sequences will be truncated and shorter sequences will be padded to max len. __call__ def __call__ ( data ) Encodes preprocessed strings into sequences of one-hot indices. encode_input def encode_input ( text ) encode_output def encode_output ( text ) decode_input def decode_input ( sequence ) decode_output def decode_output ( sequence )","title":"Bert vectorizer"},{"location":"preprocessing/bert_vectorizer/#class-bertvectorizer","text":"Transforms tuples of text into tuples of vector sequences.","title":"class BertVectorizer"},{"location":"preprocessing/bert_vectorizer/#__init__","text":"def __init__ ( tokenizer_encoder , tokenizer_decoder , max_input_len , max_output_len ) Initializes the vectorizer.","title":"__init__"},{"location":"preprocessing/bert_vectorizer/#args","text":"tokenizer_encoder : Tokenizer that encodes the input text. tokenizer_decoder : Tokenizer that encodes the target text. max_input_len (optional) : Maximum length of input sequence, longer sequences will be truncated. max_output_len (optional) : Maximum length of target sequence, longer sequences will be truncated and shorter sequences will be padded to max len.","title":"Args"},{"location":"preprocessing/bert_vectorizer/#__call__","text":"def __call__ ( data ) Encodes preprocessed strings into sequences of one-hot indices.","title":"__call__"},{"location":"preprocessing/bert_vectorizer/#encode_input","text":"def encode_input ( text )","title":"encode_input"},{"location":"preprocessing/bert_vectorizer/#encode_output","text":"def encode_output ( text )","title":"encode_output"},{"location":"preprocessing/bert_vectorizer/#decode_input","text":"def decode_input ( sequence )","title":"decode_input"},{"location":"preprocessing/bert_vectorizer/#decode_output","text":"def decode_output ( sequence )","title":"decode_output"},{"location":"preprocessing/bucket_generator/","text":"class BucketGenerator Performs bucketing of elements in a dataset by length. __init__ def __init__ ( element_length_function , batch_size , buffer_size_batches , batches_to_bucket , shuffle , seed ) Initializes the BucketGenerator. Args element_length_function : Element_length_function batch_size : The size of the batches to bucket the sequences into buffer_size_batches batches_to_bucket : Number of batches in buffer to use for bucketing. If set to buffer_size_batches, the resulting batches will be deterministic. shuffle : Whether to shuffle elements across batches and the resulting buckets. seed : Seed for shuffling. __call__ def __call__ ( data ) Returns iterable of data with elements ordered by bucketed sequence lengths, e.g for batch size = 2 the transformation could look like this: [1], [3, 3, 3], [1], [4, 4, 4, 4] -> [1], [1], [3, 3, 3], [4, 4, 4, 4]","title":"BucketGenerator"},{"location":"preprocessing/bucket_generator/#class-bucketgenerator","text":"Performs bucketing of elements in a dataset by length.","title":"class BucketGenerator"},{"location":"preprocessing/bucket_generator/#__init__","text":"def __init__ ( element_length_function , batch_size , buffer_size_batches , batches_to_bucket , shuffle , seed ) Initializes the BucketGenerator.","title":"__init__"},{"location":"preprocessing/bucket_generator/#args","text":"element_length_function : Element_length_function batch_size : The size of the batches to bucket the sequences into buffer_size_batches batches_to_bucket : Number of batches in buffer to use for bucketing. If set to buffer_size_batches, the resulting batches will be deterministic. shuffle : Whether to shuffle elements across batches and the resulting buckets. seed : Seed for shuffling.","title":"Args"},{"location":"preprocessing/bucket_generator/#__call__","text":"def __call__ ( data ) Returns iterable of data with elements ordered by bucketed sequence lengths, e.g for batch size = 2 the transformation could look like this: [1], [3, 3, 3], [1], [4, 4, 4, 4] -> [1], [1], [3, 3, 3], [4, 4, 4, 4]","title":"__call__"},{"location":"preprocessing/dataset_generator/","text":"class DatasetGenerator __init__ def __init__ ( batch_size , shuffle_buffer_size , rank ) __call__ def __call__ ( data_generator_func ) Initializes a dataset generator. Args data_generator_func : Callable that returns an iterable over the data to be batched, e.g. lambda","title":"DatasetGenerator"},{"location":"preprocessing/dataset_generator/#class-datasetgenerator","text":"","title":"class DatasetGenerator"},{"location":"preprocessing/dataset_generator/#__init__","text":"def __init__ ( batch_size , shuffle_buffer_size , rank )","title":"__init__"},{"location":"preprocessing/dataset_generator/#__call__","text":"def __call__ ( data_generator_func ) Initializes a dataset generator.","title":"__call__"},{"location":"preprocessing/dataset_generator/#args","text":"data_generator_func : Callable that returns an iterable over the data to be batched, e.g. lambda","title":"Args"},{"location":"preprocessing/keras_tokenizer/","text":"class KerasTokenizer __init__ def __init__ ( ** kwargs ) encode def encode ( text ) decode def decode ( sequence ) vocab_size def vocab_size () fit def fit ( texts ) token_index def token_index ()","title":"Keras tokenizer"},{"location":"preprocessing/keras_tokenizer/#class-kerastokenizer","text":"","title":"class KerasTokenizer"},{"location":"preprocessing/keras_tokenizer/#__init__","text":"def __init__ ( ** kwargs )","title":"__init__"},{"location":"preprocessing/keras_tokenizer/#encode","text":"def encode ( text )","title":"encode"},{"location":"preprocessing/keras_tokenizer/#decode","text":"def decode ( sequence )","title":"decode"},{"location":"preprocessing/keras_tokenizer/#vocab_size","text":"def vocab_size ()","title":"vocab_size"},{"location":"preprocessing/keras_tokenizer/#fit","text":"def fit ( texts )","title":"fit"},{"location":"preprocessing/keras_tokenizer/#token_index","text":"def token_index ()","title":"token_index"},{"location":"preprocessing/preprocessor/","text":"class Preprocessor __init__ def __init__ ( start_token , end_token , punctuation_pattern , filter_pattern , add_input_start_end , lower_case , hash_numbers ) Initializes the preprocessor. Args start_token : Unique start token to be inserted at the beginning of the target text. end_token : Unique end token to be attached at the end of a target text. punctuation_pattern : Regex pattern for punktuation that is splitted from the tokens. filter_pattern : Regex pattern for characters to be removed from the text. add_input_start_end : Whether to add start and end token to input sequence. lower_case : Whether to perform lower casing. hash_numbers : Whether to replace numbers by a #. __call__ def __call__ ( data ) Performs regex logic for string cleansing and attaches start and end tokens to the text.","title":"Preprocessor"},{"location":"preprocessing/preprocessor/#class-preprocessor","text":"","title":"class Preprocessor"},{"location":"preprocessing/preprocessor/#__init__","text":"def __init__ ( start_token , end_token , punctuation_pattern , filter_pattern , add_input_start_end , lower_case , hash_numbers ) Initializes the preprocessor.","title":"__init__"},{"location":"preprocessing/preprocessor/#args","text":"start_token : Unique start token to be inserted at the beginning of the target text. end_token : Unique end token to be attached at the end of a target text. punctuation_pattern : Regex pattern for punktuation that is splitted from the tokens. filter_pattern : Regex pattern for characters to be removed from the text. add_input_start_end : Whether to add start and end token to input sequence. lower_case : Whether to perform lower casing. hash_numbers : Whether to replace numbers by a #.","title":"Args"},{"location":"preprocessing/preprocessor/#__call__","text":"def __call__ ( data ) Performs regex logic for string cleansing and attaches start and end tokens to the text.","title":"__call__"},{"location":"preprocessing/tokenizer/","text":"class Tokenizer Encodes text to sequences and decodes sequences to text. encode def encode ( text ) Encodes a given string into a sequence of indices. Args text : Text to encode. decode def decode ( sequence ) Decodees a given sequence into a text. Args sequence : Sequence to decode. vocab_size def vocab_size () Size of token vocab.","title":"Tokenizer"},{"location":"preprocessing/tokenizer/#class-tokenizer","text":"Encodes text to sequences and decodes sequences to text.","title":"class Tokenizer"},{"location":"preprocessing/tokenizer/#encode","text":"def encode ( text ) Encodes a given string into a sequence of indices.","title":"encode"},{"location":"preprocessing/tokenizer/#args","text":"text : Text to encode.","title":"Args"},{"location":"preprocessing/tokenizer/#decode","text":"def decode ( sequence ) Decodees a given sequence into a text.","title":"decode"},{"location":"preprocessing/tokenizer/#args_1","text":"sequence : Sequence to decode.","title":"Args"},{"location":"preprocessing/tokenizer/#vocab_size","text":"def vocab_size () Size of token vocab.","title":"vocab_size"},{"location":"preprocessing/vectorizer/","text":"class Vectorizer Transforms tuples of text into tuples of vector sequences. __init__ def __init__ ( tokenizer_encoder , tokenizer_decoder , max_input_len , max_output_len ) Initializes the vectorizer. Args tokenizer_encoder : Tokenizer that encodes the input text. tokenizer_decoder : Tokenizer that encodes the target text. max_input_len (optional) : Maximum length of input sequence, longer sequences will be truncated. max_output_len (optional) : Maximum length of target sequence, longer sequences will be truncated and shorter sequences will be padded to max len. __call__ def __call__ ( data ) Encodes preprocessed strings into sequences of one-hot indices. encode_input def encode_input ( text ) encode_output def encode_output ( text ) decode_input def decode_input ( sequence ) decode_output def decode_output ( sequence )","title":"Vectorizer"},{"location":"preprocessing/vectorizer/#class-vectorizer","text":"Transforms tuples of text into tuples of vector sequences.","title":"class Vectorizer"},{"location":"preprocessing/vectorizer/#__init__","text":"def __init__ ( tokenizer_encoder , tokenizer_decoder , max_input_len , max_output_len ) Initializes the vectorizer.","title":"__init__"},{"location":"preprocessing/vectorizer/#args","text":"tokenizer_encoder : Tokenizer that encodes the input text. tokenizer_decoder : Tokenizer that encodes the target text. max_input_len (optional) : Maximum length of input sequence, longer sequences will be truncated. max_output_len (optional) : Maximum length of target sequence, longer sequences will be truncated and shorter sequences will be padded to max len.","title":"Args"},{"location":"preprocessing/vectorizer/#__call__","text":"def __call__ ( data ) Encodes preprocessed strings into sequences of one-hot indices.","title":"__call__"},{"location":"preprocessing/vectorizer/#encode_input","text":"def encode_input ( text )","title":"encode_input"},{"location":"preprocessing/vectorizer/#encode_output","text":"def encode_output ( text )","title":"encode_output"},{"location":"preprocessing/vectorizer/#decode_input","text":"def decode_input ( sequence )","title":"decode_input"},{"location":"preprocessing/vectorizer/#decode_output","text":"def decode_output ( sequence )","title":"decode_output"},{"location":"utils/logger/","text":"get_logger def get_logger ( name )","title":"Logger"},{"location":"utils/logger/#get_logger","text":"def get_logger ( name )","title":"get_logger"}]}