



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
        <meta name="author" content="Axel Springer AI">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="img/favicon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.6.3">
    
    
      
        <title>Headliner</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/application.adb8469c.css">
      
        <link rel="stylesheet" href="assets/stylesheets/application-palette.a8b3c06d.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
      <script src="assets/javascripts/modernizr.86422ebf.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="assets/fonts/material-icons.css">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-137434942-5", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="black" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#headliner" tabindex="0" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="." title="Headliner" aria-label="Headliner" class="md-header-nav__button md-logo">
          
            <img alt="logo" src="img/logo.svg" width="24" height="24">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Headliner
            </span>
            <span class="md-header-nav__topic">
              
                Home
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" aria-label="search" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/as-ideas/headliner/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    as-ideas/headliner
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="." title="Headliner" class="md-nav__button md-logo">
      
        <img alt="logo" src="img/logo.svg" width="48" height="48">
      
    </a>
    Headliner
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/as-ideas/headliner/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    as-ideas/headliner
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Home
      </label>
    
    <a href="." title="Home" class="md-nav__link md-nav__link--active">
      Home
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#update-21012020" class="md-nav__link">
    Update 21.01.2020
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#internals" class="md-nav__link">
    üß† Internals
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-headliner" class="md-nav__link">
    Why Headliner?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    ‚öôÔ∏è Installation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usage" class="md-nav__link">
    üìñ Usage
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training" class="md-nav__link">
    Training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prediction" class="md-nav__link">
    Prediction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#models" class="md-nav__link">
    Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-training" class="md-nav__link">
    Advanced training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-prediction" class="md-nav__link">
    Advanced prediction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resume-training" class="md-nav__link">
    Resume training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-pretrained-glove-embeddings" class="md-nav__link">
    Use pretrained GloVe embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-preprocessing" class="md-nav__link">
    Custom preprocessing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-pre-trained-bert-embeddings" class="md-nav__link">
    Use pre-trained BERT embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-on-large-datasets" class="md-nav__link">
    Training on large datasets
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contribute" class="md-nav__link">
    ü§ù Contribute
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cite-this-work" class="md-nav__link">
    üìù Cite this work
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#maintainers" class="md-nav__link">
    üèó Maintainers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#copyright" class="md-nav__link">
    ¬© Copyright
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#acknowlegements" class="md-nav__link">
    Acknowlegements
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      Examples
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Examples
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="examples/nmt_example/" title="Neural Machine Translation" class="md-nav__link">
      Neural Machine Translation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="examples/advanced_nmt_example/" title="Advanced Neural Machine Translation" class="md-nav__link">
      Advanced Neural Machine Translation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="examples/bert_example/" title="BERT Machine Translation" class="md-nav__link">
      BERT Machine Translation
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Documentation
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Documentation
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3-1" type="checkbox" id="nav-3-1">
    
    <label class="md-nav__link" for="nav-3-1">
      Callbacks
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-3-1">
        Callbacks
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="callbacks/validation_callback/" title="ValidationCallback" class="md-nav__link">
      ValidationCallback
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="callbacks/evaluation_callback/" title="EvaluationCallback" class="md-nav__link">
      EvaluationCallback
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="callbacks/model_checkpoint_callback/" title="ModelCheckpointCallback" class="md-nav__link">
      ModelCheckpointCallback
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3-2" type="checkbox" id="nav-3-2">
    
    <label class="md-nav__link" for="nav-3-2">
      Evaluation
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-3-2">
        Evaluation
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="evaluation/bleu_scorer/" title="BleuScorer" class="md-nav__link">
      BleuScorer
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3-3" type="checkbox" id="nav-3-3">
    
    <label class="md-nav__link" for="nav-3-3">
      Model
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-3-3">
        Model
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="model/summarizer/" title="Summarizer" class="md-nav__link">
      Summarizer
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="model/summarizer_attention.md" title="SummarizerAttention" class="md-nav__link">
      SummarizerAttention
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="model/summarizer_transformer.md" title="SummarizerTransformer" class="md-nav__link">
      SummarizerTransformer
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3-4" type="checkbox" id="nav-3-4">
    
    <label class="md-nav__link" for="nav-3-4">
      Preprocessing
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-3-4">
        Preprocessing
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="preprocessing/bucket_generator/" title="BucketGenerator" class="md-nav__link">
      BucketGenerator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="preprocessing/dataset_generator/" title="DatasetGenerator" class="md-nav__link">
      DatasetGenerator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="preprocessing/preprocessor/" title="Preprocessor" class="md-nav__link">
      Preprocessor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="preprocessing/vectorizer/" title="Vectorizer" class="md-nav__link">
      Vectorizer
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="preprocessing/tokenizer/" title="Tokenizer" class="md-nav__link">
      Tokenizer
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="trainer/" title="Trainer" class="md-nav__link">
      Trainer
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="losses/" title="Losses" class="md-nav__link">
      Losses
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="embeddings/" title="Embeddings" class="md-nav__link">
      Embeddings
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="CONTRIBUTING/" title="Contribution" class="md-nav__link">
      Contribution
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="LICENSE/" title="License" class="md-nav__link">
      License
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#update-21012020" class="md-nav__link">
    Update 21.01.2020
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#internals" class="md-nav__link">
    üß† Internals
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-headliner" class="md-nav__link">
    Why Headliner?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    ‚öôÔ∏è Installation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usage" class="md-nav__link">
    üìñ Usage
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training" class="md-nav__link">
    Training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prediction" class="md-nav__link">
    Prediction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#models" class="md-nav__link">
    Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-training" class="md-nav__link">
    Advanced training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-prediction" class="md-nav__link">
    Advanced prediction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resume-training" class="md-nav__link">
    Resume training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-pretrained-glove-embeddings" class="md-nav__link">
    Use pretrained GloVe embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-preprocessing" class="md-nav__link">
    Custom preprocessing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-pre-trained-bert-embeddings" class="md-nav__link">
    Use pre-trained BERT embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-on-large-datasets" class="md-nav__link">
    Training on large datasets
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contribute" class="md-nav__link">
    ü§ù Contribute
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cite-this-work" class="md-nav__link">
    üìù Cite this work
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#maintainers" class="md-nav__link">
    üèó Maintainers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#copyright" class="md-nav__link">
    ¬© Copyright
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#acknowlegements" class="md-nav__link">
    Acknowlegements
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/as-ideas/headliner/edit/master/docs/index.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="headliner">Headliner</h1>
<p><a href="https://dev.azure.com/axelspringerai/Public/_build/latest?definitionId=2&amp;branchName=master"><img alt="Build Status" src="https://dev.azure.com/axelspringerai/Public/_apis/build/status/as-ideas.headliner?branchName=master" /></a>
<a href="https://travis-ci.org/as-ideas/headliner"><img alt="Build Status" src="https://travis-ci.org/as-ideas/headliner.svg?branch=master" /></a>
<a href="https://as-ideas.github.io/headliner/"><img alt="Docs" src="https://img.shields.io/badge/docs-online-brightgreen" /></a>
<a href="https://codecov.io/gh/as-ideas/headliner"><img alt="codecov" src="https://codecov.io/gh/as-ideas/headliner/branch/master/graph/badge.svg" /></a>
<a href="https://pypi.org/project/headliner/"><img alt="PyPI Version" src="https://img.shields.io/pypi/v/headliner" /></a>
<a href="https://github.com/as-ideas/headliner/blob/master/LICENSE"><img alt="License" src="https://img.shields.io/badge/License-MIT-blue.svg" /></a></p>
<p>Headliner is a sequence modeling library that eases the training and <strong>in particular, the deployment of custom sequence models</strong>
for both researchers and developers. You can very easily deploy your models in a few lines of code. It was originally
built for our own research to generate headlines from <a href="https://www.welt.de/">Welt news articles</a> (see figure 1). That's why we chose the name, Headliner.</p>
<p align="center">
  <img src="figures/headline_generator.png" />
</p>

<p align="center">
  <b>Figure 1:</b> One example from our Welt.de headline generator.
</p>

<h2 id="update-21012020">Update 21.01.2020</h2>
<p>The library now supports fine-tuning pre-trained BERT models with 
custom preprocessing as in <a href="https://arxiv.org/pdf/1908.08345.pdf">Text Summarization with Pretrained Encoders</a>!</p>
<p>check out 
<a href="https://colab.research.google.com/github/as-ideas/headliner/blob/master/notebooks/BERT_Translation_Example.ipynb">this</a>
tutorial on colab!</p>
<h2 id="internals">üß† Internals</h2>
<p>We use sequence-to-sequence (seq2seq) under the hood,
an encoder-decoder framework (see figure 2). We provide a very simple interface to train
and deploy seq2seq models. Although this library was created internally to
generate headlines, you can also use it for <strong>other tasks like machine translations,
text summarization and many more.</strong></p>
<p align="center">
  <img src="figures/seq2seq.jpg" />
</p>

<p align="center">
  <b>Figure 2:</b> Encoder-decoder sequence-to-sequence model.
</p>

<h3 id="why-headliner">Why Headliner?</h3>
<p>You may ask why another seq2seq library? There are a couple of them out there already.
For example, Facebook has <a href="https://github.com/pytorch/fairseq">fairseq</a>, Google has <a href="https://github.com/google/seq2seq">seq2seq</a>
and there is also <a href="http://opennmt.net/">OpenNMT</a>.
Although those libraries are great, they have a few drawbacks for our use case e.g. the former doesn't focus much on production
whereas the Google one is not actively maintained. OpenNMT was the closest one to match our requirements i.e.
it has a strong focus on production. However, we didn't like that their workflow
(preparing data, training and evaluation) is mainly done via the command line.
They also expose a well-defined API though but the complexity there is still too high with too much custom code
(see their <a href="https://github.com/OpenNMT/OpenNMT-tf/blob/master/examples/library/minimal_transformer_training.py">minimal transformer training example</a>).    </p>
<p>Therefore, we built this library for us with the following goals in mind:</p>
<ul>
<li>Easy-to-use API for training and deployment (only a few lines of code)</li>
<li>Uses TensorFlow 2.0 with all its new features (<code>tf.function</code>, <code>tf.keras.layers</code> etc.)</li>
<li>Modular classes: text preprocessing, modeling, evaluation</li>
<li>Extensible for different encoder-decoder models</li>
<li>Works on large text data</li>
</ul>
<p>For more details on the library, read the documentation at: <a href="https://as-ideas.github.io/headliner/">https://as-ideas.github.io/headliner/</a></p>
<p>Headliner is compatible with Python 3.6 and is distributed under the MIT license.</p>
<h2 id="installation">‚öôÔ∏è Installation</h2>
<blockquote>
<p>‚ö†Ô∏è Before installing Headliner, you need to install TensorFlow as we use this as our deep learning framework. For more
details on how to install it, have a look at the <a href="https://www.tensorflow.org/install/">TensorFlow installation instructions</a>.</p>
</blockquote>
<p>Then you can install Headliner itself. There are two ways to install Headliner:</p>
<ul>
<li>Install Headliner from PyPI (recommended):</li>
</ul>
<div class="codehilite"><pre><span></span><code>pip install headliner
</code></pre></div>


<ul>
<li>Install Headliner from the GitHub source:</li>
</ul>
<div class="codehilite"><pre><span></span><code>git clone https://github.com/as-ideas/headliner.git
<span class="nb">cd</span> headliner
python setup.py install
</code></pre></div>


<h2 id="usage">üìñ Usage</h2>
<h3 id="training">Training</h3>
<p>For the training, you need to import one of our provided models or create your own custom one. Then you need to
create the dataset, a <code>tuple</code> of input-output sequences, and then train it:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">headliner.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">headliner.model.transformer_summarizer</span> <span class="kn">import</span> <span class="n">TransformerSummarizer</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;You are the stars, earth and sky for me!&#39;</span><span class="p">,</span> <span class="s1">&#39;I love you.&#39;</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;You are great, but I have other plans.&#39;</span><span class="p">,</span> <span class="s1">&#39;I like you.&#39;</span><span class="p">)]</span>

<span class="n">summarizer</span> <span class="o">=</span> <span class="n">TransformerSummarizer</span><span class="p">(</span><span class="n">embedding_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">max_prediction_len</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">summarizer</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">summarizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;/tmp/summarizer&#39;</span><span class="p">)</span>
</code></pre></div>


<h3 id="prediction">Prediction</h3>
<p>The prediction can be done in a few lines of code:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">headliner.model.transformer_summarizer</span> <span class="kn">import</span> <span class="n">TransformerSummarizer</span>

<span class="n">summarizer</span> <span class="o">=</span> <span class="n">TransformerSummarizer</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;/tmp/summarizer&#39;</span><span class="p">)</span>
<span class="n">summarizer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s1">&#39;You are the stars, earth and sky for me!&#39;</span><span class="p">)</span>
</code></pre></div>


<h3 id="models">Models</h3>
<p>Currently available models include a basic encoder-decoder, 
an encoder-decoder with Luong attention, the transformer and 
a transformer on top of a pre-trained BERT-model:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">headliner.model.basic_summarizer</span> <span class="kn">import</span> <span class="n">BasicSummarizer</span>
<span class="kn">from</span> <span class="nn">headliner.model.attention_summarizer</span> <span class="kn">import</span> <span class="n">AttentionSummarizer</span>
<span class="kn">from</span> <span class="nn">headliner.model.transformer_summarizer</span> <span class="kn">import</span> <span class="n">TransformerSummarizer</span>
<span class="kn">from</span> <span class="nn">headliner.model.bert_summarizer</span> <span class="kn">import</span> <span class="n">BertSummarizer</span>

<span class="n">basic_summarizer</span> <span class="o">=</span> <span class="n">BasicSummarizer</span><span class="p">()</span>
<span class="n">attention_summarizer</span> <span class="o">=</span> <span class="n">AttentionSummarizer</span><span class="p">()</span>
<span class="n">transformer_summarizer</span> <span class="o">=</span> <span class="n">TransformerSummarizer</span><span class="p">()</span>
<span class="n">bert_summarizer</span> <span class="o">=</span> <span class="n">BertSummarizer</span><span class="p">()</span>
</code></pre></div>


<h3 id="advanced-training">Advanced training</h3>
<p>Training using a validation split and model checkpointing:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">headliner.model.transformer_summarizer</span> <span class="kn">import</span> <span class="n">TransformerSummarizer</span>
<span class="kn">from</span> <span class="nn">headliner.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;You are the stars, earth and sky for me!&#39;</span><span class="p">,</span> <span class="s1">&#39;I love you.&#39;</span><span class="p">),</span>
              <span class="p">(</span><span class="s1">&#39;You are great, but I have other plans.&#39;</span><span class="p">,</span> <span class="s1">&#39;I like you.&#39;</span><span class="p">)]</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;You are great, but I have other plans.&#39;</span><span class="p">,</span> <span class="s1">&#39;I like you.&#39;</span><span class="p">)]</span>

<span class="n">summarizer</span> <span class="o">=</span> <span class="n">TransformerSummarizer</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                   <span class="n">feed_forward_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                                   <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                   <span class="n">embedding_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                   <span class="n">max_prediction_len</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                  <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                  <span class="n">max_vocab_size_encoder</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
                  <span class="n">max_vocab_size_decoder</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
                  <span class="n">tensorboard_dir</span><span class="o">=</span><span class="s1">&#39;/tmp/tensorboard&#39;</span><span class="p">,</span>
                  <span class="n">model_save_path</span><span class="o">=</span><span class="s1">&#39;/tmp/summarizer&#39;</span><span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">summarizer</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="o">=</span><span class="n">val_data</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div>


<h3 id="advanced-prediction">Advanced prediction</h3>
<p>Prediction information such as attention weights and logits can be accessed via predict_vectors returning a dictionary:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">headliner.model.transformer_summarizer</span> <span class="kn">import</span> <span class="n">TransformerSummarizer</span>

<span class="n">summarizer</span> <span class="o">=</span> <span class="n">TransformerSummarizer</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;/tmp/summarizer&#39;</span><span class="p">)</span>
<span class="n">summarizer</span><span class="o">.</span><span class="n">predict_vectors</span><span class="p">(</span><span class="s1">&#39;You are the stars, earth and sky for me!&#39;</span><span class="p">)</span>
</code></pre></div>


<h3 id="resume-training">Resume training</h3>
<p>A previously trained summarizer can be loaded and then retrained. In this case the data preprocessing and vectorization is loaded from the model.</p>
<div class="codehilite"><pre><span></span><code><span class="n">train_data</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;Some new training data.&#39;</span><span class="p">,</span> <span class="s1">&#39;New data.&#39;</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">10</span>

<span class="n">summarizer_loaded</span> <span class="o">=</span> <span class="n">TransformerSummarizer</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;/tmp/summarizer&#39;</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">summarizer_loaded</span><span class="p">,</span> <span class="n">train_data</span><span class="p">)</span>
<span class="n">summarizer_loaded</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;/tmp/summarizer_retrained&#39;</span><span class="p">)</span>
</code></pre></div>


<h3 id="use-pretrained-glove-embeddings">Use pretrained GloVe embeddings</h3>
<p>Embeddings in GloVe format can be injected in to the trainer as follows. Optionally, set the embedding to non-trainable.</p>
<div class="codehilite"><pre><span></span><code><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">embedding_path_encoder</span><span class="o">=</span><span class="s1">&#39;/tmp/embedding_encoder.txt&#39;</span><span class="p">,</span>
                  <span class="n">embedding_path_decoder</span><span class="o">=</span><span class="s1">&#39;/tmp/embedding_decoder.txt&#39;</span><span class="p">)</span>

<span class="c1"># make sure the embedding size matches to the embedding size of the files</span>
<span class="n">summarizer</span> <span class="o">=</span> <span class="n">TransformerSummarizer</span><span class="p">(</span><span class="n">embedding_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                   <span class="n">embedding_encoder_trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                   <span class="n">embedding_decoder_trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>


<h3 id="custom-preprocessing">Custom preprocessing</h3>
<p>A model can be initialized with custom preprocessing and tokenization:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">headliner.preprocessing.preprocessor</span> <span class="kn">import</span> <span class="n">Preprocessor</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;Some inputs.&#39;</span><span class="p">,</span> <span class="s1">&#39;Some outputs.&#39;</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">10</span>

<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">Preprocessor</span><span class="p">(</span><span class="n">filter_pattern</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span>
                            <span class="n">lower_case</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">hash_numbers</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_prep</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocessor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">]</span>
<span class="n">inputs_prep</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">train_prep</span><span class="p">]</span>
<span class="n">targets_prep</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">train_prep</span><span class="p">]</span>

<span class="c1"># Build tf subword tokenizers. Other custom tokenizers can be implemented</span>
<span class="c1"># by subclassing headliner.preprocessing.Tokenizer</span>
<span class="kn">from</span> <span class="nn">tensorflow_datasets.core.features.text</span> <span class="kn">import</span> <span class="n">SubwordTextEncoder</span>
<span class="n">tokenizer_input</span> <span class="o">=</span> <span class="n">SubwordTextEncoder</span><span class="o">.</span><span class="n">build_from_corpus</span><span class="p">(</span>
<span class="n">inputs_prep</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">13</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="o">=</span><span class="p">[</span><span class="n">preprocessor</span><span class="o">.</span><span class="n">start_token</span><span class="p">,</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">end_token</span><span class="p">])</span>
<span class="n">tokenizer_target</span> <span class="o">=</span> <span class="n">SubwordTextEncoder</span><span class="o">.</span><span class="n">build_from_corpus</span><span class="p">(</span>
    <span class="n">targets_prep</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">13</span><span class="p">,</span>  <span class="n">reserved_tokens</span><span class="o">=</span><span class="p">[</span><span class="n">preprocessor</span><span class="o">.</span><span class="n">start_token</span><span class="p">,</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">end_token</span><span class="p">])</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">Vectorizer</span><span class="p">(</span><span class="n">tokenizer_input</span><span class="p">,</span> <span class="n">tokenizer_target</span><span class="p">)</span>
<span class="n">summarizer</span> <span class="o">=</span> <span class="n">TransformerSummarizer</span><span class="p">(</span><span class="n">embedding_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">max_prediction_len</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">summarizer</span><span class="o">.</span><span class="n">init_model</span><span class="p">(</span><span class="n">preprocessor</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">summarizer</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div>


<h3 id="use-pre-trained-bert-embeddings">Use pre-trained BERT embeddings</h3>
<p>Pre-trained BERT models can be included as follows. 
Be aware that pre-trained BERT models are expensive to train and require custom preprocessing!</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">headliner.preprocessing.bert_preprocessor</span> <span class="kn">import</span> <span class="n">BertPreprocessor</span>
<span class="kn">from</span> <span class="nn">spacy.lang.en</span> <span class="kn">import</span> <span class="n">English</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;Some inputs.&#39;</span><span class="p">,</span> <span class="s1">&#39;Some outputs.&#39;</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">10</span>

<span class="c1"># use BERT-specific start and end token</span>
<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">BertPreprocessor</span><span class="p">(</span><span class="n">nlp</span><span class="o">=</span><span class="n">English</span><span class="p">()</span>
<span class="n">train_prep</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocessor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">]</span>
<span class="n">targets_prep</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">train_prep</span><span class="p">]</span>


<span class="kn">from</span> <span class="nn">tensorflow_datasets.core.features.text</span> <span class="kn">import</span> <span class="n">SubwordTextEncoder</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span>
<span class="kn">from</span> <span class="nn">headliner.model.bert_summarizer</span> <span class="kn">import</span> <span class="n">BertSummarizer</span>

<span class="c1"># Use a pre-trained BERT embedding and BERT tokenizer for the encoder </span>
<span class="n">tokenizer_input</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">tokenizer_target</span> <span class="o">=</span> <span class="n">SubwordTextEncoder</span><span class="o">.</span><span class="n">build_from_corpus</span><span class="p">(</span>
    <span class="n">targets_prep</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">13</span><span class="p">,</span>  <span class="n">reserved_tokens</span><span class="o">=</span><span class="p">[</span><span class="n">preprocessor</span><span class="o">.</span><span class="n">start_token</span><span class="p">,</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">end_token</span><span class="p">])</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">BertVectorizer</span><span class="p">(</span><span class="n">tokenizer_input</span><span class="p">,</span> <span class="n">tokenizer_target</span><span class="p">)</span>
<span class="n">summarizer</span> <span class="o">=</span> <span class="n">BertSummarizer</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">feed_forward_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                            <span class="n">num_layers_encoder</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">num_layers_decoder</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                            <span class="n">bert_embedding_encoder</span><span class="o">=</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span>
                            <span class="n">embedding_size_encoder</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
                            <span class="n">embedding_size_decoder</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
                            <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                            <span class="n">max_prediction_len</span><span class="o">=</span><span class="mi">50</span><span class="p">))</span>
<span class="n">summarizer</span><span class="o">.</span><span class="n">init_model</span><span class="p">(</span><span class="n">preprocessor</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">summarizer</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div>


<h3 id="training-on-large-datasets">Training on large datasets</h3>
<p>Large datasets can be handled by using an iterator:</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">read_data_iteratively</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">((</span><span class="s1">&#39;Some inputs.&#39;</span><span class="p">,</span> <span class="s1">&#39;Some outputs.&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>

<span class="k">class</span> <span class="nc">DataIterator</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">read_data_iteratively</span><span class="p">()</span>

<span class="n">data_iter</span> <span class="o">=</span> <span class="n">DataIterator</span><span class="p">()</span>

<span class="n">summarizer</span> <span class="o">=</span> <span class="n">TransformerSummarizer</span><span class="p">(</span><span class="n">embedding_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_prediction_len</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">summarizer</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div>


<h2 id="contribute">ü§ù Contribute</h2>
<p>We welcome all kinds of contributions such as new models, new examples and many more.
See the <a href="CONTRIBUTING/">Contribution</a> guide for more details.</p>
<h2 id="cite-this-work">üìù Cite this work</h2>
<p>Please cite Headliner in your publications if this is useful for your research. Here is an example BibTeX entry:</p>
<div class="codehilite"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">axelspringerai2019headliners</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Headliner}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Christian Sch√§fer &amp; Dat Tran}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2019}</span><span class="p">,</span>
  <span class="na">howpublished</span><span class="p">=</span><span class="s">{\url{https://github.com/as-ideas/headliner}}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>


<h2 id="maintainers">üèó Maintainers</h2>
<ul>
<li>Christian Sch√§fer, github: <a href="https://github.com/cschaefer26">cschaefer26</a></li>
<li>Dat Tran, github: <a href="https://github.com/datitran">datitran</a></li>
</ul>
<h2 id="copyright">¬© Copyright</h2>
<p>See <a href="LICENSE">LICENSE</a> for details.</p>
<h2 id="references">References</h2>
<p><a href="https://arxiv.org/pdf/1908.08345.pdf">Text Summarization with Pretrained Encoders</a></p>
<p><a href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation</a></p>
<h2 id="acknowlegements">Acknowlegements</h2>
<p>https://www.tensorflow.org/tutorials/text/transformer</p>
<p>https://github.com/huggingface/transformers</p>
<p>https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/</p>
                
                  
                
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
        
          <a href="examples/nmt_example/" title="Neural Machine Translation" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Neural Machine Translation
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org" target="_blank" rel="noopener">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="assets/javascripts/application.c33a9706.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"."}})</script>
      
    
  </body>
</html>